{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Detective's Quest: Unraveling the Money Laundering Syndicate\n",
    "\n",
    "Welcome, Detective! The king of **Valdris (K1)** has summoned you to investigate a cunning money laundering syndicate in **Goldweave Port (C5)**. Wielding the arcane art of **synthetic data**, you’ll conjure realistic, privacy-safe datasets—mimicking financial transactions, documents, and reports—without exposing sensitive information.\n",
    "\n",
    "In this quest, you will:\n",
    "- **Conjure transaction ledgers** to trace illicit financial flows (Mission 1).\n",
    "- **Forge bank statements and reports** to uncover suspicious activities (Mission 2).\n",
    "- **Train a mystical familiar** to detect anomalies in transaction networks (Mission 3).\n",
    "- **Summon a golem** to query evidence and reveal syndicate secrets (Mission 4).\n",
    "\n",
    "Through four missions, you’ll work with synthetic data tools to create accurate, privacy-preserving synthetic data. Your work will expose the K1C5 syndicate’s schemes (e.g., structured deposits) while demonstrating the ethical power of synthetic data in AML investigations.\n",
    "\n",
    "Embark on this quest to restore justice to Goldweave Port, using synthetic data to outsmart the syndicate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mission 1: Conjuring Financial Flow Projections\n",
    "**Objective**: Generate synthetic transactional databases using various methods to simulate the movement of money, enabling the tracing of illicit financial flows in a fantasy kingdom setting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def create_base_transaction_data(n_samples=3000, ml_ratio=0.05):\n",
    "    \"\"\"Generate synthetic financial transactions with money laundering patterns.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    kingdoms = ['K1', 'K2', 'K3', 'K4']\n",
    "    cities = ['C1', 'C2', 'C3', 'C4', 'C5']\n",
    "    all_locations = [f\"{k}{c}\" for k in kingdoms for c in cities]\n",
    "    common_locations = [loc for loc in all_locations if loc != 'K1C5']\n",
    "    high_risk_locations = ['K1C5']\n",
    "    monitored_locations = ['K2C5', 'K3C1', 'K4C3']\n",
    "\n",
    "    def select_location(is_ml=False, typology=None):\n",
    "        probs = [0.94, 0.05, 0.01] if not is_ml else [0.3, 0.3, 0.4] if typology != 'layering' else [0.2, 0.3, 0.5]\n",
    "        category = np.random.choice(['common', 'monitored', 'high_risk'], p=probs)\n",
    "        locations = {'common': common_locations, 'monitored': monitored_locations, 'high_risk': high_risk_locations}\n",
    "        return np.random.choice(locations[category])\n",
    "\n",
    "    data = {\n",
    "        'transaction_id': [], 'amount': [], 'account_age_days': [], 'transaction_hour': [], 'day_of_week': [],\n",
    "        'transactions_last_24h': [], 'account_balance_ratio': [], 'merchant_risk_score': [], 'cross_border': [],\n",
    "        'cash_equivalent': [], 'transaction_type': [], 'customer_segment': [], 'sender_location': [],\n",
    "        'receiver_location': [], 'is_money_laundering': []\n",
    "    }\n",
    "\n",
    "    n_legit = int(n_samples * (1 - ml_ratio))\n",
    "    for i in range(n_legit):\n",
    "        data['transaction_id'].append(f'TXN_{i+1:06d}')\n",
    "        data['amount'].append(np.random.lognormal(mean=6, sigma=2))\n",
    "        data['account_age_days'].append(int(np.random.gamma(shape=2, scale=365)))\n",
    "        data['transaction_hour'].append(np.random.randint(0, 24))\n",
    "        data['day_of_week'].append(np.random.randint(0, 7))\n",
    "        data['transactions_last_24h'].append(np.random.poisson(2))\n",
    "        data['account_balance_ratio'].append(np.random.beta(2, 5))\n",
    "        data['merchant_risk_score'].append(np.random.beta(2, 8))\n",
    "        data['cross_border'].append(np.random.choice([0, 1], p=[0.9, 0.1]))\n",
    "        data['cash_equivalent'].append(np.random.choice([0, 1], p=[0.95, 0.05]))\n",
    "        data['transaction_type'].append(np.random.choice(\n",
    "            ['wire_transfer', 'card_payment', 'atm_withdrawal', 'online_transfer', 'check_deposit', 'cash_deposit'],\n",
    "            p=[0.15, 0.35, 0.15, 0.25, 0.05, 0.05]))\n",
    "        data['customer_segment'].append(np.random.choice(\n",
    "            ['retail', 'business', 'premium', 'corporate'], p=[0.6, 0.2, 0.15, 0.05]))\n",
    "        sender = select_location()\n",
    "        receiver = select_location()\n",
    "        data['sender_location'].append(sender)\n",
    "        data['receiver_location'].append(receiver)\n",
    "        data['cross_border'][-1] = 1 if sender[:2] != receiver[:2] else 0\n",
    "        data['is_money_laundering'].append(0)\n",
    "\n",
    "    n_ml_base = int(n_samples * ml_ratio)\n",
    "    for i in range(n_ml_base):\n",
    "        typology = np.random.choice(['structuring', 'layering', 'integration'], p=[0.4, 0.4, 0.2])\n",
    "        base_amount = np.random.lognormal(mean=6, sigma=2) * np.random.uniform(3, 15)\n",
    "        account_age = int(np.random.gamma(shape=2, scale=365))\n",
    "        day_of_week = np.random.randint(0, 7)\n",
    "        merchant_risk = np.random.beta(8, 2)\n",
    "        transaction_type = np.random.choice(['wire_transfer', 'cash_deposit', 'online_transfer'], p=[0.6, 0.3, 0.1])\n",
    "        customer_segment = np.random.choice(['retail', 'business', 'premium', 'corporate'], p=[0.6, 0.2, 0.15, 0.05])\n",
    "        sender = select_location(is_ml=True, typology=typology)\n",
    "        receiver = select_location(is_ml=True, typology=typology)\n",
    "        if np.random.random() < 0.7:\n",
    "            sender, receiver = ('K1C5', receiver) if np.random.random() < 0.5 else (sender, 'K1C5')\n",
    "        cross_border = 1 if sender[:2] != receiver[:2] else 0\n",
    "        account_balance_ratio = np.random.beta(2, 5)\n",
    "\n",
    "        if typology == 'structuring':\n",
    "            n_splits = np.random.randint(3, 10)\n",
    "            split_amounts = np.full(n_splits, base_amount / n_splits) + np.random.normal(0, base_amount * 0.05, n_splits)\n",
    "            split_amounts = np.maximum(split_amounts, 1)\n",
    "            base_hour = np.random.choice([0, 1, 2, 3, 22, 23]) if np.random.random() < 0.6 else np.random.randint(0, 24)\n",
    "            hours = np.sort(np.random.randint(base_hour - 12, base_hour + 12, n_splits) % 24)\n",
    "            for j in range(n_splits):\n",
    "                amount = round(split_amounts[j], -2) if np.random.random() < 0.7 else split_amounts[j]\n",
    "                data['transaction_id'].append(f'TXN_{len(data[\"transaction_id\"])+1:06d}')\n",
    "                data['amount'].append(amount)\n",
    "                data['account_age_days'].append(account_age)\n",
    "                data['transaction_hour'].append(hours[j])\n",
    "                data['day_of_week'].append(day_of_week)\n",
    "                data['transactions_last_24h'].append(n_splits)\n",
    "                data['account_balance_ratio'].append(account_balance_ratio)\n",
    "                data['merchant_risk_score'].append(merchant_risk)\n",
    "                data['cross_border'].append(cross_border)\n",
    "                data['cash_equivalent'].append(np.random.choice([0, 1], p=[0.6, 0.4]))\n",
    "                data['transaction_type'].append('cash_deposit' if np.random.random() < 0.5 else transaction_type)\n",
    "                data['customer_segment'].append(customer_segment)\n",
    "                data['sender_location'].append(sender)\n",
    "                data['receiver_location'].append(receiver)\n",
    "                data['is_money_laundering'].append(1)\n",
    "        elif typology == 'layering':\n",
    "            amount = round(base_amount, -2) if np.random.random() < 0.7 else base_amount\n",
    "            hour = np.random.choice([0, 1, 2, 3, 22, 23]) if np.random.random() < 0.6 else np.random.randint(0, 24)\n",
    "            if sender[:2] == receiver[:2]:\n",
    "                receiver = select_location(is_ml=True, typology=typology)\n",
    "            data['transaction_id'].append(f'TXN_{len(data[\"transaction_id\"])+1:06d}')\n",
    "            data['amount'].append(amount)\n",
    "            data['account_age_days'].append(account_age)\n",
    "            data['transaction_hour'].append(hour)\n",
    "            data['day_of_week'].append(day_of_week)\n",
    "            data['transactions_last_24h'].append(np.random.randint(5, 15))\n",
    "            data['account_balance_ratio'].append(account_balance_ratio)\n",
    "            data['merchant_risk_score'].append(merchant_risk * 1.2)\n",
    "            data['cross_border'].append(1)\n",
    "            data['cash_equivalent'].append(0)\n",
    "            data['transaction_type'].append('wire_transfer' if np.random.random() < 0.7 else 'online_transfer')\n",
    "            data['customer_segment'].append(customer_segment)\n",
    "            data['sender_location'].append(sender)\n",
    "            data['receiver_location'].append(receiver)\n",
    "            data['is_money_laundering'].append(1)\n",
    "        else:  # integration\n",
    "            amount = round(base_amount * np.random.uniform(0.8, 1.2), -2) if np.random.random() < 0.5 else base_amount\n",
    "            data['transaction_id'].append(f'TXN_{len(data[\"transaction_id\"])+1:06d}')\n",
    "            data['amount'].append(amount)\n",
    "            data['account_age_days'].append(account_age)\n",
    "            data['transaction_hour'].append(np.random.randint(0, 24))\n",
    "            data['day_of_week'].append(day_of_week)\n",
    "            data['transactions_last_24h'].append(np.random.randint(1, 5))\n",
    "            data['account_balance_ratio'].append(account_balance_ratio)\n",
    "            data['merchant_risk_score'].append(np.random.beta(2, 8))\n",
    "            data['cross_border'].append(np.random.choice([0, 1], p=[0.7, 0.3]))\n",
    "            data['cash_equivalent'].append(1)\n",
    "            data['transaction_type'].append(np.random.choice(['check_deposit', 'cash_deposit'], p=[0.6, 0.4]))\n",
    "            data['customer_segment'].append('business' if np.random.random() < 0.6 else customer_segment)\n",
    "            data['sender_location'].append(sender)\n",
    "            data['receiver_location'].append(receiver)\n",
    "            data['is_money_laundering'].append(1)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"Generated {len(df)} transactions ({ml_ratio*100:.1f}% money laundering)\")\n",
    "    print(f\"K1C5 transactions: {((df['sender_location'] == 'K1C5') | (df['receiver_location'] == 'K1C5')).sum()}\")\n",
    "    return df\n",
    "\n",
    "class SDVSynthesizerEngine:\n",
    "    \"\"\"Generate synthetic data using multiple algorithms for AML investigation.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.synthesizers = {}\n",
    "        self.results = {}\n",
    "\n",
    "    def prepare_metadata(self, df):\n",
    "        \"\"\"Prepare SDV metadata for synthetic data generation.\"\"\"\n",
    "        from sdv.metadata import SingleTableMetadata\n",
    "        df_clean = df.drop('transaction_id', axis=1)\n",
    "        metadata = SingleTableMetadata()\n",
    "        metadata.detect_from_dataframe(df_clean)\n",
    "        categorical_cols = ['transaction_type', 'customer_segment', 'cross_border', 'cash_equivalent', \n",
    "                           'is_money_laundering', 'day_of_week', 'sender_location', 'receiver_location']\n",
    "        numerical_cols = ['amount', 'account_age_days', 'transaction_hour', 'transactions_last_24h', \n",
    "                         'account_balance_ratio', 'merchant_risk_score']\n",
    "        for col in categorical_cols:\n",
    "            if col in df_clean.columns:\n",
    "                metadata.update_column(col, sdtype='categorical')\n",
    "        for col in numerical_cols:\n",
    "            if col in df_clean.columns:\n",
    "                metadata.update_column(col, sdtype='numerical')\n",
    "        print(f\"Prepared metadata for {len(df_clean.columns)} columns\")\n",
    "        return df_clean, metadata\n",
    "\n",
    "    def synthesize_with_gaussian_copula(self, df, metadata, n_samples=1000):\n",
    "        \"\"\"Generate synthetic data using Gaussian Copula.\"\"\"\n",
    "        from sdv.single_table import GaussianCopulaSynthesizer\n",
    "        print(\"Synthesizing with Gaussian Copula...\")\n",
    "        try:\n",
    "            synthesizer = GaussianCopulaSynthesizer(metadata)\n",
    "            synthesizer.fit(df)\n",
    "            synthetic_data = synthesizer.sample(num_rows=n_samples)\n",
    "            self.synthesizers['gaussian_copula'] = synthesizer\n",
    "            self.results['gaussian_copula'] = synthetic_data\n",
    "            print(f\"Generated {len(synthetic_data)} transactions\")\n",
    "            return synthetic_data\n",
    "        except Exception as e:\n",
    "            print(f\"Gaussian Copula failed: {e}\")\n",
    "            return None\n",
    "\n",
    "    def synthesize_with_ctgan(self, df, metadata, n_samples=1000, epochs=50):\n",
    "        \"\"\"Generate synthetic data using CTGAN.\"\"\"\n",
    "        from sdv.single_table import CTGANSynthesizer\n",
    "        print(\"Synthesizing with CTGAN...\")\n",
    "        try:\n",
    "            synthesizer = CTGANSynthesizer(metadata, epochs=epochs, batch_size=500, verbose=False)\n",
    "            synthesizer.fit(df)\n",
    "            synthetic_data = synthesizer.sample(num_rows=n_samples)\n",
    "            self.synthesizers['ctgan'] = synthesizer\n",
    "            self.results['ctgan'] = synthetic_data\n",
    "            print(f\"Generated {len(synthetic_data)} transactions\")\n",
    "            return synthetic_data\n",
    "        except Exception as e:\n",
    "            print(f\"CTGAN failed: {e}\")\n",
    "            return None\n",
    "\n",
    "    def synthesize_with_tvae(self, df, metadata, n_samples=1000, epochs=50):\n",
    "        \"\"\"Generate synthetic data using TVAE.\"\"\"\n",
    "        from sdv.single_table import TVAESynthesizer\n",
    "        print(\"Synthesizing with TVAE...\")\n",
    "        try:\n",
    "            synthesizer = TVAESynthesizer(metadata, epochs=epochs, batch_size=500, verbose=False)\n",
    "            synthesizer.fit(df)\n",
    "            synthetic_data = synthesizer.sample(num_rows=n_samples)\n",
    "            self.synthesizers['tvae'] = synthesizer\n",
    "            self.results['tvae'] = synthetic_data\n",
    "            print(f\"Generated {len(synthetic_data)} transactions\")\n",
    "            return synthetic_data\n",
    "        except Exception as e:\n",
    "            print(f\"TVAE failed: {e}\")\n",
    "            return None\n",
    "\n",
    "    def synthesize_with_graph_network(self, df, metadata, n_samples=1000, n_nodes=500):\n",
    "        \"\"\"Generate synthetic data using network-based approach.\"\"\"\n",
    "        print(\"Synthesizing with Graph Network...\")\n",
    "        try:\n",
    "            G = nx.DiGraph()\n",
    "            np.random.seed(42)\n",
    "            n_customers = n_nodes // 2\n",
    "            n_merchants = n_nodes - n_customers\n",
    "            customers = [f'CUST_{i:05d}' for i in range(1, n_customers + 1)]\n",
    "            merchants = [f'MERCH_{i:04d}' for i in range(1, n_merchants + 1)]\n",
    "            all_locations = list(set(df['sender_location'].unique()) | set(df['receiver_location'].unique()))\n",
    "            for customer in customers:\n",
    "                location = 'K1C5' if np.random.random() < 0.3 else np.random.choice(\n",
    "                    [loc for loc in all_locations if loc != 'K1C5'] or all_locations)\n",
    "                G.add_node(customer, type='customer', location=location)\n",
    "            for merchant in merchants:\n",
    "                location = 'K1C5' if np.random.random() < 0.25 else np.random.choice(\n",
    "                    [loc for loc in all_locations if loc != 'K1C5'] or all_locations)\n",
    "                G.add_node(merchant, type='merchant', location=location)\n",
    "\n",
    "            customers_k1c5 = [n for n, d in G.nodes(data=True) if d['type'] == 'customer' and d['location'] == 'K1C5'] or customers[:5]\n",
    "            merchants_k1c5 = [n for n, d in G.nodes(data=True) if d['type'] == 'merchant' and d['location'] == 'K1C5'] or merchants[:5]\n",
    "            customers_other = [n for n, d in G.nodes(data=True) if d['type'] == 'customer' and d['location'] != 'K1C5'] or customers[5:]\n",
    "            merchants_other = [n for n, d in G.nodes(data=True) if d['type'] == 'merchant' and d['location'] != 'K1C5'] or merchants[5:]\n",
    "\n",
    "            edges = []\n",
    "            ml_ratio = df['is_money_laundering'].mean()\n",
    "            for _ in range(n_samples):\n",
    "                is_ml = np.random.random() < ml_ratio\n",
    "                if is_ml and np.random.random() < 0.7:\n",
    "                    sender = np.random.choice(customers_k1c5) if np.random.random() < 0.5 else np.random.choice(customers_k1c5 + customers_other)\n",
    "                    receiver = np.random.choice(merchants_k1c5) if sender not in customers_k1c5 else np.random.choice(merchants_k1c5 + merchants_other)\n",
    "                else:\n",
    "                    sender = np.random.choice(customers)\n",
    "                    receiver = np.random.choice(merchants)\n",
    "                sender_location = G.nodes[sender]['location']\n",
    "                receiver_location = G.nodes[receiver]['location']\n",
    "                cross_border = 1 if sender_location[:2] != receiver_location[:2] else 0\n",
    "                amount = np.random.lognormal(mean=7 if is_ml else 6, sigma=2)\n",
    "                transaction_type = np.random.choice(\n",
    "                    ['wire_transfer', 'cash_deposit', 'online_transfer'], p=[0.6, 0.3, 0.1]\n",
    "                ) if is_ml else np.random.choice(df['transaction_type'].unique(), \n",
    "                    p=df['transaction_type'].value_counts(normalize=True))\n",
    "                merchant_risk_score = np.random.beta(8, 2) if is_ml else np.random.beta(2, 8)\n",
    "                edge_data = {\n",
    "                    'amount': max(1, amount), 'transaction_type': transaction_type, 'cross_border': cross_border,\n",
    "                    'is_money_laundering': int(is_ml), 'account_age_days': int(np.random.gamma(shape=2, scale=365)),\n",
    "                    'transaction_hour': np.random.randint(0, 24), 'day_of_week': np.random.randint(0, 7),\n",
    "                    'transactions_last_24h': np.random.poisson(5 if is_ml else 2),\n",
    "                    'account_balance_ratio': np.random.beta(2, 5), 'merchant_risk_score': merchant_risk_score,\n",
    "                    'cash_equivalent': np.random.choice([0, 1], p=[0.6, 0.4] if is_ml else [0.95, 0.05]),\n",
    "                    'customer_segment': np.random.choice(df['customer_segment'].unique(), \n",
    "                        p=df['customer_segment'].value_counts(normalize=True)),\n",
    "                    'sender_location': sender_location, 'receiver_location': receiver_location\n",
    "                }\n",
    "                edges.append(edge_data)\n",
    "                G.add_edge(sender, receiver, **edge_data)\n",
    "\n",
    "            synthetic_data = pd.DataFrame(edges)[[col for col in df.columns if col != 'transaction_id']]\n",
    "            self.synthesizers['graph_network'] = G\n",
    "            self.results['graph_network'] = synthetic_data\n",
    "            print(f\"Generated {len(synthetic_data)} transactions, {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "            return synthetic_data\n",
    "        except Exception as e:\n",
    "            print(f\"Graph Network failed: {e}\")\n",
    "            return None\n",
    "\n",
    "def evaluate_synthetic_quality(original_df, synthetic_results, metadata):\n",
    "    \"\"\"Evaluate synthetic data quality using SDV metrics.\"\"\"\n",
    "    print(\"\\n🔍 Quality Assessment\")\n",
    "    if not synthetic_results:\n",
    "        print(\"No synthetic data to evaluate\")\n",
    "        return {}\n",
    "    from sdv.evaluation.single_table import evaluate_quality\n",
    "    evaluation_results = {}\n",
    "    for method_name, synthetic_df in synthetic_results.items():\n",
    "        print(f\"\\nEvaluating {method_name.upper()}:\")\n",
    "        try:\n",
    "            quality_report = evaluate_quality(real_data=original_df, synthetic_data=synthetic_df, metadata=metadata)\n",
    "            overall_score = quality_report.get_score()\n",
    "            evaluation_results[method_name] = {\n",
    "                'overall_score': overall_score,\n",
    "                'column_shapes': quality_report.get_details('Column Shapes')['Score'].mean(),\n",
    "                'pair_trends': quality_report.get_details('Column Pair Trends')['Score'].mean()\n",
    "            }\n",
    "            print(f\"Overall Score: {overall_score:.3f}\")\n",
    "        except Exception:\n",
    "            evaluation_results[method_name] = manual_evaluation(original_df, synthetic_df, method_name)\n",
    "    print(\"\\nLeaderboard:\")\n",
    "    for rank, (method, scores) in enumerate(sorted(evaluation_results.items(), key=lambda x: x[1]['overall_score'], reverse=True), 1):\n",
    "        print(f\"{rank}. {method.upper()}: {scores['overall_score']:.3f}\")\n",
    "    return evaluation_results\n",
    "\n",
    "def manual_evaluation(original_df, synthetic_df, method_name):\n",
    "    \"\"\"Manually evaluate synthetic data quality.\"\"\"\n",
    "    synthetic_df = synthetic_df[original_df.columns]\n",
    "    orig_ml_rate = original_df['is_money_laundering'].mean()\n",
    "    synth_ml_rate = synthetic_df['is_money_laundering'].mean()\n",
    "    ml_score = max(0, 1 - abs(orig_ml_rate - synth_ml_rate) / orig_ml_rate) if orig_ml_rate > 0 else 0\n",
    "    orig_amount_mean = original_df['amount'].mean()\n",
    "    synth_amount_mean = synthetic_df['amount'].mean()\n",
    "    amount_score = max(0, 1 - abs(orig_amount_mean - synth_amount_mean) / orig_amount_mean) if orig_amount_mean > 0 else 0\n",
    "    validity_score = ((synthetic_df['transaction_hour'].between(0, 23).all() if 'transaction_hour' in synthetic_df else False) + \n",
    "                     (synthetic_df['amount'] > 0).all()) / 2\n",
    "    print(f\"ML Rate Score: {ml_score:.3f}, Amount Score: {amount_score:.3f}, Validity Score: {validity_score:.3f}\")\n",
    "    return {'overall_score': (ml_score + amount_score + validity_score) / 3, \n",
    "            'ml_rate_score': ml_score, 'amount_score': amount_score, 'validity_score': validity_score}\n",
    "\n",
    "def create_customer_merchant_networks(df, n_customers=500, n_merchants=200):\n",
    "    \"\"\"Add customer-merchant relationships to synthetic data.\"\"\"\n",
    "    print(\"\\nGenerating Network Features...\")\n",
    "    customers = [f'CUST_{i:05d}' for i in range(1, n_customers + 1)]\n",
    "    merchants = [f'MERCH_{i:04d}' for i in range(1, n_merchants + 1)]\n",
    "    df_network = df.copy()\n",
    "    df_network['customer_id'] = np.random.choice(customers, len(df))\n",
    "    df_network['merchant_id'] = np.random.choice(merchants, len(df))\n",
    "    ml_mask = df_network['is_money_laundering'] == 1\n",
    "    if ml_mask.sum() > 0:\n",
    "        high_risk_merchants = merchants[:int(len(merchants) * 0.1)]\n",
    "        suspicious_customers = customers[:int(len(customers) * 0.1)]\n",
    "        df_network.loc[ml_mask, 'merchant_id'] = np.random.choice(high_risk_merchants, ml_mask.sum())\n",
    "        df_network.loc[ml_mask, 'customer_id'] = np.random.choice(suspicious_customers, ml_mask.sum())\n",
    "    print(f\"Created network: {len(customers)} customers, {len(merchants)} merchants\")\n",
    "    return df_network\n",
    "\n",
    "def generate_time_series_flows(df, days=30):\n",
    "    \"\"\"Add time-series features to synthetic data.\"\"\"\n",
    "    print(\"\\nGenerating Time-Series Flows...\")\n",
    "    start_date = datetime(2024, 1, 1)\n",
    "    timestamps = [start_date + timedelta(days=np.random.randint(0, days), hours=int(row['transaction_hour']), \n",
    "                  minutes=np.random.randint(0, 60), seconds=np.random.randint(0, 60)) for _, row in df.iterrows()]\n",
    "    df_timeseries = df.copy()\n",
    "    df_timeseries['timestamp'] = pd.to_datetime(timestamps)\n",
    "    df_timeseries = df_timeseries.sort_values('timestamp').reset_index(drop=True)\n",
    "    if 'customer_id' in df_timeseries.columns:\n",
    "        df_timeseries['running_total'] = df_timeseries.groupby('customer_id')['amount'].cumsum()\n",
    "        df_timeseries['hour_of_day'] = df_timeseries['timestamp'].dt.hour\n",
    "        df_timeseries['day_of_week'] = df_timeseries['timestamp'].dt.dayofweek\n",
    "        df_timeseries = df_timeseries.sort_values(['customer_id', 'timestamp'])\n",
    "        df_timeseries = df_timeseries.set_index('timestamp')\n",
    "        df_timeseries['velocity_1h'] = df_timeseries.groupby('customer_id')['amount'].rolling(window='1H', min_periods=1).sum().reset_index(level=0, drop=True)\n",
    "        df_timeseries['velocity_24h'] = df_timeseries.groupby('customer_id')['amount'].rolling(window='24H', min_periods=1).sum().reset_index(level=0, drop=True)\n",
    "        df_timeseries = df_timeseries.reset_index()\n",
    "    print(f\"Generated time-series over {days} days\")\n",
    "    return df_timeseries\n",
    "\n",
    "def run_complete_mission_1():\n",
    "    \"\"\"Generate and evaluate synthetic transaction data for AML investigation.\"\"\"\n",
    "    print(\"\\n🏰 FANTASY KINGDOM AML - MISSION 1: Synthetic Data Generation\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nStep 1: Generating Transaction Data...\")\n",
    "    original_df = create_base_transaction_data(n_samples=3000, ml_ratio=0.05)\n",
    "    print(\"\\nStep 2: Initializing Synthesis Engine...\")\n",
    "    engine = SDVSynthesizerEngine()\n",
    "    print(\"\\nStep 3: Preparing Metadata...\")\n",
    "    df_clean, metadata = engine.prepare_metadata(original_df)\n",
    "    print(\"\\nStep 4: Synthesizing Data...\")\n",
    "    engine.synthesize_with_gaussian_copula(df_clean, metadata, n_samples=1000)\n",
    "    engine.synthesize_with_ctgan(df_clean, metadata, n_samples=1000, epochs=50)\n",
    "    engine.synthesize_with_tvae(df_clean, metadata, n_samples=1000, epochs=50)\n",
    "    engine.synthesize_with_graph_network(df_clean, metadata, n_samples=1000)\n",
    "    print(\"\\nStep 5: Evaluating Quality...\")\n",
    "    evaluation_results = evaluate_synthetic_quality(df_clean, engine.results, metadata)\n",
    "    print(\"\\nStep 6: Adding Network Features...\")\n",
    "    best_method = max(evaluation_results.items(), key=lambda x: x[1]['overall_score'])[0]\n",
    "    best_synthetic = engine.results[best_method].copy()\n",
    "    best_synthetic['transaction_id'] = [f'SYN_{i+1:06d}' for i in range(len(best_synthetic))]\n",
    "    df_with_networks = create_customer_merchant_networks(best_synthetic, n_customers=500, n_merchants=200)\n",
    "    print(\"\\nStep 7: Adding Time-Series Flows...\")\n",
    "    final_df = generate_time_series_flows(df_with_networks, days=30)\n",
    "    print(\"\\nStep 8: Saving Data...\")\n",
    "    final_df.to_csv('mission1_synthetic_data.csv', index=False)\n",
    "    print(\"\\nStep 9: Visualizing Transaction Volume...\")\n",
    "    final_df['timestamp'] = pd.to_datetime(final_df['timestamp'])\n",
    "    k1c5_df = final_df[final_df['sender_location'].eq('K1C5') | final_df['receiver_location'].eq('K1C5')]\n",
    "    non_k1c5_df = final_df[~(final_df['sender_location'].eq('K1C5') | final_df['receiver_location'].eq('K1C5'))]\n",
    "    k1c5_daily = k1c5_df.groupby(final_df['timestamp'].dt.date)['amount'].sum()\n",
    "    non_k1c5_daily = non_k1c5_df.groupby(final_df['timestamp'].dt.date)['amount'].sum()\n",
    "    ml_dates = final_df[final_df['is_money_laundering'] == 1]['timestamp'].dt.date.unique()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(k1c5_daily.index, k1c5_daily.values, label='K1C5 Transactions', color='orange')\n",
    "    plt.plot(non_k1c5_daily.index, non_k1c5_daily.values, label='Non-K1C5 Transactions', color='blue')\n",
    "    for date in ml_dates:\n",
    "        plt.axvline(x=date, color='red', linestyle='--', alpha=0.5, label='Money Laundering' if date == ml_dates[0] else '')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Transaction Volume (Gold Pieces)')\n",
    "    plt.title('Daily Transaction Volume: K1C5 vs Non-K1C5')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('mission1_timeseries.png')\n",
    "    plt.show()\n",
    "    print(\"\\nStep 10: Visualizing Transaction Network...\")\n",
    "    sub_df = final_df[final_df['sender_location'].eq('K1C5') | \n",
    "                     final_df['receiver_location'].eq('K1C5') | \n",
    "                     final_df['is_money_laundering'] == 1].nlargest(100, 'amount')\n",
    "    G = nx.DiGraph()\n",
    "    for _, row in sub_df.iterrows():\n",
    "        G.add_edge(row['customer_id'], row['merchant_id'], weight=row['amount'])\n",
    "    node_colors = ['red' if sub_df[sub_df['customer_id'] == n]['is_money_laundering'].sum() > 0 or \n",
    "                         sub_df[sub_df['merchant_id'] == n]['is_money_laundering'].sum() > 0 else 'blue' \n",
    "                   for n in G.nodes()]\n",
    "    node_colors = ['orange' if sub_df[sub_df['customer_id'] == n]['sender_location'].eq('K1C5').any() or \n",
    "                             sub_df[sub_df['merchant_id'] == n]['receiver_location'].eq('K1C5').any() else c \n",
    "                   for n, c in zip(G.nodes(), node_colors)]\n",
    "    node_sizes = [200 if c in ['orange', 'red'] else 50 for c in node_colors]\n",
    "    edge_weights = [G[u][v]['weight'] / 1000 for u, v in G.edges()]\n",
    "    edge_alphas = [min(w / max(edge_weights), 0.6) for w in edge_weights]\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    pos = nx.kamada_kawai_layout(G)\n",
    "    nx.draw(G, pos, node_color=node_colors, node_size=node_sizes, \n",
    "            edge_color='gray', width=edge_weights, alpha=edge_alphas, arrows=True, with_labels=False)\n",
    "    plt.title('Transaction Network: K1C5 (Orange), Money Laundering (Red)')\n",
    "    plt.savefig('mission1_network.png')\n",
    "    plt.show()\n",
    "    print(\"\\nMission 1 Complete!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Original Data: {len(original_df):,} transactions\")\n",
    "    print(f\"Best Method: {best_method.upper()}\")\n",
    "    print(f\"Final Dataset: {len(final_df):,} transactions\")\n",
    "    print(f\"Money Laundering Cases: {final_df['is_money_laundering'].sum():,}\")\n",
    "    print(f\"K1C5 Involvement: {((final_df['sender_location'] == 'K1C5') | (final_df['receiver_location'] == 'K1C5')).sum():,}\")\n",
    "    print(f\"Network: {final_df['customer_id'].nunique():,} customers, {final_df['merchant_id'].nunique():,} merchants\")\n",
    "    print(f\"Time Range: {final_df['timestamp'].min()} to {final_df['timestamp'].max()}\")\n",
    "    print(f\"Data saved to 'mission1_synthetic_data.csv'\")\n",
    "    print(f\"Visualizations saved to 'mission1_timeseries.png' and 'mission1_network.png'\")\n",
    "    return {\n",
    "        'original_data': original_df, 'synthetic_results': engine.results, 'evaluation_results': evaluation_results,\n",
    "        'best_method': best_method, 'final_enhanced_data': final_df, 'engine': engine\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #matplotlib.use('Agg')\n",
    "    results = run_complete_mission_1()\n",
    "    print(\"\\nReady for Mission 2: Model Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mission 1 Review\n",
    "\n",
    "You’ve harnessed the `SDVSynthesizerEngine` to create a synthetic transaction ledger (`mission1_synthetic_data.csv`), exposing the K1C5 syndicate’s money laundering schemes in Goldweave Port. This ledger, with over 3,000 transactions, network features (500 customers, 200 merchants), and 30-day time-series flows, reveals illicit patterns such as structured deposits. Explore the four methods powering this dataset.\n",
    "\n",
    "## Synthesis Methods\n",
    "\n",
    "The `SDVSynthesizerEngine` (`sdv>=0.18.0`) employs four techniques to generate realistic transactions:\n",
    "\n",
    "- **Gaussian Copula**: Captures statistical correlations, preserving relationships like `amount` and `is_money_laundering` for accurate financial flows.\n",
    "- **CTGAN**: Uses GANs to model complex patterns, adept at rare events like the 5% money laundering cases across `amount` and `sender_location`.\n",
    "- **Graph Network**: Builds NetworkX graphs to simulate K1C5 account relationships, with edge weights (`amount`) and node features (`velocity_24h`).\n",
    "- **TVAE**: Leverages variational autoencoders to produce diverse, high-fidelity features (e.g., `account_age_days`) for balanced realism.\n",
    "\n",
    "## Process\n",
    "- **Base Data**: Generates 3,000 transactions with a 5% money laundering ratio, flagged as `is_money_laundering`.\n",
    "- **Synthesis**: Produces 1,000 transactions per method, assessed using SDV quality metrics (e.g., overall score).\n",
    "- **Enhancement**: Enriches the best method’s data with customer-merchant networks (`customer_id`, `merchant_id`) and time-series features (`timestamp`, `velocity_24h`) over 30 days.\n",
    "- **Visualization**: \n",
    "  - Displays a time-series plot (`mission1_timeseries.png`) of daily transaction volumes, highlighting K1C5 (orange) and money laundering days (red lines).\n",
    "  - Generates a network graph (`mission1_network.png`) showing K1C5 (orange), money laundering (red), and other (blue) accounts.\n",
    "- **Output**: Saves the enhanced ledger to `mission1_synthetic_data.csv` for downstream missions.\n",
    "\n",
    "## Why It Matters\n",
    "This synthetic ledger mirrors K1C5’s illicit tactics (e.g., high-value transfers, structured deposits) while ensuring privacy. The time-series and network visualizations reveal syndicate patterns, supporting document forging (Mission 2), anomaly detection (Mission 3), and evidence querying (Mission 4) in Goldweave Port.\n",
    "\n",
    "\n",
    "____________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mission 2: The Document Forge\n",
    "**Objective**: Create realistic synthetic documents critical to a money laundering investigation, including a whistleblower report, bank statements, and suspicious activity report, to build a case against the syndicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from typing import Dict, List, Any\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib.units import inch\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle\n",
    "from reportlab.lib import colors\n",
    "from reportlab.lib.enums import TA_LEFT, TA_CENTER, TA_RIGHT\n",
    "import ollama\n",
    "\n",
    "class FantasyDocumentForge:\n",
    "    \"\"\"Forge synthetic financial investigation documents for the magical kingdom.\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: str = \"./mission2_documents\", seed: int = 42):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        self.ollama_model = \"mistral:7b-instruct-v0.3-q4_0\"  \n",
    "        \n",
    "        # Fantasy kingdom structure (aligned with Mission 1)\n",
    "        self.kingdoms = {\n",
    "            'K1': 'Valdris', 'K2': 'Aethon', 'K3': 'Ironhold', 'K4': 'Mystwood'\n",
    "        }\n",
    "        self.cities = {\n",
    "            'K1C1': 'Crownhaven', 'K1C2': 'Silverdale', 'K1C3': 'Ironbridge', \n",
    "            'K1C4': 'Stormwatch', 'K1C5': 'Goldweave Port',\n",
    "            'K2C1': 'Seafoam', 'K2C2': 'Coral Bay', 'K2C3': 'Tidecrest',\n",
    "            'K2C4': 'Wavebreak', 'K2C5': 'Saltmere',\n",
    "            'K3C1': 'Hammerfall', 'K3C2': 'Anvil Rock', 'K3C3': 'Forge Gate',\n",
    "            'K3C4': 'Steel Harbor', 'K3C5': 'Copper Hill',\n",
    "            'K4C1': 'Moonvale', 'K4C2': 'Starhollow', 'K4C3': 'Shadowpine',\n",
    "            'K4C4': 'Whisperwind', 'K4C5': 'Glimmergrove'\n",
    "        }\n",
    "        self.shell_companies = [\n",
    "            \"Golden Griffin Trading Co.\", \"Dragonscale Import House\", \"Mystic Coin Exchange\",\n",
    "            \"Raven's Rest Consulting\", \"Crystal Crown Merchants\", \"Shadowport Trading Ltd\",\n",
    "            \"Royal Tide Commerce\", \"Emerald Anchor Trading\", \"Goldweave Ventures\"\n",
    "        ]\n",
    "        self.banks = [\n",
    "            \"Royal Bank of Valdris\", \"Crown Treasury\", \"Merchant Guild Financial\",\n",
    "            \"Kingdom Commercial Bank\", \"Golden Vault Banking\", \"Royal Trade Bank\"\n",
    "        ]\n",
    "        self.offshore_locations = [\n",
    "            \"Free Port of Shadowmere\", \"Neutral Isles\", \"Hidden Archipelago\",\n",
    "            \"Merchant Republic Haven\", \"Blackwater Territories\", \"Rogue Trader Isles\"\n",
    "        ]\n",
    "        self.investigators = [\n",
    "            (\"Aldric\", \"Stormwind\", \"Royal Financial Guard\"),\n",
    "            (\"Lyra\", \"Goldbrook\", \"Crown Investigators\"),\n",
    "            (\"Thane\", \"Ironforge\", \"Royal Treasury Guard\"),\n",
    "            (\"Elena\", \"Silverleaf\", \"Kingdom Revenue Service\"),\n",
    "            (\"Marcus\", \"Blackwater\", \"Royal Financial Guard\")\n",
    "        ]\n",
    "\n",
    "    def _generate_text(self, context: str, sentences: int = 3, report_type: str = \"whistleblower\") -> str:\n",
    "        \"\"\"Generate narrative text using Ollama, tailored to report type.\"\"\"\n",
    "        try:\n",
    "            if report_type == \"whistleblower\":\n",
    "                prompt = f\"\"\"\n",
    "                You are a whistleblower in the magical Valdris Kingdom, reporting suspicious financial activities at {context}. Generate a coherent, varied paragraph of approximately {sentences} sentences from an insider’s perspective, focusing on internal observations like management instructions, employee activities, or unusual operations linked to the K1C5 syndicate in Goldweave Port. Include patterns like structured deposits (9,000–9,950 gold pieces), large offshore transfers (25,000–150,000), or shell company dealings. Ensure a formal but urgent tone, avoid repetition, and align with the fantasy AML theme.\n",
    "                \"\"\"\n",
    "            else:  # SAR\n",
    "                prompt = f\"\"\"\n",
    "                You are a Royal Treasury auditor or bank compliance officer in the Valdris Kingdom, filing a Suspicious Activity Report for {context}. Generate a coherent, varied paragraph of approximately {sentences} sentences from an external perspective, focusing on transaction analysis, regulatory findings, or financial patterns linked to the K1C5 syndicate in Goldweave Port. Include patterns like structured deposits (9,000–9,950 gold pieces), large offshore transfers (25,000–150,000), or shell company activities. Ensure a formal, analytical tone, avoid repetition, and align with the fantasy AML theme.\n",
    "                \"\"\"\n",
    "            response = ollama.generate(model=self.ollama_model, prompt=prompt)\n",
    "            return response['response'].strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Ollama generation failed for {report_type}: {e}\")\n",
    "            if report_type == \"whistleblower\":\n",
    "                return f\"Employees at {context} in {self.cities['K1C5']} observed suspicious activities. Management instructed structuring deposits to evade Royal Treasury reporting. Large gold transfers to offshore accounts raised internal concerns.\"\n",
    "            return f\"Analysis of {context} in {self.cities['K1C5']} revealed suspicious financial patterns. Structured gold deposits were designed to avoid regulatory scrutiny. Transaction records showed links to offshore shell companies.\"\n",
    "\n",
    "    def load_mission1_data(self, filepath: str = \"./mission1_synthetic_data.csv\") -> pd.DataFrame:\n",
    "        \"\"\"Load Mission 1 transaction data or generate synthetic transactions.\"\"\"\n",
    "        if filepath and os.path.exists(filepath):\n",
    "            try:\n",
    "                df = pd.read_csv(filepath)\n",
    "                if 'timestamp' in df.columns and 'date' not in df.columns:\n",
    "                    df['date'] = pd.to_datetime(df['timestamp']).dt.date\n",
    "                print(f\"Loaded Mission 1 data from {filepath}\")\n",
    "                return df\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load Mission 1 data: {e}\")\n",
    "        \n",
    "        print(\"Generating synthetic transaction data aligned with Mission 1\")\n",
    "        dates = pd.date_range(start='2024-01-01', end='2024-12-31', freq='D')\n",
    "        transactions = []\n",
    "        syndicate_company = \"Golden Griffin Trading Co.\"\n",
    "        account_numbers = [f\"****{random.randint(1000, 9999)}\" for _ in range(6)]\n",
    "        \n",
    "        for date in dates:\n",
    "            if random.random() < 0.35:\n",
    "                if random.random() < 0.4:\n",
    "                    amount = random.randint(9000, 9900)\n",
    "                    trans_type = \"Gold Deposit\"\n",
    "                elif random.random() < 0.3:\n",
    "                    amount = random.randint(15000, 75000)\n",
    "                    trans_type = random.choice([\"Coin Transfer\", \"Trade Draft\", \"Merchant Exchange\"])\n",
    "                else:\n",
    "                    amount = random.randint(500, 5000)\n",
    "                    trans_type = random.choice([\"Trade Payment\", \"Merchant Draft\", \"Guild Transfer\"])\n",
    "                \n",
    "                transactions.append({\n",
    "                    'date': date,\n",
    "                    'location': 'K1C5',\n",
    "                    'account': random.choice(account_numbers),\n",
    "                    'amount': amount,\n",
    "                    'type': trans_type,\n",
    "                    'description': f\"{syndicate_company} - Trade Operations\",\n",
    "                    'counterparty': syndicate_company if random.random() < 0.6 else f\"Unknown Merchant {random.randint(1, 15)}\"\n",
    "                })\n",
    "        \n",
    "        df = pd.DataFrame(transactions)\n",
    "        print(f\"Generated {len(df)} synthetic transactions\")\n",
    "        return df\n",
    "\n",
    "    def generate_whistleblower_report(self, case_data: Dict[str, Any]) -> str:\n",
    "        \"\"\"Generate a Royal Financial Guard whistleblower report.\"\"\"\n",
    "        filename = f\"whistleblower_report_RG{random.randint(100000, 999999)}.pdf\"\n",
    "        filepath = os.path.join(self.output_dir, filename)\n",
    "        doc = SimpleDocTemplate(filepath, pagesize=letter, topMargin=1*inch, bottomMargin=1*inch)\n",
    "        styles = getSampleStyleSheet()\n",
    "        story = []\n",
    "        \n",
    "        title_style = ParagraphStyle(\n",
    "            'RoyalTitle', parent=styles['Title'], fontSize=16, textColor=colors.darkblue,\n",
    "            spaceAfter=30, alignment=TA_CENTER, fontName='Helvetica-Bold'\n",
    "        )\n",
    "        header_style = ParagraphStyle(\n",
    "            'RoyalHeader', parent=styles['Heading2'], fontSize=12, textColor=colors.black,\n",
    "            spaceBefore=15, spaceAfter=10, fontName='Helvetica-Bold'\n",
    "        )\n",
    "        \n",
    "        story.append(Paragraph(\"CONFIDENTIAL - ROYAL SEAL\", \n",
    "                              ParagraphStyle('Seal', fontSize=8, alignment=TA_RIGHT, textColor=colors.red)))\n",
    "        story.append(Spacer(1, 20))\n",
    "        story.append(Paragraph(\"ROYAL FINANCIAL GUARD\", title_style))\n",
    "        story.append(Paragraph(\"WHISTLEBLOWER INCIDENT REPORT\", title_style))\n",
    "        story.append(Spacer(1, 20))\n",
    "        \n",
    "        report_id = f\"RG-{random.randint(100000, 999999)}\"\n",
    "        report_date = datetime.date.today() - timedelta(days=random.randint(1, 30))\n",
    "        metadata = [\n",
    "            [\"Report ID:\", report_id],\n",
    "            [\"Date Filed:\", report_date.strftime(\"%B %d, %Y\")],\n",
    "            [\"Classification:\", \"ROYAL CONFIDENTIAL\"],\n",
    "            [\"Priority:\", random.choice([\"HIGH\", \"CRITICAL\"])],\n",
    "            [\"Reporting Method:\", \"Anonymous Royal Hotline\"],\n",
    "            [\"Assigned Inspector:\", f\"Inspector {random.choice(self.investigators)[1]}\"]\n",
    "        ]\n",
    "        \n",
    "        metadata_table = Table(metadata, colWidths=[1.5*inch, 3*inch])\n",
    "        metadata_table.setStyle(TableStyle([\n",
    "            ('FONTNAME', (0, 0), (-1, -1), 'Helvetica'),\n",
    "            ('FONTSIZE', (0, 0), (-1, -1), 10),\n",
    "            ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),\n",
    "            ('GRID', (0, 0), (-1, -1), 1, colors.black),\n",
    "            ('BACKGROUND', (0, 0), (0, -1), colors.lightgrey)\n",
    "        ]))\n",
    "        story.append(metadata_table)\n",
    "        story.append(Spacer(1, 30))\n",
    "        \n",
    "        story.append(Paragraph(\"I. SUBJECT INVESTIGATION\", header_style))\n",
    "        target_company = case_data.get('target_company', random.choice(self.shell_companies))\n",
    "        subject_info = [\n",
    "            [\"Trading House:\", target_company],\n",
    "            [\"Business Type:\", \"Import/Export Trading\"],\n",
    "            [\"Location:\", f\"{self.cities['K1C5']}, {self.kingdoms['K1']} Kingdom\"],\n",
    "            [\"Primary Contact:\", f\"{random.choice(['Lord', 'Master', 'Merchant'])} {random.choice(['Aldwin', 'Gareth', 'Thorne'])} {random.choice(['Goldhand', 'Coinsworth', 'Tradewing'])}\"],\n",
    "            [\"Guild Registration:\", f\"TG-{random.randint(10000, 99999)}\"]\n",
    "        ]\n",
    "        \n",
    "        subject_table = Table(subject_info, colWidths=[1.5*inch, 4*inch])\n",
    "        subject_table.setStyle(TableStyle([\n",
    "            ('FONTNAME', (0, 0), (-1, -1), 'Helvetica'),\n",
    "            ('FONTSIZE', (0, 0), (-1, -1), 9),\n",
    "            ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),\n",
    "            ('GRID', (0, 0), (-1, -1), 0.5, colors.grey)\n",
    "        ]))\n",
    "        story.append(subject_table)\n",
    "        story.append(Spacer(1, 20))\n",
    "        \n",
    "        story.append(Paragraph(\"II. REPORTED ALLEGATIONS\", header_style))\n",
    "        context = f\"{target_company} in {self.cities['K1C5']}\"\n",
    "        allegation_text = self._generate_text(context, sentences=4, report_type=\"whistleblower\")\n",
    "        story.append(Paragraph(allegation_text, styles['Normal']))\n",
    "        story.append(Spacer(1, 15))\n",
    "        \n",
    "        observations = [\n",
    "            f\"Gold deposits totaling {random.randint(500, 2000):,} thousand pieces over six months\",\n",
    "            f\"Structured transactions under 10,000 gold threshold on {random.randint(15, 25)} occasions\",\n",
    "            f\"Coin transfers to shell companies in {random.choice(self.offshore_locations)}\",\n",
    "            f\"Trading operations inconsistent with reported merchant activities\",\n",
    "            \"Customer reluctance to provide identification for large transactions\",\n",
    "            f\"Use of multiple accounts at {random.randint(3, 6)} banking houses\"\n",
    "        ]\n",
    "        for i, obs in enumerate(random.sample(observations, random.randint(4, 6)), 1):\n",
    "            story.append(Paragraph(f\"{i}. {obs}\", styles['Normal']))\n",
    "            story.append(Spacer(1, 8))\n",
    "        \n",
    "        story.append(Spacer(1, 15))\n",
    "        story.append(Paragraph(\"III. AVAILABLE EVIDENCE\", header_style))\n",
    "        evidence = [\n",
    "            \"Banking house transaction ledgers\",\n",
    "            \"Internal trading house correspondence\",\n",
    "            \"Surveillance records of unusual activities\",\n",
    "            \"Coin transfer documentation\",\n",
    "            \"Guild registration and licensing records\"\n",
    "        ]\n",
    "        for item in random.sample(evidence, random.randint(3, 5)):\n",
    "            story.append(Paragraph(f\"• {item}\", styles['Normal']))\n",
    "            story.append(Spacer(1, 6))\n",
    "        \n",
    "        story.append(Spacer(1, 30))\n",
    "        story.append(Paragraph(\"Royal Decree 5328 - Financial Crimes Reporting Act\", \n",
    "                              ParagraphStyle('Footer', fontSize=8, alignment=TA_CENTER, fontStyle='italic')))\n",
    "        doc.build(story)\n",
    "        print(f\"Generated whistleblower report: {filepath}\")\n",
    "        return filepath\n",
    "\n",
    "    def generate_bank_statement(self, transaction_data: pd.DataFrame) -> str:\n",
    "        \"\"\"Generate a banking house statement for syndicate transactions.\"\"\"\n",
    "        filename = f\"bank_statement_{random.randint(100000, 999999)}.pdf\"\n",
    "        filepath = os.path.join(self.output_dir, filename)\n",
    "        doc = SimpleDocTemplate(filepath, pagesize=letter, topMargin=0.5*inch)\n",
    "        styles = getSampleStyleSheet()\n",
    "        story = []\n",
    "        \n",
    "        bank_name = random.choice(self.banks)\n",
    "        account_holder = \"Golden Griffin Trading Co.\"\n",
    "        account_number = f\"****-****-{random.randint(1000, 9999)}\"\n",
    "        statement_date = datetime.date.today().replace(day=1) - timedelta(days=1)\n",
    "        \n",
    "        header_style = ParagraphStyle(\n",
    "            'BankHeader', fontSize=18, textColor=colors.darkblue,\n",
    "            fontName='Helvetica-Bold', alignment=TA_CENTER, spaceAfter=20\n",
    "        )\n",
    "        story.append(Paragraph(bank_name.upper(), header_style))\n",
    "        story.append(Paragraph(f\"{self.cities['K1C5']}, {self.kingdoms['K1']} Kingdom\", \n",
    "                              ParagraphStyle('Location', fontSize=10, alignment=TA_CENTER, spaceAfter=30)))\n",
    "        \n",
    "        account_info = [\n",
    "            [\"Account Holder:\", account_holder],\n",
    "            [\"Account Number:\", account_number],\n",
    "            [\"Statement Period:\", f\"{statement_date.replace(day=1).strftime('%m/%d/%Y')} - {statement_date.strftime('%m/%d/%Y')}\"],\n",
    "            [\"Account Type:\", \"Merchant Trading Account\"],\n",
    "            [\"Location:\", f\"{self.cities['K1C5']} Branch\"]\n",
    "        ]\n",
    "        info_table = Table(account_info, colWidths=[1.5*inch, 4*inch])\n",
    "        info_table.setStyle(TableStyle([\n",
    "            ('FONTNAME', (0, 0), (-1, -1), 'Helvetica'),\n",
    "            ('FONTSIZE', (0, 0), (-1, -1), 9),\n",
    "            ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),\n",
    "            ('BACKGROUND', (0, 0), (-1, -1), colors.lightgrey),\n",
    "            ('GRID', (0, 0), (-1, -1), 1, colors.black)\n",
    "        ]))\n",
    "        story.append(info_table)\n",
    "        story.append(Spacer(1, 20))\n",
    "        \n",
    "        story.append(Paragraph(\"ACCOUNT SUMMARY\", \n",
    "                              ParagraphStyle('SectionTitle', fontSize=12, fontName='Helvetica-Bold', spaceAfter=10)))\n",
    "        \n",
    "        monthly_data = transaction_data[\n",
    "            pd.to_datetime(transaction_data['date']).dt.month == statement_date.month\n",
    "        ] if len(transaction_data) > 0 and 'date' in transaction_data.columns else pd.DataFrame()\n",
    "        deposits = monthly_data[monthly_data['amount'] > 0]['amount'].sum() if len(monthly_data) > 0 else random.randint(500000, 1500000)\n",
    "        withdrawals = abs(monthly_data[monthly_data['amount'] < 0]['amount'].sum()) if len(monthly_data) > 0 else random.randint(300000, 800000)\n",
    "        beginning_balance = random.randint(50000, 200000)\n",
    "        ending_balance = beginning_balance + deposits - withdrawals\n",
    "        \n",
    "        summary_data = [\n",
    "            [\"Beginning Balance:\", f\"{beginning_balance:,} GP\"],\n",
    "            [\"Total Deposits:\", f\"{deposits:,} GP\"],\n",
    "            [\"Total Withdrawals:\", f\"{withdrawals:,} GP\"],\n",
    "            [\"Banking Fees:\", f\"{random.randint(25, 150)} GP\"],\n",
    "            [\"Ending Balance:\", f\"{ending_balance:,} GP\"]\n",
    "        ]\n",
    "        summary_table = Table(summary_data, colWidths=[2*inch, 1.5*inch])\n",
    "        summary_table.setStyle(TableStyle([\n",
    "            ('FONTNAME', (0, 0), (-1, -1), 'Helvetica'),\n",
    "            ('FONTSIZE', (0, 0), (-1, -1), 10),\n",
    "            ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),\n",
    "            ('ALIGN', (1, 0), (1, -1), 'RIGHT'),\n",
    "            ('LINEBELOW', (0, -1), (-1, -1), 2, colors.black),\n",
    "            ('FONTNAME', (0, -1), (-1, -1), 'Helvetica-Bold')\n",
    "        ]))\n",
    "        story.append(summary_table)\n",
    "        story.append(Spacer(1, 30))\n",
    "        \n",
    "        story.append(Paragraph(\"TRANSACTION LEDGER\", \n",
    "                              ParagraphStyle('SectionTitle', fontSize=12, fontName='Helvetica-Bold', spaceAfter=15)))\n",
    "        transaction_details = [[\"Date\", \"Description\", \"Withdrawal\", \"Deposit\", \"Balance\"]]\n",
    "        current_balance = beginning_balance\n",
    "        suspicious_transactions = []\n",
    "        \n",
    "        for _ in range(random.randint(8, 15)):\n",
    "            amount = random.randint(9000, 9950)\n",
    "            date_obj = statement_date.replace(day=random.randint(1, 28))\n",
    "            suspicious_transactions.append({\n",
    "                'date': date_obj,\n",
    "                'description': f\"GOLD DEPOSIT - {account_holder[:20]}\",\n",
    "                'amount': amount,\n",
    "                'type': 'deposit'\n",
    "            })\n",
    "        for _ in range(random.randint(3, 8)):\n",
    "            amount = random.randint(25000, 150000)\n",
    "            date_obj = statement_date.replace(day=random.randint(1, 28))\n",
    "            origin = random.choice(self.offshore_locations)\n",
    "            suspicious_transactions.append({\n",
    "                'date': date_obj,\n",
    "                'description': f\"COIN TRANSFER FROM {origin[:15]}\",\n",
    "                'amount': amount,\n",
    "                'type': 'deposit'\n",
    "            })\n",
    "        for _ in range(random.randint(5, 12)):\n",
    "            amount = random.choice([25000, 50000, 75000, 100000])\n",
    "            date_obj = statement_date.replace(day=random.randint(1, 28))\n",
    "            suspicious_transactions.append({\n",
    "                'date': date_obj,\n",
    "                'description': f\"TRADE DRAFT - BUSINESS EXPENSE\",\n",
    "                'amount': amount,\n",
    "                'type': 'withdrawal'\n",
    "            })\n",
    "        \n",
    "        suspicious_transactions.sort(key=lambda x: x['date'])\n",
    "        for trans in suspicious_transactions:\n",
    "            if trans['type'] == 'deposit':\n",
    "                current_balance += trans['amount']\n",
    "                transaction_details.append([\n",
    "                    trans['date'].strftime(\"%m/%d\"),\n",
    "                    trans['description'],\n",
    "                    \"\",\n",
    "                    f\"{trans['amount']:,} GP\",\n",
    "                    f\"{current_balance:,} GP\"\n",
    "                ])\n",
    "            else:\n",
    "                current_balance -= trans['amount']\n",
    "                transaction_details.append([\n",
    "                    trans['date'].strftime(\"%m/%d\"),\n",
    "                    trans['description'],\n",
    "                    f\"{trans['amount']:,} GP\",\n",
    "                    \"\",\n",
    "                    f\"{current_balance:,} GP\"\n",
    "                ])\n",
    "        \n",
    "        trans_table = Table(transaction_details, colWidths=[0.8*inch, 3.2*inch, 1*inch, 1*inch, 1*inch])\n",
    "        trans_table.setStyle(TableStyle([\n",
    "            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
    "            ('FONTNAME', (0, 1), (-1, -1), 'Helvetica'),\n",
    "            ('FONTSIZE', (0, 0), (-1, -1), 8),\n",
    "            ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n",
    "            ('ALIGN', (2, 0), (-1, -1), 'RIGHT'),\n",
    "            ('GRID', (0, 0), (-1, -1), 0.5, colors.black),\n",
    "            ('ROWBACKGROUNDS', (0, 1), (-1, -1), [colors.white, colors.beige])\n",
    "        ]))\n",
    "        story.append(trans_table)\n",
    "        \n",
    "        story.append(Spacer(1, 30))\n",
    "        notice_text = f\"\"\"\n",
    "        ROYAL NOTICE: This account is under enhanced monitoring per Royal Treasury Decree. \n",
    "        Large transactions may be reported to the Royal Financial Guard. Contact the compliance officer \n",
    "        at the {self.cities['K1C5']} branch for inquiries.\n",
    "        \"\"\"\n",
    "        story.append(Paragraph(notice_text, \n",
    "                              ParagraphStyle('Notice', fontSize=8, fontStyle='italic', textColor=colors.red, leftIndent=20, rightIndent=20)))\n",
    "        doc.build(story)\n",
    "        print(f\"Generated bank statement: {filepath}\")\n",
    "        return filepath\n",
    "\n",
    "    def generate_suspicious_activity_report(self, case_data: Dict[str, Any]) -> str:\n",
    "        \"\"\"Generate a Royal Treasury Suspicious Activity Report.\"\"\"\n",
    "        filename = f\"SAR_{random.randint(100000, 999999)}.pdf\"\n",
    "        filepath = os.path.join(self.output_dir, filename)\n",
    "        doc = SimpleDocTemplate(filepath, pagesize=letter, topMargin=0.75*inch)\n",
    "        styles = getSampleStyleSheet()\n",
    "        story = []\n",
    "        \n",
    "        header_style = ParagraphStyle(\n",
    "            'SARHeader', fontSize=14, fontName='Helvetica-Bold', alignment=TA_CENTER,\n",
    "            textColor=colors.darkred, spaceAfter=20\n",
    "        )\n",
    "        story.append(Paragraph(\"SUSPICIOUS ACTIVITY REPORT\", header_style))\n",
    "        story.append(Paragraph(\"Royal Treasury Form RT-111\", \n",
    "                              ParagraphStyle('FormNumber', fontSize=10, alignment=TA_CENTER, spaceAfter=30)))\n",
    "        \n",
    "        sar_number = f\"RT-{random.randint(1000000, 9999999)}\"\n",
    "        filing_date = datetime.date.today() - timedelta(days=random.randint(1, 15))\n",
    "        identification_data = [\n",
    "            [\"SAR Number:\", sar_number],\n",
    "            [\"Filing Date:\", filing_date.strftime(\"%m/%d/%Y\")],\n",
    "            [\"Filing Institution:\", random.choice(self.banks)],\n",
    "            [\"Branch Location:\", f\"{self.cities['K1C5']}, {self.kingdoms['K1']} Kingdom\"],\n",
    "            [\"Guild Registration:\", f\"BG-{random.randint(1000000, 9999999)}\"]\n",
    "        ]\n",
    "        id_table = Table(identification_data, colWidths=[1.5*inch, 3*inch])\n",
    "        id_table.setStyle(TableStyle([\n",
    "            ('FONTNAME', (0, 0), (-1, -1), 'Helvetica'),\n",
    "            ('FONTSIZE', (0, 0), (-1, -1), 9),\n",
    "            ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),\n",
    "            ('GRID', (0, 0), (-1, -1), 1, colors.black),\n",
    "            ('BACKGROUND', (0, 0), (0, -1), colors.lightgrey)\n",
    "        ]))\n",
    "        story.append(id_table)\n",
    "        story.append(Spacer(1, 20))\n",
    "        \n",
    "        story.append(Paragraph(\"PART I - SUBJECT INVESTIGATION\", \n",
    "                              ParagraphStyle('PartHeader', fontSize=11, fontName='Helvetica-Bold', spaceAfter=10)))\n",
    "        subject_company = case_data.get('target_company', \"Golden Griffin Trading Co.\")\n",
    "        subject_data = [\n",
    "            [\"Subject Name:\", subject_company],\n",
    "            [\"Business Address:\", f\"Harbor District, {self.cities['K1C5']}\"],\n",
    "            [\"Kingdom Location:\", f\"{self.kingdoms['K1']} Kingdom\"],\n",
    "            [\"Guild Registration:\", f\"TG-{random.randint(1000000, 9999999)}\"],\n",
    "            [\"Account Number:\", f\"****-{random.randint(1000, 9999)}\"],\n",
    "            [\"Date Established:\", f\"{random.randint(1, 12)}/{random.randint(1, 28)}/{random.randint(2015, 2020)}\"]\n",
    "        ]\n",
    "        subject_table = Table(subject_data, colWidths=[1.8*inch, 3.5*inch])\n",
    "        subject_table.setStyle(TableStyle([\n",
    "            ('FONTNAME', (0, 0), (-1, -1), 'Helvetica'),\n",
    "            ('FONTSIZE', (0, 0), (-1, -1), 9),\n",
    "            ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),\n",
    "            ('GRID', (0, 0), (-1, -1), 0.5, colors.grey)\n",
    "        ]))\n",
    "        story.append(subject_table)\n",
    "        story.append(Spacer(1, 20))\n",
    "        \n",
    "        story.append(Paragraph(\"PART II - SUSPICIOUS ACTIVITY DETAILS\", \n",
    "                              ParagraphStyle('PartHeader', fontSize=11, fontName='Helvetica-Bold', spaceAfter=10)))\n",
    "        activity_start = filing_date - timedelta(days=random.randint(90, 180))\n",
    "        activity_end = filing_date - timedelta(days=random.randint(1, 30))\n",
    "        activity_data = [\n",
    "            [\"Activity Period:\", f\"{activity_start.strftime('%m/%d/%Y')} - {activity_end.strftime('%m/%d/%Y')}\"],\n",
    "            [\"Total Amount:\", f\"{random.randint(750, 2500):,},000 Gold Pieces\"],\n",
    "            [\"Transaction Count:\", f\"{random.randint(25, 75)}\"],\n",
    "            [\"Activity Type:\", \"Structured Deposits/Money Laundering\"],\n",
    "            [\"Primary Location:\", f\"{self.cities['K1C5']} Branch\"],\n",
    "            [\"Prior SAR Filed:\", random.choice([\"Yes\", \"No\"])]\n",
    "        ]\n",
    "        activity_table = Table(activity_data, colWidths=[2*inch, 3*inch])\n",
    "        activity_table.setStyle(TableStyle([\n",
    "            ('FONTNAME', (0, 0), (-1, -1), 'Helvetica'),\n",
    "            ('FONTSIZE', (0, 0), (-1, -1), 9),\n",
    "            ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),\n",
    "            ('GRID', (0, 0), (-1, -1), 0.5, colors.grey)\n",
    "        ]))\n",
    "        story.append(activity_table)\n",
    "        story.append(Spacer(1, 20))\n",
    "        \n",
    "        story.append(Paragraph(\"PART III - INVESTIGATION NARRATIVE\", \n",
    "                              ParagraphStyle('PartHeader', fontSize=11, fontName='Helvetica-Bold', spaceAfter=10)))\n",
    "        context = f\"{subject_company} in {self.cities['K1C5']}\"\n",
    "        narrative_text = self._generate_text(context, sentences=6, report_type=\"sar\")\n",
    "        story.append(Paragraph(narrative_text, styles['Normal']))\n",
    "        story.append(Spacer(1, 15))\n",
    "        \n",
    "        story.append(Paragraph(\"Suspicious Activity Indicators:\", \n",
    "                              ParagraphStyle('SubHeader', fontSize=10, fontName='Helvetica-Bold', spaceAfter=8)))\n",
    "        red_flags = [\n",
    "            \"Multiple gold deposits under 10,000 piece threshold\",\n",
    "            \"Customer reluctance to provide source documentation\",\n",
    "            \"Trading activity inconsistent with gold volume\",\n",
    "            \"Multiple accounts used to fragment transactions\",\n",
    "            \"Connections to high-risk territories via coin transfers\",\n",
    "            \"Unusual gold transportation and handling\"\n",
    "        ]\n",
    "        for flag in red_flags:\n",
    "            story.append(Paragraph(f\"• {flag}\", styles['Normal']))\n",
    "            story.append(Spacer(1, 4))\n",
    "        \n",
    "        story.append(Spacer(1, 20))\n",
    "        story.append(Paragraph(\"PART IV - ROYAL ENFORCEMENT CONTACT\", \n",
    "                              ParagraphStyle('PartHeader', fontSize=11, fontName='Helvetica-Bold', spaceAfter=10)))\n",
    "        investigator = random.choice(self.investigators)\n",
    "        contact_data = [\n",
    "            [\"Agency Notified:\", investigator[2]],\n",
    "            [\"Contact Name:\", f\"Inspector {investigator[0]} {investigator[1]}\"],\n",
    "            [\"Royal Station:\", f\"{self.cities['K1C1']} Headquarters\"],\n",
    "            [\"Date Contacted:\", (filing_date + timedelta(days=1)).strftime(\"%m/%d/%Y\")],\n",
    "            [\"Badge Number:\", f\"RG-{random.randint(1000, 9999)}\"]\n",
    "        ]\n",
    "        contact_table = Table(contact_data, colWidths=[1.5*inch, 2.5*inch])\n",
    "        contact_table.setStyle(TableStyle([\n",
    "            ('FONTNAME', (0, 0), (-1, -1), 'Helvetica'),\n",
    "            ('FONTSIZE', (0, 0), (-1, -1), 9),\n",
    "            ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),\n",
    "            ('GRID', (0, 0), (-1, -1), 0.5, colors.grey)\n",
    "        ]))\n",
    "        story.append(contact_table)\n",
    "        story.append(Spacer(1, 30))\n",
    "        \n",
    "        story.append(Paragraph(\"ROYAL CERTIFICATION\", \n",
    "                              ParagraphStyle('CertHeader', fontSize=11, fontName='Helvetica-Bold', \n",
    "                                           alignment=TA_CENTER, spaceAfter=15)))\n",
    "        cert_text = f\"\"\"\n",
    "        I certify that this Suspicious Activity Report is true and accurate to the best of my knowledge. \n",
    "        Providing false information may result in penalties under Royal Treasury Law.\n",
    "        \n",
    "        ________________________________                    Date: {filing_date.strftime('%m/%d/%Y')}\n",
    "        Banking House Compliance Officer\n",
    "        \n",
    "        {random.choice(['Master Aldwin Goldkeeper', 'Lord Gareth Coinwatch', 'Dame Sarah Vaultguard'])}\n",
    "        Royal Treasury Compliance Officer\n",
    "        \"\"\"\n",
    "        story.append(Paragraph(cert_text, styles['Normal']))\n",
    "        story.append(Spacer(1, 20))\n",
    "        story.append(Paragraph(\"Royal Treasury Form RT-111 - Suspicious Activity Report\", \n",
    "                              ParagraphStyle('Footer', fontSize=8, alignment=TA_CENTER, \n",
    "                                           textColor=colors.grey)))\n",
    "        doc.build(story)\n",
    "        print(f\"Generated Suspicious Activity Report: {filepath}\")\n",
    "        return filepath\n",
    "\n",
    "    def generate_all_documents(self, mission1_data_path: str = \"./mission1_synthetic_data.csv\") -> List[str]:\n",
    "        \"\"\"Generate investigation documents for the K1C5 syndicate.\"\"\"\n",
    "        print(\"\\n🏰 FANTASY KINGDOM AML - MISSION 2: Document Forging\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"\\nTarget: Golden Griffin Trading Co., Goldweave Port, Valdris Kingdom\")\n",
    "        \n",
    "        transaction_data = self.load_mission1_data(mission1_data_path)\n",
    "        case_data = {\n",
    "            'target_company': 'Golden Griffin Trading Co.',\n",
    "            'base_location': 'K1C5',\n",
    "            'investigation_period': {\n",
    "                'start': datetime.date.today() - timedelta(days=180),\n",
    "                'end': datetime.date.today() - timedelta(days=30)\n",
    "            },\n",
    "            'total_suspicious_amount': random.randint(1000000, 3000000),\n",
    "            'primary_investigator': random.choice(self.investigators)\n",
    "        }\n",
    "        \n",
    "        generated_files = []\n",
    "        generated_files.append(self.generate_whistleblower_report(case_data))\n",
    "        generated_files.append(self.generate_bank_statement(transaction_data))\n",
    "        generated_files.append(self.generate_suspicious_activity_report(case_data))\n",
    "        \n",
    "        print(\"\\nGenerated Documents:\")\n",
    "        document_types = [\"Whistleblower Report\", \"Bank Statement\", \"Suspicious Activity Report\"]\n",
    "        for i, file in enumerate(generated_files, 1):\n",
    "            print(f\"{i}. {document_types[i-1]}: {os.path.basename(file)}\")\n",
    "        \n",
    "        self._generate_mission_summary(generated_files, case_data)\n",
    "        print(\"\\nMission 2 Complete!\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Documents saved to: {self.output_dir}/\")\n",
    "        print(f\"Investigation Focus: {case_data['target_company']} in {self.cities[case_data['base_location']]}\")\n",
    "        return generated_files\n",
    "    \n",
    "    def _generate_mission_summary(self, generated_files: List[str], case_data: Dict[str, Any]):\n",
    "        \"\"\"Generate a summary of Mission 2 for investigators.\"\"\"\n",
    "        summary_file = os.path.join(self.output_dir, \"mission2_summary.txt\")\n",
    "        with open(summary_file, 'w') as f:\n",
    "            f.write(\"MISSION 2: FANTASY AML DOCUMENT FORGING\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "            f.write(\"INVESTIGATION TARGET:\\n\")\n",
    "            f.write(f\"• Syndicate: {case_data['target_company']}\\n\")\n",
    "            f.write(f\"• Location: {self.cities[case_data['base_location']]}, {self.kingdoms['K1']} Kingdom\\n\")\n",
    "            f.write(f\"• Period: {case_data['investigation_period']['start']} to {case_data['investigation_period']['end']}\\n\")\n",
    "            f.write(f\"• Suspicious Amount: {case_data['total_suspicious_amount']:,} Gold Pieces\\n\\n\")\n",
    "            f.write(\"TECHNIQUES USED:\\n\")\n",
    "            f.write(\"• Ollama: Generated coherent, domain-specific investigation narratives\\n\")\n",
    "            f.write(\"• ReportLab: Formatted professional PDF documents\\n\")\n",
    "            f.write(\"• Fantasy Theming: Adapted to magical kingdom setting\\n\")\n",
    "            f.write(\"• Data Integration: Aligned with Mission 1 transaction patterns\\n\\n\")\n",
    "            f.write(\"GENERATED DOCUMENTS:\\n\")\n",
    "            for i, file in enumerate(generated_files, 1):\n",
    "                f.write(f\"{i}. {os.path.basename(file)}\\n\")\n",
    "            f.write(\"\\nMONEY LAUNDERING PATTERNS:\\n\")\n",
    "            f.write(\"• Structured deposits under 10,000 gold pieces\\n\")\n",
    "            f.write(\"• Large transfers from high-risk offshore territories\\n\")\n",
    "            f.write(\"• Inconsistent trading operations\\n\")\n",
    "            f.write(\"• Multiple banking relationships\\n\")\n",
    "            f.write(\"• Round-number withdrawals\\n\\n\")\n",
    "            f.write(\"INVESTIGATION OUTCOMES:\\n\")\n",
    "            f.write(\"• Identification of financial crime indicators\\n\")\n",
    "            f.write(\"• Understanding of regulatory reporting\\n\")\n",
    "            f.write(\"• Analysis of investigation documents\\n\")\n",
    "            f.write(\"• Cross-document consistency for AML investigations\\n\")\n",
    "        print(f\"Generated mission summary: {summary_file}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Execute Mission 2 to forge AML investigation documents.\"\"\"\n",
    "    doc_forge = FantasyDocumentForge()\n",
    "    generated_files = doc_forge.generate_all_documents(\"./mission1_synthetic_data.csv\")\n",
    "    return generated_files\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Mission 2 Review\n",
    "\n",
    "In Mission 2, you wield the `FantasyDocumentForge` to conjure synthetic investigation documents that expose the money laundering schemes of the K1C5 syndicate in Goldweave Port, Valdris Kingdom. Building on Mission 1’s enchanted ledger (`./mission1_synthetic_data.csv`), you craft three critical documents: a whistleblower report, a bank statement, and a Suspicious Activity Report (SAR). These documents, stored in `./mission2_documents/`, reveal the syndicate’s illicit activities with coherent, varied narratives powered by Ollama, preparing the Valdris Council for Missions 3 and 4.\n",
    "\n",
    "## Forging the Evidence\n",
    "\n",
    "The `FantasyDocumentForge` employs three mystical techniques to unmask the K1C5 syndicate:\n",
    "\n",
    "### 1. Narrative Conjuring\n",
    "- **Technique**: Harnesses Ollama’s language model (e.g., Mistral) to generate coherent, non-repetitive investigation narratives tailored to AML patterns.\n",
    "- **Features**: Produces professional text for whistleblower reports and SARs, detailing suspicious activities like structured gold deposits (9,000–9,950 pieces), offshore transfers (25,000–150,000), and shell company operations.\n",
    "- **Output**: Varied, context-aware allegations centered on the K1C5 syndicate in Goldweave Port.\n",
    "\n",
    "**Purpose**: Embeds authentic, diverse narrative evidence in investigative documents.\n",
    "\n",
    "### 2. Document Crafting\n",
    "- **Technique**: Uses ReportLab to create professional PDF documents with structured tables, headers, and fantasy-themed styling.\n",
    "- **Features**: Includes syndicate details (e.g., Golden Griffin Trading Co.), Goldweave Port location, and red flags like multiple accounts and inconsistent trading activity.\n",
    "- **Output**: Three PDFs: whistleblower report, bank statement (aligned with Mission 1 transactions), and SAR, saved to `./mission2_documents/`.\n",
    "\n",
    "**Purpose**: Generates formal documents mirroring real-world AML investigations.\n",
    "\n",
    "### 3. Evidence Integration\n",
    "- **Technique**: Synchronizes with Mission 1’s transaction data, mapping `timestamp` to `date`, `amount`, and `transaction_type` for consistency.\n",
    "- **Features**: Embeds patterns like structured deposits, large offshore transfers, and round-number withdrawals.\n",
    "- **Output**: Documents reflecting K1C5’s money laundering tactics, with a summary (`mission2_summary.txt`) for investigators.\n",
    "\n",
    "**Purpose**: Links documents to Mission 1’s data, exposing the syndicate’s financial network.\n",
    "\n",
    "## Why It Matters\n",
    "These forged documents, powered by Ollama’s modern language generation, illuminate the K1C5 syndicate’s schemes with professional, non-repetitive narratives. They align with Mission 1’s synthetic transactions and enable Mission 3’s anomaly detection and Mission 4’s chatbot-driven evidence queries. By mastering document forging, you empower the Valdris Council to combat financial crime in Goldweave Port.\n",
    "_______________________________________________________\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mission 3: The Prophecy Familiar\n",
    "**Objective**: Train an anomaly detection mythical beast on synthetic data patterns to identify suspicious financial activities, strengthening the investigation against the money laundering syndicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_score\n",
    "import networkx as nx\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class TransactionGraphBuilder:\n",
    "    \"\"\"\n",
    "    Builds graph representations for AML detection, enhanced for K1C5 syndicate patterns.\n",
    "    Incorporates clustering coefficients and time-series features from Mission 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, time_window_hours=24, min_edge_weight=0.01):\n",
    "        self.time_window_hours = time_window_hours\n",
    "        self.min_edge_weight = min_edge_weight\n",
    "        self.flagged_accounts = {'K1C5', 'K2C5', 'K3C1', 'K4C3'}\n",
    "        \n",
    "    def create_temporal_graph(self, df, window_start=0, window_size=300):\n",
    "        window_df = df.iloc[window_start:window_start + window_size].copy()\n",
    "        \n",
    "        sender_col, receiver_col = ('customer_id', 'merchant_id') if 'customer_id' in df.columns else ('sender_location', 'receiver_location')\n",
    "        accounts = set(window_df[sender_col].unique()) | set(window_df[receiver_col].unique())\n",
    "        \n",
    "        if len(accounts) < 5:\n",
    "            return None, None\n",
    "        account_to_idx = {acc: idx for idx, acc in enumerate(sorted(accounts))}\n",
    "        \n",
    "        G = nx.DiGraph()\n",
    "        for _, row in window_df.iterrows():\n",
    "            G.add_edge(row[sender_col], row[receiver_col], weight=row['amount'])\n",
    "        \n",
    "        clustering_coeffs = nx.clustering(G, weight=None)\n",
    "        \n",
    "        edge_index = []\n",
    "        edge_features = []\n",
    "        \n",
    "        grouped = window_df.groupby([sender_col, receiver_col])\n",
    "        \n",
    "        for (sender, receiver), group in grouped:\n",
    "            if sender in account_to_idx and receiver in account_to_idx:\n",
    "                edge_index.append([account_to_idx[sender], account_to_idx[receiver]])\n",
    "                edge_feat = [\n",
    "                    group['amount'].sum(),\n",
    "                    group['amount'].mean(),\n",
    "                    len(group),\n",
    "                    group['amount'].std() if len(group) > 1 else 0,\n",
    "                    group['transaction_hour'].std() if len(group) > 1 else 0,\n",
    "                    group['cross_border'].mean(),\n",
    "                    group['cash_equivalent'].mean(),\n",
    "                    group['merchant_risk_score'].mean(),\n",
    "                    group['transactions_last_24h'].mean(),\n",
    "                    3 if sender == 'K1C5' or receiver == 'K1C5' else 1 if sender in self.flagged_accounts or receiver in self.flagged_accounts else 0,  # Boost K1C5\n",
    "                    group.get('velocity_24h', pd.Series([0] * len(group))).mean(),\n",
    "                ]\n",
    "                edge_features.append(edge_feat)\n",
    "        \n",
    "        node_features = []\n",
    "        node_labels = []\n",
    "        \n",
    "        for account in sorted(accounts):\n",
    "            sent = window_df[window_df[sender_col] == account]\n",
    "            received = window_df[window_df[receiver_col] == account]\n",
    "            \n",
    "            total_txns = len(sent) + len(received)\n",
    "            ml_sent = len(sent[sent['is_money_laundering'] == 1]) if len(sent) > 0 else 0\n",
    "            ml_received = len(received[received['is_money_laundering'] == 1]) if len(received) > 0 else 0\n",
    "            ml_count = ml_sent + ml_received\n",
    "            flagged_interactions = len(set(sent[receiver_col]) & self.flagged_accounts) + \\\n",
    "                                  len(set(received[sender_col]) & self.flagged_accounts)\n",
    "            k1c5_interactions = len(set(sent[receiver_col]) & {'K1C5'}) + len(set(received[sender_col]) & {'K1C5'})\n",
    "            \n",
    "            node_feat = [\n",
    "                len(sent),\n",
    "                len(received),\n",
    "                sent['amount'].sum() if len(sent) > 0 else 0,\n",
    "                received['amount'].sum() if len(received) > 0 else 0,\n",
    "                (received['amount'].sum() - sent['amount'].sum()) if total_txns > 0 else 0,\n",
    "                sent['account_age_days'].mean() if len(sent) > 0 else 0,\n",
    "                3 if account == 'K1C5' else 1 if account in self.flagged_accounts else 0,  # Boost K1C5\n",
    "                ml_count * 2.0,\n",
    "                flagged_interactions * 3.0,  # Increased weight\n",
    "                k1c5_interactions * 7.0,  # New K1C5-specific feature\n",
    "                sent.get('velocity_24h', pd.Series([0] * len(sent))).mean() if len(sent) > 0 else 0,\n",
    "                received.get('velocity_24h', pd.Series([0] * len(received))).mean() if len(received) > 0 else 0,\n",
    "                clustering_coeffs.get(account, 0.0),\n",
    "            ]\n",
    "            node_features.append(node_feat)\n",
    "            node_labels.append(1 if ml_count > 0 or k1c5_interactions > 0 else 0)  # Include K1C5 interactions in labels\n",
    "        \n",
    "        if len(edge_index) < 5:\n",
    "            return None, None\n",
    "        \n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t()\n",
    "        edge_attr = torch.tensor(edge_features, dtype=torch.float32)\n",
    "        x = torch.tensor(node_features, dtype=torch.float32)\n",
    "        y = torch.tensor(node_labels, dtype=torch.float32)\n",
    "        \n",
    "        return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y), account_to_idx\n",
    "    \n",
    "    def create_dataset(self, df, window_size=300, stride=20):\n",
    "        graphs = []\n",
    "        for start in range(0, len(df) - window_size + 1, stride):\n",
    "            graph, _ = self.create_temporal_graph(df, start, window_size)\n",
    "            if graph is not None and graph.edge_index.shape[1] > 0:\n",
    "                graphs.append(graph)\n",
    "        return graphs\n",
    "\n",
    "class AMLGAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Autoencoder for anomalous subgraph detection, fine-tuned for K1C5 syndicates.\n",
    "    \"\"\"\n",
    "    def __init__(self, node_features, edge_features, hidden_dim=32, latent_dim=16):\n",
    "        super(AMLGAE, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.ModuleList([\n",
    "            GCNConv(node_features, hidden_dim),\n",
    "            GCNConv(hidden_dim, latent_dim),\n",
    "        ])\n",
    "        \n",
    "        self.decoder = nn.ModuleList([\n",
    "            GCNConv(latent_dim, hidden_dim),\n",
    "            GCNConv(hidden_dim, node_features),\n",
    "        ])\n",
    "        \n",
    "        self.edge_reconstructor = nn.Sequential(\n",
    "            nn.Linear(latent_dim * 2 + edge_features, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "        \n",
    "        self.supervised_head = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "    \n",
    "    def encode(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.encoder):\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x) if i < len(self.encoder) - 1 else x\n",
    "        return x\n",
    "    \n",
    "    def decode_nodes(self, z, edge_index):\n",
    "        for i, conv in enumerate(self.decoder):\n",
    "            z = conv(z, edge_index)\n",
    "            z = F.relu(z) if i < len(self.decoder) - 1 else z\n",
    "        return z\n",
    "    \n",
    "    def decode_edges(self, z, edge_index, edge_attr):\n",
    "        row, col = edge_index\n",
    "        edge_inputs = torch.cat([z[row], z[col], edge_attr], dim=1)\n",
    "        return self.edge_reconstructor(edge_inputs)\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        z = self.encode(x, edge_index)\n",
    "        node_recon = self.decode_nodes(z, edge_index)\n",
    "        edge_recon = self.decode_edges(z, edge_index, edge_attr) if edge_attr is not None else None\n",
    "        supervised_score = self.supervised_head(z)\n",
    "        return z, node_recon, edge_recon, supervised_score\n",
    "\n",
    "class AMLDetectionSystem:\n",
    "    \"\"\"\n",
    "    AML system using GAE with semi-supervised fine-tuning for K1C5 syndicate detection.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=32, latent_dim=16, learning_rate=0.001):\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = None\n",
    "        self.graph_builder = TransactionGraphBuilder()\n",
    "        self.scaler_node = None\n",
    "        self.scaler_edge = None\n",
    "        self.threshold = None\n",
    "        self.val_recon_errors = None\n",
    "        self.val_error_mean = None\n",
    "        self.val_error_std = None\n",
    "    \n",
    "    def train(self, df, epochs=50, batch_size=16, val_split=0.2):\n",
    "        print(\"🔨 Building transaction graphs...\")\n",
    "        graphs = self.graph_builder.create_dataset(df, window_size=300, stride=20)\n",
    "        graphs = [g for g in graphs if g is not None and g.edge_index.shape[1] > 0]\n",
    "        print(f\"✓ Created {len(graphs)} temporal graph snapshots\")\n",
    "        if graphs:\n",
    "            print(f\"Sample graph - Nodes: {graphs[0].x.shape}, Edges: {graphs[0].edge_index.shape}, Edge attr: {graphs[0].edge_attr.shape}\")\n",
    "        \n",
    "        train_size = int(len(graphs) * (1 - val_split))\n",
    "        train_graphs = graphs[:train_size]\n",
    "        val_graphs = graphs[train_size:]\n",
    "        \n",
    "        all_x_train = np.vstack([g.x.numpy() for g in train_graphs]) if train_graphs else np.empty((0, graphs[0].x.shape[1]))\n",
    "        self.scaler_node = StandardScaler().fit(all_x_train) if len(all_x_train) > 0 else None\n",
    "        all_edge_train = np.vstack([g.edge_attr.numpy() for g in train_graphs if g.edge_attr.shape[0] > 0]) if any(g.edge_attr.shape[0] > 0 for g in train_graphs) else np.empty((0, 11))\n",
    "        self.scaler_edge = StandardScaler().fit(all_edge_train) if len(all_edge_train) > 0 else None\n",
    "        \n",
    "        for graphs_list in [train_graphs, val_graphs]:\n",
    "            for g in graphs_list:\n",
    "                if self.scaler_node is not None:\n",
    "                    g.x = torch.tensor(self.scaler_node.transform(g.x.numpy()), dtype=torch.float32)\n",
    "                if g.edge_attr.shape[0] > 0 and self.scaler_edge is not None:\n",
    "                    g.edge_attr = torch.tensor(self.scaler_edge.transform(g.edge_attr.numpy()), dtype=torch.float32)\n",
    "        \n",
    "        train_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_graphs, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        node_features = graphs[0].x.shape[1] if graphs else 0\n",
    "        edge_features = graphs[0].edge_attr.shape[1] if graphs and graphs[0].edge_attr.shape[0] > 0 else 11\n",
    "        \n",
    "        self.model = AMLGAE(node_features, edge_features, self.hidden_dim, self.latent_dim).to(self.device)\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=5e-4)\n",
    "        \n",
    "        # Compute validation threshold and statistics for z-score scaling\n",
    "        self.model.eval()\n",
    "        self.val_recon_errors = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(self.device)\n",
    "                _, node_recon, _, _ = self.model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "                errors = torch.mean((node_recon - batch.x) ** 2, dim=1).cpu().numpy()\n",
    "                self.val_recon_errors.extend(errors)\n",
    "        self.threshold = np.percentile(self.val_recon_errors, 98) if self.val_recon_errors else 0.5\n",
    "        self.val_error_mean = np.mean(self.val_recon_errors) if self.val_recon_errors else 0\n",
    "        self.val_error_std = np.std(self.val_recon_errors) if self.val_recon_errors else 1\n",
    "        \n",
    "        print(\"\\n🚀 Training GAE model...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience = 10\n",
    "        counter = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            all_train_scores = []\n",
    "            all_train_labels = []\n",
    "            \n",
    "            for batch in train_loader:\n",
    "                batch = batch.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                z, node_recon, edge_recon, supervised_score = self.model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "                \n",
    "                node_loss = F.mse_loss(node_recon, batch.x)\n",
    "                edge_loss = F.binary_cross_entropy_with_logits(edge_recon.squeeze(), torch.ones_like(edge_recon.squeeze())) if edge_recon is not None else 0\n",
    "                supervised_loss = F.mse_loss(supervised_score.squeeze(), batch.y) if batch.y is not None else 0\n",
    "                loss = node_loss + 0.1 * edge_loss + 0.1 * supervised_loss\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                \n",
    "                recon_errors = torch.mean((node_recon - batch.x) ** 2, dim=1).cpu().detach().numpy()\n",
    "                all_train_scores.append(recon_errors)\n",
    "                all_train_labels.append(batch.y.cpu().numpy())\n",
    "            \n",
    "            self.model.eval()\n",
    "            val_loss = 0\n",
    "            all_val_scores = []\n",
    "            all_val_labels = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    batch = batch.to(self.device)\n",
    "                    z, node_recon, edge_recon, supervised_score = self.model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "                    \n",
    "                    node_loss = F.mse_loss(node_recon, batch.x)\n",
    "                    edge_loss = F.binary_cross_entropy_with_logits(edge_recon.squeeze(), torch.ones_like(edge_recon.squeeze())) if edge_recon is not None else 0\n",
    "                    supervised_loss = F.mse_loss(supervised_score.squeeze(), batch.y) if batch.y is not None else 0\n",
    "                    loss = node_loss + 0.1 * edge_loss + 0.1 * supervised_loss\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                    \n",
    "                    recon_errors = torch.mean((node_recon - batch.x) ** 2, dim=1).cpu().numpy()\n",
    "                    all_val_scores.append(recon_errors)\n",
    "                    all_val_labels.append(batch.y.cpu().numpy())\n",
    "            \n",
    "            avg_train_loss = train_loss / len(train_loader) if len(train_loader) > 0 else 0\n",
    "            avg_val_loss = val_loss / len(val_loader) if len(val_loader) > 0 else 0\n",
    "            \n",
    "            train_auc = roc_auc_score(np.concatenate(all_train_labels), np.concatenate(all_train_scores)) if all_train_labels and np.any(np.concatenate(all_train_labels)) else 0\n",
    "            val_auc = roc_auc_score(np.concatenate(all_val_labels), np.concatenate(all_val_scores)) if all_val_labels and np.any(np.concatenate(all_val_labels)) else 0\n",
    "            val_pr_auc = average_precision_score(np.concatenate(all_val_labels), np.concatenate(all_val_scores)) if all_val_labels and np.any(np.concatenate(all_val_labels)) else 0\n",
    "            \n",
    "            if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "                print(f\"  Train Loss: {avg_train_loss:.4f}, AUC: {train_auc:.4f}\")\n",
    "                print(f\"  Val Loss: {avg_val_loss:.4f}, AUC: {val_auc:.4f}, PR-AUC: {val_pr_auc:.4f}\")\n",
    "            \n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(\"Early stopping!\")\n",
    "                    break\n",
    "        \n",
    "        print(\"\\n✓ Training complete!\")\n",
    "        return self.model\n",
    "    \n",
    "    def detect_suspicious_patterns(self, df, threshold=None):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained yet!\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        graph, account_to_idx = self.graph_builder.create_temporal_graph(df, 0, len(df))\n",
    "        if graph is None or account_to_idx is None:\n",
    "            return {'suspicious_accounts': [], 'total_accounts_analyzed': 0, 'high_risk_count': 0, 'k1c5_involvement': 0, 'suspicious_clusters': [], 'precision': 0.0}\n",
    "        \n",
    "        if self.scaler_node is not None:\n",
    "            graph.x = torch.tensor(self.scaler_node.transform(graph.x.numpy()), dtype=torch.float32)\n",
    "        if graph.edge_attr.shape[0] > 0 and self.scaler_edge is not None:\n",
    "            graph.edge_attr = torch.tensor(self.scaler_edge.transform(graph.edge_attr.numpy()), dtype=torch.float32)\n",
    "        \n",
    "        graph = graph.to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            z, node_recon, edge_recon, _ = self.model(graph.x, graph.edge_index, graph.edge_attr)\n",
    "            recon_errors = torch.mean((node_recon - graph.x) ** 2, dim=1).cpu().numpy()\n",
    "        \n",
    "        # Z-score based anomaly score scaling\n",
    "        scaled_errors = np.zeros_like(recon_errors)\n",
    "        if self.val_recon_errors:\n",
    "            scaled_errors = (recon_errors - self.val_error_mean) / (self.val_error_std + 1e-10)\n",
    "            scaled_errors = np.log1p(np.clip(scaled_errors, 0, None))  # Logarithmic transformation\n",
    "            scaled_errors = 100 * (scaled_errors - np.min(scaled_errors)) / (np.max(scaled_errors) - np.min(scaled_errors) + 1e-10)\n",
    "            scaled_errors = np.clip(scaled_errors, 0, 100)\n",
    "        \n",
    "        threshold = threshold if threshold is not None else self.threshold\n",
    "        accounts = sorted(set(df['customer_id'].unique() if 'customer_id' in df.columns else df['sender_location'].unique()) | \n",
    "                         set(df['merchant_id'].unique() if 'merchant_id' in df.columns else df['receiver_location'].unique()))\n",
    "        suspicious_accounts = []\n",
    "        pred_labels = []\n",
    "        true_labels = []\n",
    "        \n",
    "        for i, (account, error, scaled_error) in enumerate(zip(accounts, recon_errors, scaled_errors)):\n",
    "            is_suspicious = error > threshold\n",
    "            pred_labels.append(1 if is_suspicious else 0)\n",
    "            true_labels.append(graph.y[i].item())\n",
    "            if is_suspicious:\n",
    "                suspicious_accounts.append({\n",
    "                    'account': account,\n",
    "                    'anomaly_score': float(scaled_error),\n",
    "                    'is_high_risk_location': account == 'K1C5' or ('CUST_' in account and df[df['customer_id'] == account]['sender_location'].iloc[0] == 'K1C5' if 'customer_id' in df.columns else False),\n",
    "                    'pattern': self._identify_pattern(df, account)\n",
    "                })\n",
    "        \n",
    "        precision = precision_score(true_labels, pred_labels) if any(true_labels) else 0.0\n",
    "        \n",
    "        G = nx.DiGraph()\n",
    "        for _, row in df.iterrows():\n",
    "            sender = row['customer_id' if 'customer_id' in df.columns else 'sender_location']\n",
    "            receiver = row['merchant_id' if 'merchant_id' in df.columns else 'receiver_location']\n",
    "            G.add_edge(sender, receiver, weight=row['amount'])\n",
    "        \n",
    "        suspicious_clusters = []\n",
    "        components = list(nx.weakly_connected_components(G))\n",
    "        for component in components:\n",
    "            valid_accounts = [acc for acc in component if acc in account_to_idx]\n",
    "            if not valid_accounts or len(valid_accounts) <= 3:  # Relaxed to >2 nodes\n",
    "                continue\n",
    "            avg_error = np.mean([recon_errors[account_to_idx[acc]] for acc in valid_accounts])\n",
    "            avg_scaled_error = np.mean([scaled_errors[account_to_idx[acc]] for acc in valid_accounts])\n",
    "            k1c5_nodes = sum(1 for acc in component if acc == 'K1C5' or ('CUST_' in account and df[df['customer_id'] == acc]['sender_location'].iloc[0] == 'K1C5' if 'customer_id' in df.columns else False))\n",
    "            if avg_error > threshold and k1c5_nodes > 0:\n",
    "                suspicious_clusters.append({\n",
    "                    'nodes': list(component),\n",
    "                    'avg_anomaly_score': float(avg_scaled_error),\n",
    "                    'k1c5_nodes': k1c5_nodes\n",
    "                })\n",
    "        \n",
    "        suspicious_accounts.sort(key=lambda x: x['anomaly_score'], reverse=True)\n",
    "        suspicious_clusters.sort(key=lambda x: x['avg_anomaly_score'], reverse=True)\n",
    "        \n",
    "        return {\n",
    "            'suspicious_accounts': suspicious_accounts,\n",
    "            'total_accounts_analyzed': len(accounts),\n",
    "            'high_risk_count': len(suspicious_accounts),\n",
    "            'k1c5_involvement': sum(1 for acc in suspicious_accounts if acc['is_high_risk_location']),\n",
    "            'suspicious_clusters': suspicious_clusters[:5],\n",
    "            'precision': precision\n",
    "        }\n",
    "    \n",
    "    def _identify_pattern(self, df, account):\n",
    "        sent = df[df['customer_id' if 'customer_id' in df.columns else 'sender_location'] == account]\n",
    "        received = df[df['merchant_id' if 'merchant_id' in df.columns else 'receiver_location'] == account]\n",
    "        all_txns = pd.concat([sent, received])\n",
    "        \n",
    "        if len(all_txns) == 0:\n",
    "            return \"unknown\"\n",
    "        \n",
    "        patterns = []\n",
    "        if len(all_txns) > 5:\n",
    "            amounts = all_txns['amount'].values\n",
    "            if np.std(amounts) < np.mean(amounts) * 0.3:\n",
    "                time_spread = all_txns['transaction_hour'].max() - all_txns['transaction_hour'].min()\n",
    "                if time_spread < 6:\n",
    "                    patterns.append(\"structuring\")\n",
    "        \n",
    "        if all_txns['cross_border'].mean() > 0.7:\n",
    "            if len(set(all_txns['sender_location'].unique()) | set(all_txns['receiver_location'].unique())) > 4:\n",
    "                patterns.append(\"layering\")\n",
    "        \n",
    "        if all_txns['cash_equivalent'].mean() > 0.5:\n",
    "            if 'business' in all_txns['customer_segment'].values:\n",
    "                patterns.append(\"integration\")\n",
    "        \n",
    "        return \", \".join(patterns) if patterns else \"suspicious_activity\"\n",
    "\n",
    "def demonstrate_aml_system(df):\n",
    "    print(\"🏰 Fantasy Kingdom AML Detection System\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Investigating corrupt syndicate in K1C5...\")\n",
    "    print(f\"\\nDataset Overview:\")\n",
    "    print(f\"  Total transactions: {len(df)}\")\n",
    "    print(f\"  Money laundering cases: {df['is_money_laundering'].sum()}\")\n",
    "    print(f\"  K1C5 involvement: {((df['sender_location'] == 'K1C5') | (df['receiver_location'] == 'K1C5')).sum()}\")\n",
    "    \n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    aml_system = AMLDetectionSystem(hidden_dim=32, latent_dim=16)\n",
    "    train_size = int(len(df) * 0.8)\n",
    "    train_df = df.iloc[:train_size]\n",
    "    test_df = df.iloc[train_size:]\n",
    "    \n",
    "    aml_system.train(train_df, epochs=50, batch_size=16)\n",
    "    \n",
    "    print(\"\\n🔍 Analyzing test transactions for suspicious patterns...\")\n",
    "    results = aml_system.detect_suspicious_patterns(test_df)\n",
    "    \n",
    "    print(f\"\\n📊 Detection Results:\")\n",
    "    print(f\"  Accounts analyzed: {results['total_accounts_analyzed']}\")\n",
    "    print(f\"  High-risk accounts detected: {results['high_risk_count']}\")\n",
    "    print(f\"  K1C5 syndicate connections: {results['k1c5_involvement']}\")\n",
    "    print(f\"  Precision: {results['precision']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n🚨 Top Suspicious Accounts:\")\n",
    "    for i, acc in enumerate(results['suspicious_accounts'][:5], 1):\n",
    "        print(f\"  {i}. {acc['account']} - Anomaly Score: {acc['anomaly_score']:.2f} - Pattern: {acc['pattern']}\")\n",
    "        if acc['is_high_risk_location']:\n",
    "            print(f\"     ⚠️  ALERT: Direct K1C5 syndicate member!\")\n",
    "    \n",
    "    \n",
    "    return aml_system, results\n",
    "\n",
    "# Example usage with Mission 1 data\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate base data\n",
    "    df = create_base_transaction_data(n_samples=10000, ml_ratio=0.05)\n",
    "    \n",
    "    # Enhance with customer-merchant networks\n",
    "    df = create_customer_merchant_networks(df, n_customers=500, n_merchants=200)\n",
    "    \n",
    "    # Add time-series features\n",
    "    df = generate_time_series_flows(df, days=30)\n",
    "    \n",
    "    # Run AML detection\n",
    "    aml_system, results = demonstrate_aml_system(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________\n",
    "\n",
    "# Mission 3 Review\n",
    "\n",
    "In Mission 3, you’ve trained a mystical familiar to hunt money laundering in Goldweave Port’s synthetic transaction ledger. This enchanted pet sniffs out syndicate patterns, revealing suspicious accounts and networks. Your notebook outputs—high-risk accounts, K1C5 ties, and precision metrics—expose the syndicate’s schemes!\n",
    "\n",
    "## The Familiar’s Magic\n",
    "\n",
    "The Prophecy Familiar uses three powers to track activities:\n",
    "\n",
    "### 1. Web Weaving\n",
    "- **Action**: Spins transactions into webs (`networkx>=3.4.2`), with accounts as nodes and transactions as weighted threads (`amount`).\n",
    "- **Features**: Adds transaction counts, K1C5 flags (weighted x7), and time-series data (`velocity_24h`).\n",
    "- **Output**: Temporal graph snapshots (300 transactions, stride 20).\n",
    "\n",
    "**Role**: Maps K1C5 networks for detection.\n",
    "\n",
    "### 2. Arcane Vision\n",
    "- **Action**: Uses Graph Autoencoder (`torch>=2.8.0`, `torch-geometric>=2.6.1`) to learn normal patterns and spot anomalies.\n",
    "- **Process**: Encodes nodes with GCNs, reconstructs webs, and flags high-error nodes (anomaly scores 0–100).\n",
    "- **Training**: Fine-tunes on K1C5 labels.\n",
    "\n",
    "**Role**: Detects suspicious accounts (e.g., structuring, layering).\n",
    "\n",
    "### 3. Syndicate Hunt\n",
    "- **Action**: Trains on 80% of synthetic data (50 epochs), tests on 20%, and identifies suspicious clusters.\n",
    "- **Outputs**: Lists top accounts, scores, patterns (e.g., structuring for frequent small deposits), and K1C5-linked webs.\n",
    "- **Metrics**: Reports precision, account counts, K1C5 involvement.\n",
    "\n",
    "**Role**: Pinpoints high-risk accounts and networks.\n",
    "\n",
    "## Why It Matters\n",
    "Your familiar’s visions expose money laundering tactics, bridging Missions 1 and 2, turning data into actionable omens. \n",
    "\n",
    "_______________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mission 4: The Oracle's Golem\n",
    "**Objective**: Develop a Retrieval-Augmented Generation (RAG) golem to analyze and answer questions about the synthetic data and insights for Missions 1, and 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "🔮 **Initiating Valdris Financial Investigation System...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "================================================================================"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "✓ **Initialized Arcane RAG System for Transaction and Document Analysis**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "1️⃣ **Loading Enchanted Ledger from mission1_synthetic_data.csv...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "✓ **Loaded 44 ledger entries**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "2️⃣ **Deciphering Ancient Scrolls from mission2_documents...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "📜 **Decoding bank_statement_571029.pdf...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "📜 **Decoding SAR_816751.pdf...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "📜 **Decoding whistleblower_report_RG126225.pdf...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "✓ **Processed 18 scroll fragments**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "✓ **Indexed 62 artifacts with FAISS and BM25**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "🏰 **Arcane Investigation Chamber Active**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Type your query or 'exit' to close the chamber."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Example Queries:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- Average transaction amount in the database\n",
       "- Location indicated in the whistleblower report\n",
       "- Suspicious transactions in K1C5\n",
       "- Distribution of amount"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "🔍 **Arcane Query: Location indicated by the whistle blower report**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "📜 **Arcane Findings**:\n",
       "The reported suspicious activity is associated with Golden Griffin Trading Co., located in Goldweave Port (K1C5). Key findings include: - Multiple gold deposits under 10,000 pieces threshold - Structured transactions as a possible attempt to avoid detection - Large offshore transfers ranging between 25,000 and 150,000 gold pieces with no..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "🔍 **Arcane Query: average transaction amount in the ledger**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "📜 **Arcane Findings**:\n",
       "Average amount: 3,599.35 Gold Pieces (K1C5)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pdfplumber\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from IPython.display import display, Markdown\n",
    "import ollama\n",
    "import signal\n",
    "import sys\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "# Pydantic model for query parsing\n",
    "class QueryCommand(BaseModel):\n",
    "    command: str = Field(..., description=\"Command type (e.g., analyze, answer_question)\")\n",
    "    params: Dict = Field(default_factory=dict, description=\"Parameters like column, agg_type\")\n",
    "\n",
    "# Signal handler for graceful exit\n",
    "def signal_handler(sig, frame):\n",
    "    display(Markdown(\"🪄 **Arcane Investigation Terminated.**\"))\n",
    "    sys.exit(0)\n",
    "\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "display(Markdown(\"🔮 **Initiating Valdris Financial Investigation System...**\"))\n",
    "display(Markdown(\"=\" * 80))\n",
    "\n",
    "class FantasyRAGChatbot:\n",
    "    def __init__(self, transaction_csv=\"mission1_synthetic_data.csv\", docs_folder=\"mission2_documents\"):\n",
    "        self.transaction_csv = transaction_csv\n",
    "        self.docs_folder = docs_folder\n",
    "        self.documents = []\n",
    "        self.embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        self.faiss_index = None\n",
    "        self.bm25 = None\n",
    "        self.model_name = \"mistral:7b-instruct-v0.3-q4_0\"\n",
    "        self.df = None\n",
    "        display(Markdown(\"✓ **Initialized Arcane RAG System for Transaction and Document Analysis**\"))\n",
    "\n",
    "    def load_transaction_data(self):\n",
    "        \"\"\"Load and process transaction data from CSV with detailed metadata\"\"\"\n",
    "        display(Markdown(f\"\\n1️⃣ **Loading Enchanted Ledger from {self.transaction_csv}...**\"))\n",
    "        try:\n",
    "            self.df = pd.read_csv(self.transaction_csv)\n",
    "            expected_columns = [\n",
    "                'timestamp', 'amount', 'account_age_days', 'transaction_hour', 'day_of_week',\n",
    "                'transactions_last_24h', 'account_balance_ratio', 'merchant_risk_score',\n",
    "                'cross_border', 'cash_equivalent', 'transaction_type', 'customer_segment',\n",
    "                'sender_location', 'receiver_location', 'is_money_laundering', 'customer_id',\n",
    "                'merchant_id', 'running_total', 'hour_of_day', 'velocity_1h', 'velocity_24h'\n",
    "            ]\n",
    "            if not all(col in self.df.columns for col in expected_columns):\n",
    "                missing = [col for col in expected_columns if col not in self.df.columns]\n",
    "                display(Markdown(f\"⚠️ **Error: CSV missing columns: {missing}.**\"))\n",
    "                return []\n",
    "            self.df['date'] = pd.to_datetime(self.df['timestamp'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "            if self.df['date'].isna().all():\n",
    "                display(Markdown(\"⚠️ **Error: All timestamps are invalid.**\"))\n",
    "                self.df['date'] = \"Unknown\"\n",
    "        except FileNotFoundError:\n",
    "            display(Markdown(f\"⚠️ **Error: {self.transaction_csv} not found. Ensure Mission 1 data is generated.**\"))\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            display(Markdown(f\"⚠️ **Error loading CSV: {str(e)}.**\"))\n",
    "            return []\n",
    "\n",
    "        transaction_docs = []\n",
    "        summary = f\"CSV: {self.transaction_csv}, Columns: {', '.join(self.df.columns)}, Rows: {len(self.df)}\"\n",
    "        transaction_docs.append({\n",
    "            \"text\": summary,\n",
    "            \"metadata\": {\"type\": \"transaction\", \"file_name\": self.transaction_csv, \"content_type\": \"summary\"}\n",
    "        })\n",
    "        for col in self.df.columns:\n",
    "            sample_vals = self.df[col].dropna().head(3).tolist()\n",
    "            col_info = f\"Column '{col}': {self.df[col].dtype}, sample values: {sample_vals}\"\n",
    "            if pd.api.types.is_numeric_dtype(self.df[col]):\n",
    "                col_info += f\", min: {self.df[col].min()}, max: {self.df[col].max()}, mean: {self.df[col].mean():.2f}\"\n",
    "            transaction_docs.append({\n",
    "                \"text\": col_info,\n",
    "                \"metadata\": {\"type\": \"transaction\", \"file_name\": self.transaction_csv, \"column\": col, \"content_type\": \"column_info\"}\n",
    "            })\n",
    "        for idx, row in self.df.head(20).iterrows():\n",
    "            doc_text = f\"\"\"\n",
    "TRANSACTION #{idx}\n",
    "Date: {row['date']}\n",
    "Location: {row['sender_location']}\n",
    "Account: {row['customer_id']}\n",
    "Amount: {row['amount']:,} Gold Pieces\n",
    "Type: {row['transaction_type']}\n",
    "Description: {row.get('description', 'N/A')}\n",
    "Counterparty: {row['merchant_id']}\n",
    "            \"\"\".strip()\n",
    "            metadata = {\n",
    "                \"type\": \"transaction\",\n",
    "                \"file_name\": self.transaction_csv,\n",
    "                \"transaction_id\": str(idx),\n",
    "                \"location\": row['sender_location'],\n",
    "                \"amount\": float(row['amount']),\n",
    "                \"counterparty\": row['merchant_id'],\n",
    "                \"description\": row.get('description', 'N/A'),\n",
    "                \"is_money_laundering\": bool(row['is_money_laundering']),\n",
    "                \"merchant_risk_score\": float(row['merchant_risk_score']),\n",
    "                \"content_type\": \"data\"\n",
    "            }\n",
    "            transaction_docs.append({\"text\": doc_text, \"metadata\": metadata})\n",
    "        display(Markdown(f\"✓ **Loaded {len(transaction_docs)} ledger entries**\"))\n",
    "        return transaction_docs\n",
    "\n",
    "    def load_investigation_documents(self):\n",
    "        \"\"\"Extract text and tables from PDFs with enhanced metadata\"\"\"\n",
    "        display(Markdown(f\"\\n2️⃣ **Deciphering Ancient Scrolls from {self.docs_folder}...**\"))\n",
    "        docs_path = Path(self.docs_folder)\n",
    "        if not docs_path.exists():\n",
    "            display(Markdown(f\"⚠️ **Error: {self.docs_folder} not found. Run Mission 2 to generate documents.**\"))\n",
    "            return []\n",
    "        \n",
    "        investigation_docs = []\n",
    "        for pdf_file in docs_path.glob(\"*.pdf\"):\n",
    "            display(Markdown(f\"📜 **Decoding {pdf_file.name}...**\"))\n",
    "            try:\n",
    "                with pdfplumber.open(pdf_file) as pdf:\n",
    "                    full_text = \"\"\n",
    "                    for page_num, page in enumerate(pdf.pages, 1):\n",
    "                        try:\n",
    "                            text = page.extract_text() or \"\"\n",
    "                            tables = page.extract_tables() or []\n",
    "                            for table in tables:\n",
    "                                table_str = \"\\n\".join([\",\".join(str(cell or '') for cell in row) for row in table])\n",
    "                                text += f\"\\nTable:\\n{table_str}\\n\"\n",
    "                            if text.strip():\n",
    "                                investigation_docs.append({\n",
    "                                    \"text\": f\"Page {page_num}: {text}\",\n",
    "                                    \"metadata\": {\n",
    "                                        \"type\": \"document\",\n",
    "                                        \"file_name\": pdf_file.name,\n",
    "                                        \"document_type\": (\"whistleblower_report\" if \"whistleblower\" in pdf_file.name.lower() else\n",
    "                                                         \"bank_statement\" if \"bank_statement\" in pdf_file.name.lower() else\n",
    "                                                         \"suspicious_activity_report\" if \"sar\" in pdf_file.name.lower() else \"unknown\"),\n",
    "                                        \"page\": page_num,\n",
    "                                        \"content_type\": \"text\"\n",
    "                                    }\n",
    "                                })\n",
    "                        except Exception as e:\n",
    "                            display(Markdown(f\"⚠️ **Warning: Failed to extract page {page_num} from {pdf_file.name}: {str(e)}**\"))\n",
    "                    if not full_text.strip():\n",
    "                        full_text = \"\\n\".join(doc[\"text\"] for doc in investigation_docs if doc[\"metadata\"][\"file_name\"] == pdf_file.name)\n",
    "                    if full_text.strip():\n",
    "                        chunks = [full_text[i:i+1000] for i in range(0, len(full_text), 1000)]\n",
    "                        for chunk_idx, chunk in enumerate(chunks):\n",
    "                            investigation_docs.append({\n",
    "                                \"text\": chunk,\n",
    "                                \"metadata\": {\n",
    "                                    \"type\": \"document\",\n",
    "                                    \"file_name\": pdf_file.name,\n",
    "                                    \"document_type\": investigation_docs[-1][\"metadata\"][\"document_type\"],\n",
    "                                    \"chunk_id\": chunk_idx,\n",
    "                                    \"content_type\": \"chunk\"\n",
    "                                }\n",
    "                            })\n",
    "            except Exception as e:\n",
    "                display(Markdown(f\"⚠️ **Error processing {pdf_file.name}: {str(e)}**\"))\n",
    "                continue\n",
    "        display(Markdown(f\"✓ **Processed {len(investigation_docs)} scroll fragments**\"))\n",
    "        return investigation_docs\n",
    "\n",
    "    def create_visualization(self, query: str, command: QueryCommand) -> Tuple[Optional[object], str]:\n",
    "        \"\"\"Create visualizations for statistical queries\"\"\"\n",
    "        if not hasattr(self, 'df') or self.df is None or self.df.empty:\n",
    "            return None, \"\"\n",
    "        try:\n",
    "            query_lower = query.lower()\n",
    "            viz_type = command.params.get(\"viz_type\", \"\")\n",
    "            plt.style.use('default')\n",
    "            sns.set_palette(\"husl\")\n",
    "\n",
    "            if any(word in query_lower for word in ['distribution', 'histogram', 'hist', 'spread']):\n",
    "                column = command.params.get(\"column\", \"amount\")\n",
    "                if column not in self.df.columns:\n",
    "                    numeric_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "                    column = numeric_cols[0] if numeric_cols else None\n",
    "                if column and pd.api.types.is_numeric_dtype(self.df[column]):\n",
    "                    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "                    clean_data = self.df[column].dropna()\n",
    "                    ax.hist(clean_data, bins=30, alpha=0.7, edgecolor='black')\n",
    "                    ax.axvline(clean_data.mean(), color='red', linestyle='--', label=f'Mean: {clean_data.mean():.2f}')\n",
    "                    ax.set_xlabel(column.replace('_', ' ').title())\n",
    "                    ax.set_ylabel('Frequency')\n",
    "                    ax.set_title(f'Distribution of {column.replace(\"_\", \" \").title()} in K1C5')\n",
    "                    ax.legend()\n",
    "                    ax.grid(True, alpha=0.3)\n",
    "                    plt.tight_layout()\n",
    "                    return fig, f\"distribution of {column}\"\n",
    "            return None, \"\"\n",
    "        except Exception as e:\n",
    "            display(Markdown(f\"⚠️ **Error creating visualization: {str(e)}**\"))\n",
    "            return None, \"\"\n",
    "\n",
    "    def extract_command(self, query: str) -> QueryCommand:\n",
    "        \"\"\"Parse query into a structured command\"\"\"\n",
    "        query_lower = query.lower()\n",
    "        if any(word in query_lower for word in ['distribution', 'histogram', 'hist', 'spread']):\n",
    "            return QueryCommand(command=\"visualize\", params={\"viz_type\": \"histogram\"})\n",
    "        elif any(word in query_lower for word in ['average', 'mean', 'avg']):\n",
    "            if any(phrase in query_lower for phrase in ['per customer', 'by customer', 'per location']):\n",
    "                return QueryCommand(command=\"analyze\", params={\"agg_type\": \"group_average\", \"group_by\": \"customer_id\" if \"customer\" in query_lower else \"sender_location\"})\n",
    "            return QueryCommand(command=\"analyze\", params={\"agg_type\": \"average\", \"column\": \"amount\"})\n",
    "        elif any(word in query_lower for word in ['total', 'sum']):\n",
    "            return QueryCommand(command=\"analyze\", params={\"agg_type\": \"sum\", \"column\": \"amount\"})\n",
    "        elif any(word in query_lower for word in ['count', 'number of']):\n",
    "            return QueryCommand(command=\"analyze\", params={\"agg_type\": \"count\", \"column\": \"amount\"})\n",
    "        elif any(word in query_lower for word in ['largest', 'biggest', 'maximum']):\n",
    "            return QueryCommand(command=\"analyze\", params={\"agg_type\": \"largest\"})\n",
    "        elif any(word in query_lower for word in ['highest', 'top', 'max'] + ['merchant risk score']):\n",
    "            return QueryCommand(command=\"analyze\", params={\"agg_type\": \"highest_risk\"})\n",
    "        elif any(word in query_lower for word in ['suspicious', 'money laundering', 'K1C5']):\n",
    "            return QueryCommand(command=\"analyze\", params={\"agg_type\": \"suspicious\"})\n",
    "        return QueryCommand(command=\"answer_question\", params={\"keyword\": query})\n",
    "\n",
    "    def generate_transaction_stats(self, query: str, command: QueryCommand) -> Tuple[str, Optional[object], str]:\n",
    "        \"\"\"Compute specific statistics from CSV with visualization support\"\"\"\n",
    "        if not hasattr(self, 'df') or self.df is None or self.df.empty:\n",
    "            return \"No transaction data available.\", None, \"\"\n",
    "        \n",
    "        try:\n",
    "            if 'date' not in self.df.columns or 'amount' not in self.df.columns:\n",
    "                missing = [col for col in ['date', 'amount'] if col not in self.df.columns]\n",
    "                display(Markdown(f\"⚠️ **Error: Missing columns: {missing}.**\"))\n",
    "                return f\"Missing columns: {missing}\", None, \"\"\n",
    "            \n",
    "            column = command.params.get(\"column\", \"amount\")\n",
    "            if column not in self.df.columns:\n",
    "                numeric_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "                column = numeric_cols[0] if numeric_cols else None\n",
    "                if not column:\n",
    "                    return f\"No numeric columns found. Available: {', '.join(self.df.columns)}\", None, \"\"\n",
    "            \n",
    "            agg_type = command.params.get(\"agg_type\")\n",
    "            chart, chart_type = self.create_visualization(query, command)\n",
    "\n",
    "            if agg_type == \"average\":\n",
    "                result = self.df[column].mean()\n",
    "                result_text = f\"Average {column}: {result:,.2f} Gold Pieces (K1C5)\"\n",
    "            elif agg_type == \"sum\":\n",
    "                result = self.df[column].sum()\n",
    "                result_text = f\"Total {column}: {result:,.2f} Gold Pieces (K1C5)\"\n",
    "            elif agg_type == \"count\":\n",
    "                result = self.df[column].count()\n",
    "                result_text = f\"Count of {column} transactions: {result:,} (K1C5)\"\n",
    "            elif agg_type == \"group_average\":\n",
    "                group_by = command.params.get(\"group_by\", \"customer_id\")\n",
    "                if group_by not in self.df.columns:\n",
    "                    result_text = f\"Group column '{group_by}' not found. Available: {', '.join(self.df.columns)}\"\n",
    "                elif pd.api.types.is_numeric_dtype(self.df[column]):\n",
    "                    grouped = self.df.groupby(group_by)[column].mean()\n",
    "                    result_text = f\"Average {column} per {group_by}: {grouped.mean():,.2f} Gold Pieces ({len(grouped)} groups in K1C5)\"\n",
    "                else:\n",
    "                    result_text = f\"Column '{column}' is not numeric.\"\n",
    "            elif agg_type == \"largest\":\n",
    "                max_trans = self.df.loc[self.df['amount'].idxmax()]\n",
    "                date_str = str(max_trans['date']) if not pd.isna(max_trans['date']) else \"Unknown\"\n",
    "                result_text = f\"\"\"\n",
    "Largest Transaction (K1C5):\n",
    "- Amount: {max_trans['amount']:,} Gold Pieces\n",
    "- Date: {date_str}\n",
    "- Location: {max_trans['sender_location']}\n",
    "- Account: {max_trans['customer_id']}\n",
    "- Counterparty: {max_trans['merchant_id']}\n",
    "- Type: {max_trans['transaction_type']}\n",
    "\"\"\"\n",
    "            elif agg_type == \"highest_risk\":\n",
    "                max_risk = self.df.loc[self.df['merchant_risk_score'].idxmax()]\n",
    "                result_text = f\"\"\"\n",
    "Highest Risk Account (K1C5):\n",
    "- Account: {max_risk['customer_id']}\n",
    "- Merchant Risk Score: {max_risk['merchant_risk_score']:.2f}\n",
    "- Amount: {max_risk['amount']:,} Gold Pieces\n",
    "- Location: {max_risk['sender_location']}\n",
    "- Counterparty: {max_risk['merchant_id']}\n",
    "\"\"\"\n",
    "            elif agg_type == \"suspicious\":\n",
    "                suspicious_df = self.df[\n",
    "                    (self.df['is_money_laundering'] == 1) |\n",
    "                    (self.df['amount'].between(9000, 9950)) |\n",
    "                    (self.df['amount'] > 15000) |\n",
    "                    (self.df['sender_location'] == 'K1C5')\n",
    "                ]\n",
    "                result_text = f\"\"\"\n",
    "Suspicious Transactions (K1C5):\n",
    "- Count: {len(suspicious_df)}\n",
    "- Total Amount: {suspicious_df['amount'].sum():,.2f} Gold Pieces\n",
    "- Top Locations: {suspicious_df['sender_location'].value_counts().head(3).to_dict()}\n",
    "- Top Types: {suspicious_df['transaction_type'].value_counts().head(2).to_dict()}\n",
    "\"\"\"\n",
    "            else:\n",
    "                result_text = f\"Dataset has {len(self.df)} rows, {len(self.df.columns)} columns. Numeric: {', '.join(self.df.select_dtypes(include=['number']).columns)}\"\n",
    "            \n",
    "            return result_text, chart, chart_type\n",
    "        except Exception as e:\n",
    "            display(Markdown(f\"⚠️ **Error in stats computation: {str(e)}**\"))\n",
    "            return f\"Unable to compute statistics: {str(e)}\", None, \"\"\n",
    "\n",
    "    def search_documents(self, query, top_k=5, prioritize_transactions=False):\n",
    "        \"\"\"Hybrid search with option to prioritize transaction data\"\"\"\n",
    "        try:\n",
    "            query_embedding = self.embedder.encode(query, convert_to_numpy=True).astype('float32')\n",
    "            tokenized_query = query.lower().split()\n",
    "            bm25_scores = self.bm25.get_scores(tokenized_query)\n",
    "            distances, indices = self.faiss_index.search(query_embedding.reshape(1, -1), top_k * 2)\n",
    "            semantic_scores = [1 / (1 + d) for d in distances[0]]\n",
    "            combined_scores = []\n",
    "            for j, i in enumerate(indices[0]):\n",
    "                score = 0.4 * bm25_scores[i] + 0.6 * semantic_scores[j]\n",
    "                if prioritize_transactions and self.documents[i][\"metadata\"][\"type\"] == \"transaction\":\n",
    "                    score *= 1.5\n",
    "                if \"K1C5\" in query.lower() and \"K1C5\" in self.documents[i][\"metadata\"].get(\"location\", \"\"):\n",
    "                    score *= 2.0\n",
    "                combined_scores.append((i, score))\n",
    "            top_indices = sorted(combined_scores, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "            results = {\n",
    "                \"documents\": [self.documents[i][\"text\"] for i, _ in top_indices],\n",
    "                \"metadatas\": [self.documents[i][\"metadata\"] for i, _ in top_indices]\n",
    "            }\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            display(Markdown(f\"⚠️ **Error during search: {str(e)}**\"))\n",
    "            return {\"documents\": [], \"metadatas\": []}\n",
    "\n",
    "    def generate_response(self, query, search_results):\n",
    "        \"\"\"Generate concise response with Ollama, tailored to query type\"\"\"\n",
    "        command = self.extract_command(query)\n",
    "        if command.command == \"analyze\":\n",
    "            result_text, chart, chart_type = self.generate_transaction_stats(query, command)\n",
    "            return f\"{result_text}\\nChart: {chart_type}\" if chart_type else result_text\n",
    "        \n",
    "        context = \"\\n\\n\".join(search_results[\"documents\"][:3])\n",
    "        prompt = f\"\"\"\n",
    "By the decree of the Valdris Council, you are the Arcane Investigator, probing financial crimes in Goldweave Port (K1C5).\n",
    "CONTEXT: {context}\n",
    "QUESTION: {query}\n",
    "Answer in 50 words or less, using only the context. Focus on K1C5, list key findings as bullet points, and avoid speculation. If no relevant information, state so.\n",
    "\"\"\"\n",
    "        try:\n",
    "            response = ollama.chat(model=self.model_name, messages=[{'role': 'user', 'content': prompt}])\n",
    "            response_text = response['message']['content']\n",
    "            words = response_text.split()\n",
    "            if len(words) > 50:\n",
    "                response_text = ' '.join(words[:50]) + \"...\"\n",
    "            return response_text\n",
    "        except Exception as e:\n",
    "            display(Markdown(f\"⚠️ **Error: Ollama failed - {str(e)}. Ensure Ollama server is running with model {self.model_name}.**\"))\n",
    "            return \"Unable to generate response due to Ollama error.\"\n",
    "\n",
    "    def investigate(self, query):\n",
    "        \"\"\"Run investigation query\"\"\"\n",
    "        display(Markdown(f\"\\n🔍 **Arcane Query: {query}**\"))\n",
    "        if not self.documents or self.faiss_index is None:\n",
    "            display(Markdown(\"⚠️ **Error: Index not set up. Run setup_index first.**\"))\n",
    "            return \"\"\n",
    "        prioritize_transactions = any(term in query.lower() for term in [\"transaction\", \"database\", \"largest\", \"highest\", \"account\", \"amount\", \"average\", \"sum\", \"count\"])\n",
    "        search_results = self.search_documents(query, top_k=10, prioritize_transactions=prioritize_transactions)\n",
    "        if not search_results[\"documents\"]:\n",
    "            display(Markdown(\"⚠️ **No relevant documents found for query.**\"))\n",
    "            return \"No relevant information found.\"\n",
    "        response = self.generate_response(query, search_results)\n",
    "        display(Markdown(f\"📜 **Arcane Findings**:\\n{response}\"))\n",
    "        return response\n",
    "\n",
    "    def setup_index(self):\n",
    "        \"\"\"Build FAISS and BM25 indices\"\"\"\n",
    "        self.documents = self.load_transaction_data() + self.load_investigation_documents()\n",
    "        if not self.documents:\n",
    "            display(Markdown(\"⚠️ **Error: No documents loaded. Check data and document paths.**\"))\n",
    "            return\n",
    "        for doc in self.documents:\n",
    "            try:\n",
    "                doc[\"embedding\"] = self.embedder.encode(doc[\"text\"], convert_to_numpy=True)\n",
    "            except Exception as e:\n",
    "                display(Markdown(f\"⚠️ **Error embedding document: {str(e)}**\"))\n",
    "                doc[\"embedding\"] = np.zeros(384)\n",
    "        embeddings = np.array([doc[\"embedding\"] for doc in self.documents]).astype('float32')\n",
    "        dimension = embeddings.shape[1]\n",
    "        self.faiss_index = faiss.IndexFlatL2(dimension)\n",
    "        self.faiss_index.add(embeddings)\n",
    "        tokenized_docs = [doc[\"text\"].lower().split() for doc in self.documents]\n",
    "        self.bm25 = BM25Okapi(tokenized_docs)\n",
    "        display(Markdown(f\"✓ **Indexed {len(self.documents)} artifacts with FAISS and BM25**\"))\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Main loop for interactive investigation\"\"\"\n",
    "        self.setup_index()\n",
    "        if not self.documents:\n",
    "            display(Markdown(\"⚠️ **Error: No data loaded. Investigation halted.**\"))\n",
    "            return\n",
    "        display(Markdown(\"\\n🏰 **Arcane Investigation Chamber Active**\"))\n",
    "        display(Markdown(\"Type your query or 'exit' to close the chamber.\"))\n",
    "        display(Markdown(\"Example Queries:\"))\n",
    "        display(Markdown(\"- Average transaction amount in the database\\n- Location indicated in the whistleblower report\\n- Suspicious transactions in K1C5\\n- Distribution of amount\"))\n",
    "        question_count = 0\n",
    "        while True:\n",
    "            try:\n",
    "                question = input(f\"🪄 Query #{question_count + 1}: \").strip()\n",
    "                if question.lower() in ['exit', 'quit', 'stop']:\n",
    "                    display(Markdown(f\"\\n👋 **Chamber Closed. {question_count} queries investigated.**\"))\n",
    "                    break\n",
    "                if question:\n",
    "                    self.investigate(question)\n",
    "                    question_count += 1\n",
    "                else:\n",
    "                    display(Markdown(\"💡 **Enter a query or 'exit'.**\"))\n",
    "            except Exception as e:\n",
    "                display(Markdown(f\"⚠️ **Error during query: {str(e)}**\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot = FantasyRAGChatbot()\n",
    "    chatbot.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Mission 4 Review \n",
    "\n",
    "In mission 4, you deployed the `FantasyRAGChatbot`, a mystical Retrieval-Augmented Generation (RAG) golem, to query the enchanted transaction ledger (`mission1_synthetic_data.csv`) and key reports (`mission2_documents/`) from Goldweave Port. By running the chatbot in your notebook, you’ve extracted insights about suspicious transactions and documents, uncovering potential money laundering tied to the K1C5 syndicate. Let’s dive into how this arcane tool works and what it reveals!\n",
    "\n",
    "## How It Works\n",
    "\n",
    "The `FantasyRAGChatbot` harnesses synthetic data and documents:\n",
    "\n",
    "- **Transaction Loader** (`pandas>=2.2.3`): Reads synthetic CSV with columns like `amount`, `customer_id`, `is_money_laundering`. Converts timestamps to dates. Creates text summaries (e.g., “10,000 Gold Pieces, K1C5”).\n",
    "- **Document Extractor** (`pdfplumber>=0.11.7`): Extracts text/tables from PDFs (e.g., whistleblower reports). Splits into 500-char chunks, labeling as `bank_statement` or `suspicious_activity_report`.\n",
    "- **RAG System**: Uses BM25 (`rank-bm25>=0.2.2`) and SentenceTransformers (`sentence-transformers==3.2.0`) for retrieval, FAISS (`faiss-cpu==1.9.0`) for embedding search, and Mistral (`ollama>=0.5.3`) for <100-word responses. Prioritizes synthetic transaction data for queries like “largest transaction.”\n",
    "\n",
    "## Why It Matters\n",
    "Mission 1’s synthetic data mimics real AML patterns (e.g., K1C5’s high-amount transfers) using Gaussian Copula for statistical fidelity and CTGAN for complex distributions. The golem queries these patterns, revealing suspicious transactions or document clues. Mission 2's PDF reports provide additional context to the golem. Any additional reports will be accessible to the golem. This empowers the high court, giving them the tool to further assess the evnidence, expose synthetic patterns, and take down the money laundering syndicate!\n",
    "\n",
    "____________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Conclusion: Triumph of the Arcane Investigator\n",
    "\n",
    "Noble Investigator, through the 4 missions, you have successfully unraveled Goldweave Port’s financial mysteries and money laundering patterns! \n",
    "\n",
    "In Mission 1, you forged a synthetic ledger (mission1_synthetic_data.csv) using Gaussian Copula, CTGAN, Graph Network, and TVAE, crafting realistic transaction patterns to expose K1C5’s schemes. \n",
    "\n",
    "Mission 2, you summoned critical documents (mission2_documents/) like whistleblowers report and bank statements. \n",
    "\n",
    "Mission 3, your synthetic data laid the groud work for training your familiar so it has the power to detect suspicious patterns utilizing... \n",
    "\n",
    "In Mission 4, the FantasyRAGChatbot golem empowered you to query transactions and documents, revealing high-value transfers and syndicate clues with BM25, SentenceTransformers, and Mistral. Your synthetic data-driven insights have armed the Valdris Council to combat money laundering. Your arcane mastery has safeguarded the realm!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epilogue: Technical Takeaways from the AML Lab\n",
    "\n",
    "This lab highlights accessible synthetic data tools to simulate financial crime investigations. Below are details on the benefits, limitations, and alternatives for each mission’s tools.\n",
    "\n",
    "#### Mission 1: Gaussian Copula, CTGAN, Graph Network, TVAE (`sdv>=0.18.0`)\n",
    "- **Benefits**: \n",
    "  - **Gaussian Copula**: Uses copula functions to model multivariate dependencies, preserving correlations (e.g., Pearson coefficients for `amount` and `is_money_laundering`) with low computational cost (O(n) for 3,000 transactions).\n",
    "  - **CTGAN**: Employs conditional GANs to learn non-linear distributions, adept at rare events (5% money laundering cases) via adversarial training, handling mixed data (numerical `amount`, categorical `sender_location`).\n",
    "  - **Graph Network**: Generates synthetic graphs with NetworkX, capturing relational structures (e.g., K1C5 account clusters) via edge weights (`amount`) and node features (`velocity_24h`), ideal for syndicate analysis.\n",
    "  - **TVAE**: Leverages variational autoencoders to encode latent distributions, ensuring diverse transaction features (e.g., `account_age_days`) with high fidelity and balanced variance.\n",
    "  - **Overall**: Privacy-safe, supports 500 customers/200 merchants, integrates with downstream missions.\n",
    "- **Limitations**: Gaussian assumes normality, limiting non-Gaussian data; CTGAN/TVAE require high compute (50 epochs); Graph Network may miss non-relational patterns.\n",
    "- **Alternatives**: \n",
    "  - **Synthpop (R)**: Statistical modeling for interpretable outputs, ideal for academic research with smaller datasets.\n",
    "  - **SmartNoise**: Adds differential privacy, suitable for compliance-driven environments sharing sensitive data.\n",
    "  - **YData**: Enterprise-focused platform, ideal for production deployments requiring vendor support but may introduce dependency conflicts in complex multi-tool environments.\n",
    "\n",
    "#### Mission 2: Ollama (`>=0.5.3`), ReportLab (`>=4.2.2`)\n",
    "- **Benefits**: \n",
    "  - **Ollama**: Runs `mistral:7b-instruct-v0.3-q4_0` locally (4.1GB), generating coherent, role-specific narratives for whistleblower reports (insider perspective, e.g., management instructions) and SARs (external auditor perspective, e.g., transaction analysis). Produces varied, professional text for K1C5 syndicate activities (e.g., structured deposits of 9,000–9,950 gold pieces) with high context awareness.\n",
    "  - **ReportLab**: Creates professional PDFs with precise styling (e.g., 9pt Helvetica, grid-aligned tables), integrating Mission 1 data (e.g., `amount`) for realistic regulatory formats.\n",
    "  - **Overall**: Generates authentic AML documents with distinct narratives, aligned with synthetic transactions, privacy-safe via local LLM.\n",
    "- **Limitations**: Ollama requires setup (`ollama pull mistral:7b-instruct-v0.3-q4_0`, ~4GB disk) and moderate compute; ReportLab’s manual formatting limits scalability.\n",
    "- **Alternatives**: \n",
    "  - **Hugging Face Transformers (DistilGPT-2)**: Lightweight LLM (~500MB) for narrative generation, suitable for low-resource devices but less coherent.\n",
    "  - **WeasyPrint**: Converts HTML to PDFs, ideal for dynamic, web-based document generation with simpler styling.\n",
    "\n",
    "#### Mission 3: Torch-Geometric (`>=2.6.1`)\n",
    "- **Benefits**: \n",
    "  - **Torch-Geometric**: Implements Graph Convolutional Networks (GCNs) in AMLGAE, encoding transaction graphs (300-transaction windows) into 16D latent spaces, detecting anomalies via reconstruction errors (98th percentile threshold) with high AUC (e.g., 0.8+). Supports semi-supervised K1C5 detection with weighted features (e.g., `k1c5_interactions` x7).\n",
    "  - **Overall**: Efficient for network-driven AML, educational for graph ML concepts.\n",
    "- **Limitations**: Compute-heavy (50 epochs); sensitive to hyperparameters (e.g., `hidden_dim=32`).\n",
    "- **Alternatives**: \n",
    "  - **LightGBM**: Fast anomaly detection for tabular data, ideal for non-graph use cases like transaction monitoring.\n",
    "  - **Graph Neural Networks (GNNs) with PyG**: Advanced graph tasks, suitable for real-time network analysis.\n",
    "\n",
    "#### Mission 4: BM25 (`>=0.2.2`), SentenceTransformers (`==3.2.0`), FAISS (`==1.9.0`), Ollama (`>=0.5.3`)\n",
    "- **Benefits**: \n",
    "  - **BM25**: Delivers fast keyword-based retrieval using Okapi BM25 scoring, ranking transactions/documents (e.g., “K1C5 suspicious transactions”) in sub-second queries.\n",
    "  - **SentenceTransformers**: Generates 768D embeddings via `all-MiniLM-L6-v2`, enabling semantic search with high contextual similarity across PDFs and transactions.\n",
    "  - **FAISS**: Supports efficient vector search with approximate nearest neighbors, scaling to thousands of 500-char document chunks with low latency (<100ms).\n",
    "  - **Ollama**: Runs Mistral (7B parameters) locally, ensuring privacy for AML queries with concise, grounded responses (<100 words).\n",
    "  - **Overall**: Combines hybrid search and local LLM for interactive, privacy-preserving analysis.\n",
    "- **Limitations**: Ollama requires local setup; limited context size; embedding quality varies.\n",
    "- **Alternatives**: \n",
    "  - **DistilBERT**: Lightweight embeddings, ideal for resource-constrained environments.\n",
    "  - **Milvus**: Scalable vector search, suitable for enterprise-scale document databases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
