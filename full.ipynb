{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Detective's Quest: Unraveling the Money Laundering Syndicate\n",
    "\n",
    "Welcome, Detective! The king of **Valdris (Kingdom 1)** has entrusted you with a critical mission: investigate the bustling **Goldweave Port (City 5)**, which has been overrun by a sophisticated money laundering syndicate. Armed with magical powers, you can conjure **synthetic data**—realistic, fabricated datasets that mimic the kingdom's financial flows, documents, and testimonies without touching sensitive information or alerting the syndicate to the investigation.\n",
    "\n",
    "By influencing the city's *data realm*, you will:\n",
    "- Generate **simulated transaction databases** to trace illicit money movements.\n",
    "- Craft **bank statements** revealing suspicious businesses.\n",
    "- Summon **investigative reports** to build a compelling case.\n",
    "\n",
    "You will then use the synthetic data to:\n",
    "- Train your familiar to **detect anomalies** in the transaction database.\n",
    "- Develop a golem that **answers questions and queries** on the synthetic data.\n",
    "\n",
    "As you progress, you’ll master advanced spells (tools and practices) to ensure your synthetic creations are **accurate**, **private**, and powerful enough to dismantle the syndicate’s operations. Through this lab, you’ll expose the syndicate’s secrets while demonstrating the ethical power of synthetic data in real-world investigations.\n",
    "\n",
    "Embark on this quest to bring justice to City 5, wielding the art of synthetic data to outsmart the criminals and protect the kingdom!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mission 1: Conjuring Financial Flow Projections\n",
    "**Objective**: Generate synthetic transactional databases using various methods to simulate the movement of money, enabling the tracing of illicit financial flows in a fantasy kingdom setting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def create_base_transaction_data(n_samples=3000, ml_ratio=0.05):\n",
    "    \"\"\"Create realistic financial transaction data with money laundering cases, including typologies in a fantasy kingdom setting\"\"\"\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Define fantasy locations: 4 kingdoms, each with 5 cities\n",
    "    kingdoms = ['K1', 'K2', 'K3', 'K4']\n",
    "    cities_per_kingdom = ['C1', 'C2', 'C3', 'C4', 'C5']\n",
    "    all_locations = [f\"{k}{c}\" for k in kingdoms for c in cities_per_kingdom]\n",
    "    \n",
    "    # Common locations: Most cities\n",
    "    common_locations = [loc for loc in all_locations if loc not in ['K1C5']]\n",
    "    \n",
    "    # High-risk locations: Primarily K1C5 (the corrupt city under investigation)\n",
    "    high_risk_locations = ['K1C5']\n",
    "    \n",
    "    # Monitored locations: Some cities in other kingdoms, e.g., border cities\n",
    "    monitored_locations = ['K2C5', 'K3C1', 'K4C3']\n",
    "    \n",
    "    # Probabilities for location selection\n",
    "    def select_location(is_ml=False, typology=None):\n",
    "        if is_ml:\n",
    "            if typology == 'layering':\n",
    "                # Higher chance for monitored or high-risk for layering, favor K1C5\n",
    "                probs = [0.2, 0.3, 0.5]  # common, monitored, high_risk\n",
    "            else:\n",
    "                probs = [0.3, 0.3, 0.4]  # Slightly favor high-risk (K1C5)\n",
    "        else:\n",
    "            probs = [0.94, 0.05, 0.01]\n",
    "        \n",
    "        category = np.random.choice(['common', 'monitored', 'high_risk'], p=probs)\n",
    "        if category == 'common':\n",
    "            return np.random.choice(common_locations)\n",
    "        elif category == 'monitored':\n",
    "            return np.random.choice(monitored_locations)\n",
    "        else:\n",
    "            return np.random.choice(high_risk_locations)\n",
    "    \n",
    "    # Base transaction features\n",
    "    data = {\n",
    "        'transaction_id': [],\n",
    "        'amount': [],\n",
    "        'account_age_days': [],\n",
    "        'transaction_hour': [],\n",
    "        'day_of_week': [],\n",
    "        'transactions_last_24h': [],\n",
    "        'account_balance_ratio': [],\n",
    "        'merchant_risk_score': [],\n",
    "        'cross_border': [],\n",
    "        'cash_equivalent': [],\n",
    "        'transaction_type': [],\n",
    "        'customer_segment': [],\n",
    "        'sender_location': [],\n",
    "        'receiver_location': [],\n",
    "        'is_money_laundering': []\n",
    "    }\n",
    "    \n",
    "    # Generate legitimate transactions\n",
    "    n_legit = int(n_samples * (1 - ml_ratio))\n",
    "    for i in range(n_legit):\n",
    "        data['transaction_id'].append(f'TXN_{len(data[\"transaction_id\"]) + 1:06d}')\n",
    "        data['amount'].append(np.random.lognormal(mean=6, sigma=2))\n",
    "        data['account_age_days'].append(int(np.random.gamma(shape=2, scale=365)))\n",
    "        data['transaction_hour'].append(np.random.randint(0, 24))\n",
    "        data['day_of_week'].append(np.random.randint(0, 7))\n",
    "        data['transactions_last_24h'].append(np.random.poisson(2))\n",
    "        data['account_balance_ratio'].append(np.random.beta(2, 5))\n",
    "        data['merchant_risk_score'].append(np.random.beta(2, 8))\n",
    "        data['cross_border'].append(np.random.choice([0, 1], p=[0.9, 0.1]))\n",
    "        data['cash_equivalent'].append(np.random.choice([0, 1], p=[0.95, 0.05]))\n",
    "        data['transaction_type'].append(np.random.choice(\n",
    "            ['wire_transfer', 'card_payment', 'atm_withdrawal', \n",
    "             'online_transfer', 'check_deposit', 'cash_deposit'],\n",
    "            p=[0.15, 0.35, 0.15, 0.25, 0.05, 0.05]\n",
    "        ))\n",
    "        data['customer_segment'].append(np.random.choice(\n",
    "            ['retail', 'business', 'premium', 'corporate'],\n",
    "            p=[0.6, 0.2, 0.15, 0.05]\n",
    "        ))\n",
    "        sender = select_location(is_ml=False)\n",
    "        receiver = select_location(is_ml=False)\n",
    "        data['sender_location'].append(sender)\n",
    "        data['receiver_location'].append(receiver)\n",
    "        data['cross_border'][-1] = 1 if sender[:2] != receiver[:2] else 0  # Cross-border if different kingdoms\n",
    "        data['is_money_laundering'].append(0)\n",
    "    \n",
    "    # Generate money laundering transactions with typologies (focusing on corrupt syndicate in K1C5)\n",
    "    n_ml_base = int(n_samples * ml_ratio)\n",
    "    for i in range(n_ml_base):\n",
    "        # Choose typology\n",
    "        typology = np.random.choice(['structuring', 'layering', 'integration'], p=[0.4, 0.4, 0.2])\n",
    "        \n",
    "        # Base features for this ML case\n",
    "        base_amount = np.random.lognormal(mean=6, sigma=2) * np.random.uniform(3, 15)\n",
    "        account_age = int(np.random.gamma(shape=2, scale=365))\n",
    "        day_of_week = np.random.randint(0, 7)\n",
    "        merchant_risk = np.random.beta(8, 2)\n",
    "        transaction_type = np.random.choice(\n",
    "            ['wire_transfer', 'cash_deposit', 'online_transfer'],\n",
    "            p=[0.6, 0.3, 0.1]\n",
    "        )\n",
    "        customer_segment = np.random.choice(\n",
    "            ['retail', 'business', 'premium', 'corporate'],\n",
    "            p=[0.6, 0.2, 0.15, 0.05]\n",
    "        )\n",
    "        sender = select_location(is_ml=True, typology=typology)\n",
    "        receiver = select_location(is_ml=True, typology=typology)\n",
    "        # Ensure K1C5 involvement in at least one side for syndicate investigation\n",
    "        if np.random.random() < 0.7:  # 70% chance to involve K1C5\n",
    "            if np.random.random() < 0.5:\n",
    "                sender = 'K1C5'\n",
    "            else:\n",
    "                receiver = 'K1C5'\n",
    "        cross_border = 1 if sender[:2] != receiver[:2] else 0\n",
    "        cash_equivalent = np.random.choice([0, 1], p=[0.6, 0.4])\n",
    "        account_balance_ratio = np.random.beta(2, 5)\n",
    "        \n",
    "        if typology == 'structuring':\n",
    "            # Structuring: multiple small transactions to avoid detection by kingdom guards\n",
    "            n_splits = np.random.randint(3, 10)\n",
    "            split_amounts = np.full(n_splits, base_amount / n_splits)\n",
    "            # Add some variation\n",
    "            split_amounts += np.random.normal(0, base_amount * 0.05, n_splits)\n",
    "            split_amounts = np.maximum(split_amounts, 1)  # Ensure positive\n",
    "            \n",
    "            # Close timings: same day, hours within 24h\n",
    "            base_hour = np.random.choice([0, 1, 2, 3, 22, 23]) if np.random.random() < 0.6 else np.random.randint(0, 24)\n",
    "            hours = np.sort(np.random.randint(base_hour - 12, base_hour + 12, n_splits) % 24)\n",
    "            \n",
    "            for j in range(n_splits):\n",
    "                amount = split_amounts[j]\n",
    "                if np.random.random() < 0.7:\n",
    "                    amount = round(amount, -2)  # Round to nearest 100\n",
    "                \n",
    "                data['transaction_id'].append(f'TXN_{len(data[\"transaction_id\"]) + 1:06d}')\n",
    "                data['amount'].append(amount)\n",
    "                data['account_age_days'].append(account_age)\n",
    "                data['transaction_hour'].append(hours[j])\n",
    "                data['day_of_week'].append(day_of_week)\n",
    "                data['transactions_last_24h'].append(n_splits)\n",
    "                data['account_balance_ratio'].append(account_balance_ratio)\n",
    "                data['merchant_risk_score'].append(merchant_risk)\n",
    "                data['cross_border'].append(cross_border)\n",
    "                data['cash_equivalent'].append(cash_equivalent)\n",
    "                data['transaction_type'].append('cash_deposit' if np.random.random() < 0.5 else transaction_type)\n",
    "                data['customer_segment'].append(customer_segment)\n",
    "                data['sender_location'].append(sender)\n",
    "                data['receiver_location'].append(receiver)\n",
    "                data['is_money_laundering'].append(1)\n",
    "        \n",
    "        elif typology == 'layering':\n",
    "            # Layering: cross-kingdom transfers to obscure origins, often through K1C5\n",
    "            hop_count = np.random.randint(3, 8)  # Not directly used, but influence risk\n",
    "            amount = base_amount\n",
    "            if np.random.random() < 0.7:\n",
    "                amount = round(amount, -2)\n",
    "            hour = np.random.choice([0, 1, 2, 3, 22, 23]) if np.random.random() < 0.6 else np.random.randint(0, 24)\n",
    "            transactions_last_24h = np.random.randint(5, 15)\n",
    "            cross_border = 1  # Always cross-border for layering\n",
    "            transaction_type = 'wire_transfer' if np.random.random() < 0.7 else 'online_transfer'\n",
    "            # Ensure different kingdoms if needed\n",
    "            if sender[:2] == receiver[:2]:\n",
    "                receiver = select_location(is_ml=True, typology=typology)\n",
    "            \n",
    "            data['transaction_id'].append(f'TXN_{len(data[\"transaction_id\"]) + 1:06d}')\n",
    "            data['amount'].append(amount)\n",
    "            data['account_age_days'].append(account_age)\n",
    "            data['transaction_hour'].append(hour)\n",
    "            data['day_of_week'].append(day_of_week)\n",
    "            data['transactions_last_24h'].append(transactions_last_24h)\n",
    "            data['account_balance_ratio'].append(account_balance_ratio)\n",
    "            data['merchant_risk_score'].append(merchant_risk * 1.2)  # Slightly higher\n",
    "            data['cross_border'].append(cross_border)\n",
    "            data['cash_equivalent'].append(0)\n",
    "            data['transaction_type'].append(transaction_type)\n",
    "            data['customer_segment'].append(customer_segment)\n",
    "            data['sender_location'].append(sender)\n",
    "            data['receiver_location'].append(receiver)\n",
    "            data['is_money_laundering'].append(1)\n",
    "        \n",
    "        else:  # integration\n",
    "            # Integration: blending funds as legitimate in kingdom economy, often via K1C5 merchants\n",
    "            amount = base_amount * np.random.uniform(0.8, 1.2)\n",
    "            if np.random.random() < 0.5:\n",
    "                amount = round(amount, -2)\n",
    "            hour = np.random.randint(0, 24)\n",
    "            transactions_last_24h = np.random.randint(1, 5)\n",
    "            cross_border = np.random.choice([0, 1], p=[0.7, 0.3])\n",
    "            transaction_type = np.random.choice(['check_deposit', 'cash_deposit'], p=[0.6, 0.4])\n",
    "            merchant_risk = np.random.beta(2, 8)  # Lower risk\n",
    "            \n",
    "            data['transaction_id'].append(f'TXN_{len(data[\"transaction_id\"]) + 1:06d}')\n",
    "            data['amount'].append(amount)\n",
    "            data['account_age_days'].append(account_age)\n",
    "            data['transaction_hour'].append(hour)\n",
    "            data['day_of_week'].append(day_of_week)\n",
    "            data['transactions_last_24h'].append(transactions_last_24h)\n",
    "            data['account_balance_ratio'].append(account_balance_ratio)\n",
    "            data['merchant_risk_score'].append(merchant_risk)\n",
    "            data['cross_border'].append(cross_border)\n",
    "            data['cash_equivalent'].append(1)\n",
    "            data['transaction_type'].append(transaction_type)\n",
    "            data['customer_segment'].append('business' if np.random.random() < 0.6 else customer_segment)\n",
    "            data['sender_location'].append(sender)\n",
    "            data['receiver_location'].append(receiver)\n",
    "            data['is_money_laundering'].append(1)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"✓ Total transactions generated: {len(df)} (approx {ml_ratio*100:.1f}% ML syndicate cases)\")\n",
    "    print(f\"  💰 Corrupt syndicate cases: {df['is_money_laundering'].sum()}\")\n",
    "    print(f\"  🏰 Transactions involving K1C5: {((df['sender_location'] == 'K1C5') | (df['receiver_location'] == 'K1C5')).sum()}\")\n",
    "    return df\n",
    "\n",
    "class SDVSynthesizerEngine:\n",
    "    \"\"\"\n",
    "    Multi-algorithm synthetic data generation using SDV for fantasy detective investigation:\n",
    "    - GaussianCopula (Statistical approach)\n",
    "    - CTGAN (Deep learning approach)  \n",
    "    - TVAE (Variational autoencoder approach)\n",
    "    - GraphNetwork (Network-based approach)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.synthesizers = {}\n",
    "        self.results = {}\n",
    "        \n",
    "    def prepare_metadata(self, df):\n",
    "        \"\"\"Create SDV metadata for the dataset\"\"\"\n",
    "        from sdv.metadata import SingleTableMetadata\n",
    "        \n",
    "        # Remove transaction_id for synthesis\n",
    "        df_clean = df.drop('transaction_id', axis=1)\n",
    "        \n",
    "        # Create metadata automatically\n",
    "        metadata = SingleTableMetadata()\n",
    "        metadata.detect_from_dataframe(df_clean)\n",
    "        \n",
    "        # Define categorical columns explicitly\n",
    "        categorical_cols = [\n",
    "            'transaction_type', 'customer_segment', 'cross_border', \n",
    "            'cash_equivalent', 'is_money_laundering', 'day_of_week',\n",
    "            'sender_location', 'receiver_location'\n",
    "        ]\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            if col in df_clean.columns:\n",
    "                metadata.update_column(col, sdtype='categorical')\n",
    "        \n",
    "        # Define numerical columns with constraints\n",
    "        numerical_cols = [\n",
    "            'amount', 'account_age_days', 'transaction_hour',\n",
    "            'transactions_last_24h', 'account_balance_ratio',\n",
    "            'merchant_risk_score'\n",
    "        ]\n",
    "        for col in numerical_cols:\n",
    "            if col in df_clean.columns:\n",
    "                metadata.update_column(col, sdtype='numerical')\n",
    "        \n",
    "        print(f\"✓ Created metadata for {len(df_clean.columns)} columns\")\n",
    "        return df_clean, metadata\n",
    "    \n",
    "    def synthesize_with_gaussian_copula(self, df, metadata, n_samples=1000):\n",
    "        \"\"\"Fast statistical approach - great for preserving correlations in syndicate patterns\"\"\"\n",
    "        print(\"📊 Conjuring with SDV GaussianCopula (Statistical Magic)...\")\n",
    "        \n",
    "        try:\n",
    "            from sdv.single_table import GaussianCopulaSynthesizer\n",
    "            \n",
    "            # Initialize synthesizer\n",
    "            synthesizer = GaussianCopulaSynthesizer(metadata)\n",
    "            \n",
    "            # Fit the model\n",
    "            synthesizer.fit(df)\n",
    "            \n",
    "            # Generate synthetic data\n",
    "            synthetic_data = synthesizer.sample(num_rows=n_samples)\n",
    "            \n",
    "            self.synthesizers['gaussian_copula'] = synthesizer\n",
    "            self.results['gaussian_copula'] = synthetic_data\n",
    "            \n",
    "            print(f\"✓ GaussianCopula: Generated {len(synthetic_data)} transactions\")\n",
    "            return synthetic_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ GaussianCopula failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def synthesize_with_ctgan(self, df, metadata, n_samples=1000, epochs=100):\n",
    "        \"\"\"Deep learning GAN approach - captures complex syndicate patterns\"\"\"\n",
    "        print(\"🧠 Channeling SDV CTGAN (Deep Learning Sorcery)...\")\n",
    "        \n",
    "        try:\n",
    "            from sdv.single_table import CTGANSynthesizer\n",
    "            \n",
    "            # Initialize with optimized parameters for speed\n",
    "            synthesizer = CTGANSynthesizer(\n",
    "                metadata,\n",
    "                epochs=epochs,\n",
    "                batch_size=500,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            # Fit the model\n",
    "            synthesizer.fit(df)\n",
    "            \n",
    "            # Generate synthetic data\n",
    "            synthetic_data = synthesizer.sample(num_rows=n_samples)\n",
    "            \n",
    "            self.synthesizers['ctgan'] = synthesizer\n",
    "            self.results['ctgan'] = synthetic_data\n",
    "            \n",
    "            print(f\"✓ CTGAN: Generated {len(synthetic_data)} transactions\")\n",
    "            return synthetic_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ CTGAN failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def synthesize_with_tvae(self, df, metadata, n_samples=1000, epochs=100):\n",
    "        \"\"\"Variational Autoencoder approach - good for detecting syndicate anomalies\"\"\"\n",
    "        print(\"🤖 Weaving with SDV TVAE (Variational Enchantment)...\")\n",
    "        \n",
    "        try:\n",
    "            from sdv.single_table import TVAESynthesizer\n",
    "            \n",
    "            # Initialize with optimized parameters\n",
    "            synthesizer = TVAESynthesizer(\n",
    "                metadata,\n",
    "                epochs=epochs,\n",
    "                batch_size=500,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            # Fit the model\n",
    "            synthesizer.fit(df)\n",
    "            \n",
    "            # Generate synthetic data\n",
    "            synthetic_data = synthesizer.sample(num_rows=n_samples)\n",
    "            \n",
    "            self.synthesizers['tvae'] = synthesizer\n",
    "            self.results['tvae'] = synthetic_data\n",
    "            \n",
    "            print(f\"✓ TVAE: Generated {len(synthetic_data)} transactions\")\n",
    "            return synthetic_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ TVAE failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def synthesize_with_graph_network(self, df, metadata, n_samples=1000, n_nodes=500):\n",
    "        \"\"\"Network-based approach - models transaction relationships in the syndicate\"\"\"\n",
    "        print(\"🕸️ Weaving Synthetic Transaction Network (Graph Magic)...\")\n",
    "        \n",
    "        try:\n",
    "            import networkx as nx\n",
    "            \n",
    "            # Initialize directed graph\n",
    "            G = nx.DiGraph()\n",
    "            np.random.seed(42)\n",
    "            \n",
    "            # Create nodes (entities: customers and merchants)\n",
    "            n_customers = n_nodes // 2\n",
    "            n_merchants = n_nodes - n_customers\n",
    "            customers = [f'CUST_{i:05d}' for i in range(1, n_customers + 1)]\n",
    "            merchants = [f'MERCH_{i:04d}' for i in range(1, n_merchants + 1)]\n",
    "            \n",
    "            # Get all unique locations from the original data\n",
    "            all_locations = list(set(df['sender_location'].unique()) | set(df['receiver_location'].unique()))\n",
    "            \n",
    "            # Add nodes with attributes\n",
    "            for customer in customers:\n",
    "                # 30% of customers are in K1C5 (the corrupt city)\n",
    "                location = 'K1C5' if np.random.random() < 0.3 else np.random.choice(\n",
    "                    [loc for loc in all_locations if loc != 'K1C5'] if len([loc for loc in all_locations if loc != 'K1C5']) > 0 else all_locations\n",
    "                )\n",
    "                G.add_node(customer, type='customer', location=location)\n",
    "            \n",
    "            for merchant in merchants:\n",
    "                # 25% of merchants are in K1C5\n",
    "                location = 'K1C5' if np.random.random() < 0.25 else np.random.choice(\n",
    "                    [loc for loc in all_locations if loc != 'K1C5'] if len([loc for loc in all_locations if loc != 'K1C5']) > 0 else all_locations\n",
    "                )\n",
    "                G.add_node(merchant, type='merchant', location=location)\n",
    "            \n",
    "            # Create lists of nodes by type and location for efficient selection\n",
    "            customers_k1c5 = [n for n, d in G.nodes(data=True) if d['type'] == 'customer' and d['location'] == 'K1C5']\n",
    "            merchants_k1c5 = [n for n, d in G.nodes(data=True) if d['type'] == 'merchant' and d['location'] == 'K1C5']\n",
    "            customers_other = [n for n, d in G.nodes(data=True) if d['type'] == 'customer' and d['location'] != 'K1C5']\n",
    "            merchants_other = [n for n, d in G.nodes(data=True) if d['type'] == 'merchant' and d['location'] != 'K1C5']\n",
    "            \n",
    "            # Ensure we have nodes in each category\n",
    "            if not customers_k1c5:\n",
    "                customers_k1c5 = customers[:5]  # Fallback to first 5 customers\n",
    "                for c in customers_k1c5:\n",
    "                    G.nodes[c]['location'] = 'K1C5'\n",
    "            if not merchants_k1c5:\n",
    "                merchants_k1c5 = merchants[:5]  # Fallback to first 5 merchants\n",
    "                for m in merchants_k1c5:\n",
    "                    G.nodes[m]['location'] = 'K1C5'\n",
    "            if not customers_other:\n",
    "                customers_other = customers[5:] if len(customers) > 5 else customers\n",
    "            if not merchants_other:\n",
    "                merchants_other = merchants[5:] if len(merchants) > 5 else merchants\n",
    "            \n",
    "            # Generate edges (transactions) with attributes\n",
    "            ml_ratio = df['is_money_laundering'].mean()\n",
    "            edges = []\n",
    "            \n",
    "            for _ in range(n_samples):\n",
    "                # Determine if this is a money laundering transaction\n",
    "                is_ml = np.random.random() < ml_ratio\n",
    "                \n",
    "                # Select sender and receiver based on ML status\n",
    "                if is_ml and np.random.random() < 0.7:  # 70% of ML involves K1C5\n",
    "                    if np.random.random() < 0.5:\n",
    "                        # Sender from K1C5\n",
    "                        sender = np.random.choice(customers_k1c5)\n",
    "                        receiver = np.random.choice(merchants_k1c5 + merchants_other)\n",
    "                    else:\n",
    "                        # Receiver in K1C5\n",
    "                        sender = np.random.choice(customers_k1c5 + customers_other)\n",
    "                        receiver = np.random.choice(merchants_k1c5)\n",
    "                else:\n",
    "                    # Normal transaction pattern\n",
    "                    sender = np.random.choice(customers)\n",
    "                    receiver = np.random.choice(merchants)\n",
    "                \n",
    "                # Get locations\n",
    "                sender_location = G.nodes[sender]['location']\n",
    "                receiver_location = G.nodes[receiver]['location']\n",
    "                \n",
    "                # Calculate cross_border\n",
    "                cross_border = 1 if sender_location[:2] != receiver_location[:2] else 0\n",
    "                \n",
    "                # Generate transaction attributes based on ML status\n",
    "                if is_ml:\n",
    "                    # ML transactions tend to be larger and use specific types\n",
    "                    amount = np.random.lognormal(mean=7, sigma=2)\n",
    "                    transaction_type = np.random.choice(\n",
    "                        ['wire_transfer', 'cash_deposit', 'online_transfer'],\n",
    "                        p=[0.6, 0.3, 0.1]\n",
    "                    )\n",
    "                    merchant_risk_score = np.random.beta(8, 2)  # Higher risk\n",
    "                    cash_equivalent = np.random.choice([0, 1], p=[0.6, 0.4])\n",
    "                    transactions_last_24h = np.random.poisson(5)  # More activity\n",
    "                else:\n",
    "                    # Normal transactions\n",
    "                    amount = np.random.lognormal(mean=6, sigma=2)\n",
    "                    transaction_type = np.random.choice(\n",
    "                        df['transaction_type'].unique(),\n",
    "                        p=df['transaction_type'].value_counts(normalize=True).values\n",
    "                    )\n",
    "                    merchant_risk_score = np.random.beta(2, 8)  # Lower risk\n",
    "                    cash_equivalent = np.random.choice([0, 1], p=[0.95, 0.05])\n",
    "                    transactions_last_24h = np.random.poisson(2)\n",
    "                \n",
    "                # Create edge data\n",
    "                edge_data = {\n",
    "                    'sender': sender,\n",
    "                    'receiver': receiver,\n",
    "                    'amount': max(1, amount),  # Ensure positive amount\n",
    "                    'transaction_type': transaction_type,\n",
    "                    'cross_border': cross_border,\n",
    "                    'is_money_laundering': int(is_ml),\n",
    "                    'account_age_days': int(np.random.gamma(shape=2, scale=365)),\n",
    "                    'transaction_hour': np.random.randint(0, 24),\n",
    "                    'day_of_week': np.random.randint(0, 7),\n",
    "                    'transactions_last_24h': transactions_last_24h,\n",
    "                    'account_balance_ratio': np.random.beta(2, 5),\n",
    "                    'merchant_risk_score': merchant_risk_score,\n",
    "                    'cash_equivalent': cash_equivalent,\n",
    "                    'customer_segment': np.random.choice(\n",
    "                        df['customer_segment'].unique(),\n",
    "                        p=df['customer_segment'].value_counts(normalize=True).values\n",
    "                    ),\n",
    "                    'sender_location': sender_location,\n",
    "                    'receiver_location': receiver_location\n",
    "                }\n",
    "                \n",
    "                edges.append(edge_data)\n",
    "                G.add_edge(sender, receiver, **edge_data)\n",
    "            \n",
    "            # Convert edges to DataFrame\n",
    "            synthetic_data = pd.DataFrame(edges)\n",
    "            \n",
    "            # Drop the sender and receiver columns as they're not in the original schema\n",
    "            synthetic_data = synthetic_data.drop(['sender', 'receiver'], axis=1)\n",
    "            \n",
    "            # Ensure column order matches original (minus transaction_id)\n",
    "            original_cols = [col for col in df.columns if col != 'transaction_id']\n",
    "            synthetic_data = synthetic_data[original_cols]\n",
    "            \n",
    "            self.synthesizers['graph_network'] = G\n",
    "            self.results['graph_network'] = synthetic_data\n",
    "            \n",
    "            # Print statistics\n",
    "            ml_count = synthetic_data['is_money_laundering'].sum()\n",
    "            k1c5_count = ((synthetic_data['sender_location'] == 'K1C5') | \n",
    "                        (synthetic_data['receiver_location'] == 'K1C5')).sum()\n",
    "            \n",
    "            print(f\"✓ GraphNetwork: Generated {len(synthetic_data)} transactions\")\n",
    "            print(f\"  Network: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "            print(f\"  ML transactions: {ml_count} ({ml_count/len(synthetic_data)*100:.1f}%)\")\n",
    "            print(f\"  K1C5 involvement: {k1c5_count} ({k1c5_count/len(synthetic_data)*100:.1f}%)\")\n",
    "            \n",
    "            return synthetic_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ GraphNetwork failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "def evaluate_synthetic_quality(original_df, synthetic_results, metadata):\n",
    "    \"\"\"Comprehensive evaluation using SDV quality metrics for detective analysis\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🔍 PHANTOM QUALITY ASSESSMENT - SDV MULTI-METHOD ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not synthetic_results:\n",
    "        print(\"❌ No synthetic phantoms to evaluate!\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        from sdv.evaluation.single_table import evaluate_quality\n",
    "        \n",
    "        evaluation_results = {}\n",
    "        \n",
    "        for method_name, synthetic_df in synthetic_results.items():\n",
    "            print(f\"\\n🎭 Evaluating {method_name.upper()} Phantoms:\")\n",
    "            print(\"-\" * 45)\n",
    "            \n",
    "            try:\n",
    "                # Ensure synthetic_df has the same columns as original_df (excluding transaction_id)\n",
    "                synthetic_df = synthetic_df[original_df.columns]\n",
    "                \n",
    "                # SDV quality evaluation with metadata\n",
    "                quality_report = evaluate_quality(\n",
    "                    real_data=original_df,\n",
    "                    synthetic_data=synthetic_df,\n",
    "                    metadata=metadata\n",
    "                )\n",
    "                overall_score = quality_report.get_score()\n",
    "                \n",
    "                print(f\"🏆 Overall Quality Score: {overall_score:.3f}/1.000\")\n",
    "                \n",
    "                # Detailed breakdown\n",
    "                column_shapes = quality_report.get_details('Column Shapes')\n",
    "                column_pair_trends = quality_report.get_details('Column Pair Trends')\n",
    "                \n",
    "                print(f\"📊 Column Shape Score: {column_shapes['Score'].mean():.3f}\")\n",
    "                print(f\"🔗 Pair Trends Score: {column_pair_trends['Score'].mean():.3f}\")\n",
    "                \n",
    "                evaluation_results[method_name] = {\n",
    "                    'overall_score': overall_score,\n",
    "                    'column_shapes': column_shapes['Score'].mean(),\n",
    "                    'pair_trends': column_pair_trends['Score'].mean()\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ SDV evaluation failed for {method_name}: {e}\")\n",
    "                # Fallback to manual evaluation\n",
    "                evaluation_results[method_name] = manual_evaluation(original_df, synthetic_df, method_name)\n",
    "        \n",
    "        # Summary comparison\n",
    "        print(f\"\\n🏅 PHANTOM SYNTHESIS LEADERBOARD:\")\n",
    "        print(\"-\" * 40)\n",
    "        sorted_results = sorted(evaluation_results.items(), \n",
    "                               key=lambda x: x[1]['overall_score'], reverse=True)\n",
    "        \n",
    "        for rank, (method, scores) in enumerate(sorted_results, 1):\n",
    "            print(f\"{rank}. {method.upper()}: {scores['overall_score']:.3f}\")\n",
    "        \n",
    "        return evaluation_results\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"⚠️ SDV evaluation metrics not available, using manual assessment...\")\n",
    "        return manual_evaluation_all(original_df, synthetic_results)\n",
    "\n",
    "def manual_evaluation(original_df, synthetic_df, method_name):\n",
    "    \"\"\"Manual quality assessment when SDV evaluation not available\"\"\"\n",
    "    \n",
    "    # Ensure synthetic_df has the same columns as original_df\n",
    "    synthetic_df = synthetic_df[original_df.columns]\n",
    "    \n",
    "    # ML ratio preservation\n",
    "    orig_ml_rate = original_df['is_money_laundering'].mean()\n",
    "    synth_ml_rate = synthetic_df['is_money_laundering'].mean()\n",
    "    ml_score = max(0, 1 - abs(orig_ml_rate - synth_ml_rate) / orig_ml_rate) if orig_ml_rate > 0 else 0\n",
    "    \n",
    "    # Amount distribution similarity\n",
    "    orig_amount_mean = original_df['amount'].mean()\n",
    "    synth_amount_mean = synthetic_df['amount'].mean()\n",
    "    amount_score = max(0, 1 - abs(orig_amount_mean - synth_amount_mean) / orig_amount_mean) if orig_amount_mean > 0 else 0\n",
    "    \n",
    "    # Data validity checks\n",
    "    hour_valid = ((synthetic_df['transaction_hour'] >= 0) & \n",
    "                  (synthetic_df['transaction_hour'] <= 23)).all() if 'transaction_hour' in synthetic_df.columns else False\n",
    "    amount_positive = (synthetic_df['amount'] > 0).all() if 'amount' in synthetic_df.columns else False\n",
    "    \n",
    "    validity_score = (hour_valid + amount_positive) / 2\n",
    "    \n",
    "    print(f\"💰 ML Rate Score: {ml_score:.3f}\")\n",
    "    print(f\"💵 Amount Score: {amount_score:.3f}\")\n",
    "    print(f\"✅ Validity Score: {validity_score:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'overall_score': (ml_score + amount_score + validity_score) / 3,\n",
    "        'ml_rate_score': ml_score,\n",
    "        'amount_score': amount_score,\n",
    "        'validity_score': validity_score\n",
    "    }\n",
    "\n",
    "def manual_evaluation_all(original_df, synthetic_results):\n",
    "    \"\"\"Manual evaluation for all methods\"\"\"\n",
    "    \n",
    "    evaluation_results = {}\n",
    "    for method_name, synthetic_df in synthetic_results.items():\n",
    "        print(f\"\\n🎭 Evaluating {method_name.upper()} Phantoms:\")\n",
    "        print(\"-\" * 45)\n",
    "        evaluation_results[method_name] = manual_evaluation(original_df, synthetic_df, method_name)\n",
    "    \n",
    "    return evaluation_results\n",
    "\n",
    "def create_customer_merchant_networks(df, n_customers=500, n_merchants=200):\n",
    "    \"\"\"Generate customer-merchant relationship networks with suspicious syndicate patterns\"\"\"\n",
    "    \n",
    "    print(\"\\n🕸️ Weaving Customer-Merchant Networks...\")\n",
    "    \n",
    "    # Create customer and merchant IDs in fantasy theme\n",
    "    customers = [f'CUST_{i:05d}' for i in range(1, n_customers + 1)]\n",
    "    merchants = [f'MERCH_{i:04d}' for i in range(1, n_merchants + 1)]\n",
    "    \n",
    "    # Add network relationships to transactions\n",
    "    df_network = df.copy()\n",
    "    df_network['customer_id'] = np.random.choice(customers, len(df))\n",
    "    df_network['merchant_id'] = np.random.choice(merchants, len(df))\n",
    "    \n",
    "    # Create suspicious clustering patterns for ML cases, focusing on K1C5 syndicate\n",
    "    ml_mask = df_network['is_money_laundering'] == 1\n",
    "    \n",
    "    if ml_mask.sum() > 0:\n",
    "        # High-risk merchant cluster (syndicate-linked)\n",
    "        high_risk_merchants = merchants[:int(len(merchants) * 0.1)]  # Top 10% as high-risk\n",
    "        df_network.loc[ml_mask, 'merchant_id'] = np.random.choice(\n",
    "            high_risk_merchants, ml_mask.sum())\n",
    "        \n",
    "        # Suspicious customer cluster (layering pattern in corrupt city)\n",
    "        suspicious_customers = customers[:int(len(customers) * 0.1)]  # Top 10% suspicious\n",
    "        df_network.loc[ml_mask, 'customer_id'] = np.random.choice(\n",
    "            suspicious_customers, ml_mask.sum())\n",
    "    \n",
    "    print(f\"✓ Network created: {len(customers)} customers, {len(merchants)} merchants\")\n",
    "    print(f\"  High-risk merchants: {len(high_risk_merchants) if ml_mask.sum() > 0 else 0}\")\n",
    "    print(f\"  Suspicious customers: {len(suspicious_customers) if ml_mask.sum() > 0 else 0}\")\n",
    "    \n",
    "    return df_network\n",
    "\n",
    "def generate_time_series_flows(df, days=30):\n",
    "    \"\"\"Generate realistic time-series financial flow patterns for investigation\"\"\"\n",
    "    \n",
    "    print(\"\\n⏰ Conjuring Time-Series Financial Flows...\")\n",
    "    \n",
    "    # Create realistic timestamps\n",
    "    start_date = datetime(2024, 1, 1)\n",
    "    timestamps = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        day_offset = int(np.random.randint(0, days))\n",
    "        hour = int(df.iloc[i]['transaction_hour'])\n",
    "        minute = int(np.random.randint(0, 60))\n",
    "        second = int(np.random.randint(0, 60))\n",
    "        \n",
    "        timestamp = start_date + timedelta(days=day_offset, hours=hour, \n",
    "                                         minutes=minute, seconds=second)\n",
    "        timestamps.append(timestamp)\n",
    "    \n",
    "    df_timeseries = df.copy()\n",
    "    df_timeseries['timestamp'] = pd.to_datetime(timestamps)  # Ensure datetime type\n",
    "    \n",
    "    # Sort by timestamp to ensure chronological order\n",
    "    df_timeseries = df_timeseries.sort_values('timestamp').reset_index(drop=True)\n",
    "    \n",
    "    # Add velocity and pattern features\n",
    "    if 'customer_id' in df_timeseries.columns:\n",
    "        # Running totals by customer\n",
    "        df_timeseries['running_total'] = df_timeseries.groupby('customer_id')['amount'].cumsum()\n",
    "        \n",
    "        # Transaction frequency features\n",
    "        df_timeseries['hour_of_day'] = df_timeseries['timestamp'].dt.hour\n",
    "        df_timeseries['day_of_week'] = df_timeseries['timestamp'].dt.dayofweek\n",
    "        \n",
    "        # Set timestamp as index for rolling operations\n",
    "        df_timeseries = df_timeseries.set_index('timestamp')\n",
    "        \n",
    "        # Velocity indicators (amount of money moved recently)\n",
    "        df_timeseries = df_timeseries.sort_values(['customer_id', 'timestamp'])\n",
    "        df_timeseries['velocity_1h'] = df_timeseries.groupby('customer_id')['amount'].rolling(\n",
    "            window='1H', min_periods=1).sum().reset_index(level=0, drop=True)\n",
    "        df_timeseries['velocity_24h'] = df_timeseries.groupby('customer_id')['amount'].rolling(\n",
    "            window='24H', min_periods=1).sum().reset_index(level=0, drop=True)\n",
    "        \n",
    "        # Reset index to make timestamp a column again\n",
    "        df_timeseries = df_timeseries.reset_index()\n",
    "    \n",
    "    print(f\"✓ Time-series flows generated over {days} days\")\n",
    "    print(f\"  Features: timestamp, running_total, velocity_1h, velocity_24h\")\n",
    "    \n",
    "    return df_timeseries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏰==========================================================🏰\n",
      "     FANTASY KINGDOM AML INVESTIGATION - MISSION 1\n",
      "         Synthetic Data Generation & Evaluation\n",
      "🏰==========================================================🏰\n",
      "\n",
      "🎯 STEP 1: Generating Base Transaction Data...\n",
      "✓ Total transactions generated: 3349 (approx 5.0% ML syndicate cases)\n",
      "  💰 Corrupt syndicate cases: 499\n",
      "  🏰 Transactions involving K1C5: 489\n",
      "\n",
      "🎯 STEP 2: Initializing Multi-Algorithm Synthesis Engine...\n",
      "\n",
      "🎯 STEP 3: Preparing SDV Metadata...\n",
      "✓ Created metadata for 14 columns\n",
      "\n",
      "🎯 STEP 4: Running All Synthesis Methods...\n",
      "📊 Conjuring with SDV GaussianCopula (Statistical Magic)...\n",
      "✓ GaussianCopula: Generated 1000 transactions\n",
      "🧠 Channeling SDV CTGAN (Deep Learning Sorcery)...\n",
      "✓ CTGAN: Generated 1000 transactions\n",
      "🤖 Weaving with SDV TVAE (Variational Enchantment)...\n",
      "✓ TVAE: Generated 1000 transactions\n",
      "🕸️ Weaving Synthetic Transaction Network (Graph Magic)...\n",
      "✓ GraphNetwork: Generated 1000 transactions\n",
      "  Network: 500 nodes, 994 edges\n",
      "  ML transactions: 158 (15.8%)\n",
      "  K1C5 involvement: 524 (52.4%)\n",
      "\n",
      "🎯 STEP 5: Comprehensive Quality Evaluation...\n",
      "\n",
      "============================================================\n",
      "🔍 PHANTOM QUALITY ASSESSMENT - SDV MULTI-METHOD ANALYSIS\n",
      "============================================================\n",
      "\n",
      "🎭 Evaluating GAUSSIAN_COPULA Phantoms:\n",
      "---------------------------------------------\n",
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Column Shapes: |██████████| 14/14 [00:00<00:00, 1491.23it/s]|\n",
      "Column Shapes Score: 94.76%\n",
      "\n",
      "(2/2) Evaluating Column Pair Trends: |██████████| 91/91 [00:00<00:00, 485.25it/s]|\n",
      "Column Pair Trends Score: 87.08%\n",
      "\n",
      "Overall Score (Average): 90.92%\n",
      "\n",
      "🏆 Overall Quality Score: 0.909/1.000\n",
      "📊 Column Shape Score: 0.948\n",
      "🔗 Pair Trends Score: 0.871\n",
      "\n",
      "🎭 Evaluating CTGAN Phantoms:\n",
      "---------------------------------------------\n",
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Column Shapes: |██████████| 14/14 [00:00<00:00, 312.84it/s]|\n",
      "Column Shapes Score: 88.78%\n",
      "\n",
      "(2/2) Evaluating Column Pair Trends: |██████████| 91/91 [00:00<00:00, 529.45it/s]|\n",
      "Column Pair Trends Score: 82.47%\n",
      "\n",
      "Overall Score (Average): 85.62%\n",
      "\n",
      "🏆 Overall Quality Score: 0.856/1.000\n",
      "📊 Column Shape Score: 0.888\n",
      "🔗 Pair Trends Score: 0.825\n",
      "\n",
      "🎭 Evaluating TVAE Phantoms:\n",
      "---------------------------------------------\n",
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Column Shapes: |██████████| 14/14 [00:00<00:00, 224.24it/s]|\n",
      "Column Shapes Score: 62.96%\n",
      "\n",
      "(2/2) Evaluating Column Pair Trends: |██████████| 91/91 [00:00<00:00, 496.27it/s]|\n",
      "Column Pair Trends Score: 51.0%\n",
      "\n",
      "Overall Score (Average): 56.98%\n",
      "\n",
      "🏆 Overall Quality Score: 0.570/1.000\n",
      "📊 Column Shape Score: 0.630\n",
      "🔗 Pair Trends Score: 0.510\n",
      "\n",
      "🎭 Evaluating GRAPH_NETWORK Phantoms:\n",
      "---------------------------------------------\n",
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Column Shapes: |██████████| 14/14 [00:00<00:00, 560.00it/s]|\n",
      "Column Shapes Score: 91.6%\n",
      "\n",
      "(2/2) Evaluating Column Pair Trends: |██████████| 91/91 [00:00<00:00, 520.06it/s]|\n",
      "Column Pair Trends Score: 81.55%\n",
      "\n",
      "Overall Score (Average): 86.58%\n",
      "\n",
      "🏆 Overall Quality Score: 0.866/1.000\n",
      "📊 Column Shape Score: 0.916\n",
      "🔗 Pair Trends Score: 0.816\n",
      "\n",
      "🏅 PHANTOM SYNTHESIS LEADERBOARD:\n",
      "----------------------------------------\n",
      "1. GAUSSIAN_COPULA: 0.909\n",
      "2. GRAPH_NETWORK: 0.866\n",
      "3. CTGAN: 0.856\n",
      "4. TVAE: 0.570\n",
      "\n",
      "🎯 STEP 6: Generating Enhanced Network Features...\n",
      "\n",
      "🕸️ Weaving Customer-Merchant Networks...\n",
      "✓ Network created: 500 customers, 200 merchants\n",
      "  High-risk merchants: 20\n",
      "  Suspicious customers: 50\n",
      "\n",
      "🎯 STEP 7: Generating Time-Series Financial Flows...\n",
      "\n",
      "⏰ Conjuring Time-Series Financial Flows...\n",
      "✓ Time-series flows generated over 30 days\n",
      "  Features: timestamp, running_total, velocity_1h, velocity_24h\n",
      "\n",
      "🏆 MISSION 1 COMPLETE!\n",
      "==================================================\n",
      "📊 Original Data: 3,349 transactions\n",
      "🎭 Best Method: GAUSSIAN_COPULA\n",
      "✨ Final Dataset: 1,000 transactions\n",
      "💰 ML Cases: 135\n",
      "🏰 K1C5 Involvement: 185\n",
      "🕸️ Network Entities: 421 customers, 197 merchants\n",
      "⏰ Time Range: 2024-01-01 00:14:20 to 2024-01-30 23:15:54\n",
      "\n",
      "🎉 Ready for Mission 2: Detective ML Model Training!\n"
     ]
    }
   ],
   "source": [
    "# Fantasy AML Mission 1 - Complete Runner Script\n",
    "# Run this after your main code to execute and evaluate all synthesis methods\n",
    "\n",
    "def run_complete_mission_1():\n",
    "    \"\"\"Execute the complete Mission 1 pipeline\"\"\"\n",
    "    \n",
    "    print(\"🏰\" + \"=\"*58 + \"🏰\")\n",
    "    print(\"     FANTASY KINGDOM AML INVESTIGATION - MISSION 1\")\n",
    "    print(\"         Synthetic Data Generation & Evaluation\")\n",
    "    print(\"🏰\" + \"=\"*58 + \"🏰\")\n",
    "    \n",
    "    # Step 1: Generate base transaction data\n",
    "    print(\"\\n🎯 STEP 1: Generating Base Transaction Data...\")\n",
    "    original_df = create_base_transaction_data(n_samples=3000, ml_ratio=0.05)\n",
    "    \n",
    "    # Step 2: Initialize SDV synthesis engine\n",
    "    print(\"\\n🎯 STEP 2: Initializing Multi-Algorithm Synthesis Engine...\")\n",
    "    engine = SDVSynthesizerEngine()\n",
    "    \n",
    "    # Step 3: Prepare metadata\n",
    "    print(\"\\n🎯 STEP 3: Preparing SDV Metadata...\")\n",
    "    df_clean, metadata = engine.prepare_metadata(original_df)\n",
    "    \n",
    "    # Step 4: Run all synthesis methods\n",
    "    print(\"\\n🎯 STEP 4: Running All Synthesis Methods...\")\n",
    "    \n",
    "    # Method 1: GaussianCopula (Fast statistical approach)\n",
    "    gaussian_result = engine.synthesize_with_gaussian_copula(\n",
    "        df_clean, metadata, n_samples=1000\n",
    "    )\n",
    "    \n",
    "    # Method 2: CTGAN (Deep learning - reduce epochs for demo)\n",
    "    ctgan_result = engine.synthesize_with_ctgan(\n",
    "        df_clean, metadata, n_samples=1000, epochs=50  # Reduced for demo\n",
    "    )\n",
    "    \n",
    "    # Method 3: TVAE (Variational autoencoder)\n",
    "    tvae_result = engine.synthesize_with_tvae(\n",
    "        df_clean, metadata, n_samples=1000, epochs=50  # Reduced for demo\n",
    "    )\n",
    "    \n",
    "    # Method 4: Graph Network (Custom network-based)\n",
    "    graph_result = engine.synthesize_with_graph_network(\n",
    "        df_clean, metadata, n_samples=1000\n",
    "    )\n",
    "    \n",
    "    # Step 5: Quality evaluation\n",
    "    print(\"\\n🎯 STEP 5: Comprehensive Quality Evaluation...\")\n",
    "    evaluation_results = evaluate_synthetic_quality(\n",
    "        df_clean, engine.results, metadata\n",
    "    )\n",
    "    \n",
    "    # Step 6: Enhanced features\n",
    "    print(\"\\n🎯 STEP 6: Generating Enhanced Network Features...\")\n",
    "    \n",
    "    # Add customer-merchant networks to the best performing method\n",
    "    best_method = max(evaluation_results.items(), key=lambda x: x[1]['overall_score'])[0]\n",
    "    best_synthetic = engine.results[best_method].copy()\n",
    "    \n",
    "    # Add transaction IDs back\n",
    "    best_synthetic['transaction_id'] = [f'SYN_{i+1:06d}' for i in range(len(best_synthetic))]\n",
    "    \n",
    "    # Create network features\n",
    "    df_with_networks = create_customer_merchant_networks(\n",
    "        best_synthetic, n_customers=500, n_merchants=200\n",
    "    )\n",
    "    \n",
    "    # Step 7: Time-series flows\n",
    "    print(\"\\n🎯 STEP 7: Generating Time-Series Financial Flows...\")\n",
    "    final_df = generate_time_series_flows(df_with_networks, days=30)\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n🏆 MISSION 1 COMPLETE!\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"📊 Original Data: {len(original_df):,} transactions\")\n",
    "    print(f\"🎭 Best Method: {best_method.upper()}\")\n",
    "    print(f\"✨ Final Dataset: {len(final_df):,} transactions\")\n",
    "    print(f\"💰 ML Cases: {final_df['is_money_laundering'].sum():,}\")\n",
    "    print(f\"🏰 K1C5 Involvement: {((final_df['sender_location'] == 'K1C5') | (final_df['receiver_location'] == 'K1C5')).sum():,}\")\n",
    "    print(f\"🕸️ Network Entities: {final_df['customer_id'].nunique():,} customers, {final_df['merchant_id'].nunique():,} merchants\")\n",
    "    print(f\"⏰ Time Range: {final_df['timestamp'].min()} to {final_df['timestamp'].max()}\")\n",
    "    \n",
    "    return {\n",
    "        'original_data': original_df,\n",
    "        'synthetic_results': engine.results,\n",
    "        'evaluation_results': evaluation_results,\n",
    "        'best_method': best_method,\n",
    "        'final_enhanced_data': final_df,\n",
    "        'engine': engine\n",
    "    }\n",
    "\n",
    "# Run the complete mission\n",
    "if __name__ == \"__main__\":\n",
    "    # Execute Mission 1\n",
    "    results = run_complete_mission_1()\n",
    "    \n",
    "    \n",
    "    print(\"\\n🎉 Ready for Mission 2: Detective ML Model Training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mission 1 Review\n",
    "\n",
    "\n",
    "Your task was to forge a synthetic dataset of financial transactions—a magical ledger mimicking real commerce while guarding sensitive secrets. This dataset powers our anti-money laundering (AML) efforts, targeting shady syndicates in K1C5 (Kingdom 1, City 5).\n",
    "\n",
    "You ran the data generation and got these scores:\n",
    "\n",
    "- **GAUSSIAN_COPULA**: 0.909\n",
    "- **CTGAN**: 0.869\n",
    "- **GRAPH_NETWORK**: 0.862\n",
    "- **TVAE**: 0.579\n",
    "\n",
    "These scores, from the `SDV` library, measure how well the synthetic data matches the real dataset’s patterns (e.g., via log-likelihood or similarity metrics). Higher scores mean better fidelity. Let’s explore the four methods behind your enchanted ledger!\n",
    "\n",
    "## The Four Methods of Synthetic Data Creation\n",
    "\n",
    "You used four methods to conjure synthetic transactions, each like a unique spell in the Arcane Library, designed to replicate the Kingdom’s financial patterns, including K1C5’s suspicious activities. Here’s what each does and why your scores matter.\n",
    "\n",
    "### 1. Gaussian Copula (Score: 0.909)\n",
    "**What It Does**: Models relationships between features (e.g., amount, sender_location) using a copula, assuming data fits a Gaussian distribution.\n",
    "\n",
    "**Key Points**:\n",
    "- Captures correlations (e.g., high amounts with cross-border transactions).\n",
    "- Uses `sdv.tabular.GaussianCopula` with `ml_ratio=0.05` (5% money laundering).\n",
    "- Fast and great for numerical data.\n",
    "\n",
    "**Your Score**: 0.909 is the highest, showing excellent fidelity for transaction patterns.\n",
    "\n",
    "### 2. CTGAN (Score: 0.869)\n",
    "**What It Does**: Uses a GAN (generator vs. discriminator) to create realistic transactions, modeling complex patterns.\n",
    "\n",
    "**Key Points**:\n",
    "- Handles numerical and categorical features (e.g., transaction_type).\n",
    "- Uses `ctgan>=0.11.0` with constraints.\n",
    "- Strong for money laundering patterns.\n",
    "\n",
    "**Your Score**: 0.869 indicates robust data, capturing intricate K1C5 behaviors.\n",
    "\n",
    "### 3. Graph Network (Score: 0.862)\n",
    "**What It Does**: Models transactions as a network (accounts as nodes, transactions as edges) to mimic syndicate structures.\n",
    "\n",
    "**Key Points**:\n",
    "- Uses `networkx>=3.4.2` with features like clustering coefficients.\n",
    "- Focuses on K1C5 syndicate patterns.\n",
    "- Ideal for graph-based AML detection.\n",
    "\n",
    "**Your Score**: 0.862 shows strong network fidelity, great for syndicate analysis.\n",
    "\n",
    "### 4. TVAE (Score: 0.579)\n",
    "**What It Does**: Uses a variational autoencoder to learn and generate transactions from a compressed data representation.\n",
    "\n",
    "**Key Points**:\n",
    "- Uses `sdv.tabular.TVAE` for complex patterns.\n",
    "- Good for mixed data but sensitive to tuning.\n",
    "- May struggle with sparse categories.\n",
    "\n",
    "**Your Score**: 0.579 suggests poor performance, likely due to tuning issues.\n",
    "\n",
    "## Understanding Your Scores\n",
    "The scores (0.909, 0.869, 0.862, 0.579) reflect synthetic data quality. Gaussian Copula (0.909) excels for numerical patterns, CTGAN (0.869) captures complex behaviors, Graph Network (0.862) suits syndicate detection, and TVAE (0.579) needs tuning. Use Gaussian Copula or CTGAN data for most AML tests, and Graph Network for K1C5 syndicates.\n",
    "\n",
    "Well done, Investigators! Your ledger is ready to unmask K1C5’s secrets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mission 2: The Document Forge\n",
    "**Objective**: Create realistic synthetic documents critical to a money laundering investigation, including a whistleblower report, bank statements, and suspicious activity report, to build a case against the syndicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔮 Beginning Mission 2: Summoning Investigation Documents...\n",
      "🏰 Target: The syndicate operating from Goldweave Port, Valdris Kingdom\n",
      "======================================================================\n",
      "📊 Generating synthetic transaction data aligned with Mission 1\n",
      "✨ Conjured whistleblower report: mission2_documents\\whistleblower_report_RG924718.pdf\n",
      "✨ Conjured banking house statement: mission2_documents\\bank_statement_265055.pdf\n",
      "✨ Conjured Suspicious Activity Report: mission2_documents\\SAR_883301.pdf\n",
      "======================================================================\n",
      "✅ Mission 2 Complete! Generated 3 investigation documents:\n",
      "   1. WHISTLEBLOWER REPORT: whistleblower_report_RG924718.pdf\n",
      "   2. BANK STATEMENT: bank_statement_265055.pdf\n",
      "   3. SUSPICIOUS ACTIVITY REPORT: SAR_883301.pdf\n",
      "\n",
      "📁 All documents saved to: mission2_documents/\n",
      "🎯 Investigation Focus: Golden Griffin Trading Co. in Goldweave Port\n",
      "📊 Investigation summary generated: mission2_documents\\Mission2_Investigation_Summary.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from typing import Dict, List, Any\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import markovify\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib.units import inch\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle\n",
    "from reportlab.lib import colors\n",
    "from reportlab.lib.enums import TA_LEFT, TA_CENTER, TA_RIGHT\n",
    "\n",
    "class FantasyDocumentForge:\n",
    "    \"\"\"Forge synthetic documents for the magical kingdom's financial investigation\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: str = \"mission2_documents\", seed: int = 42):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize text generation\n",
    "        self._setup_text_generators()\n",
    "        \n",
    "        # Fantasy kingdom structure (matching Mission 1)\n",
    "        self.kingdoms = {\n",
    "            'K1': 'Valdris', 'K2': 'Aethon', 'K3': 'Ironhold', 'K4': 'Mystwood'\n",
    "        }\n",
    "        \n",
    "        self.cities = {\n",
    "            'K1C1': 'Crownhaven', 'K1C2': 'Silverdale', 'K1C3': 'Ironbridge', \n",
    "            'K1C4': 'Stormwatch', 'K1C5': 'Goldweave Port',  # Syndicate base\n",
    "            'K2C1': 'Seafoam', 'K2C2': 'Coral Bay', 'K2C3': 'Tidecrest',\n",
    "            'K2C4': 'Wavebreak', 'K2C5': 'Saltmere',\n",
    "            'K3C1': 'Hammerfall', 'K3C2': 'Anvil Rock', 'K3C3': 'Forge Gate',\n",
    "            'K3C4': 'Steel Harbor', 'K3C5': 'Copper Hill',\n",
    "            'K4C1': 'Moonvale', 'K4C2': 'Starhollow', 'K4C3': 'Shadowpine',\n",
    "            'K4C4': 'Whisperwind', 'K4C5': 'Glimmergrove'\n",
    "        }\n",
    "        \n",
    "        # Syndicate shell companies (based in K1C5)\n",
    "        self.shell_companies = [\n",
    "            \"Golden Griffin Trading Co.\", \"Dragonscale Import House\", \"Mystic Coin Exchange\",\n",
    "            \"Raven's Rest Consulting\", \"Crystal Crown Merchants\", \"Shadowport Trading Ltd\",\n",
    "            \"Royal Tide Commerce\", \"Emerald Anchor Trading\", \"Goldweave Ventures\"\n",
    "        ]\n",
    "        \n",
    "        # Kingdom banking institutions\n",
    "        self.banks = [\n",
    "            \"Royal Bank of Valdris\", \"Crown Treasury\", \"Merchant Guild Financial\",\n",
    "            \"Kingdom Commercial Bank\", \"Golden Vault Banking\", \"Royal Trade Bank\"\n",
    "        ]\n",
    "        \n",
    "        # Offshore jurisdictions (suspicious locations)\n",
    "        self.offshore_locations = [\n",
    "            \"Free Port of Shadowmere\", \"Neutral Isles\", \"Hidden Archipelago\",\n",
    "            \"Merchant Republic Haven\", \"Blackwater Territories\", \"Rogue Trader Isles\"\n",
    "        ]\n",
    "        \n",
    "        # Royal investigators\n",
    "        self.investigators = [\n",
    "            (\"Aldric\", \"Stormwind\", \"Royal Financial Guard\"),\n",
    "            (\"Lyra\", \"Goldbrook\", \"Crown Investigators\"),\n",
    "            (\"Thane\", \"Ironforge\", \"Royal Treasury Guard\"),\n",
    "            (\"Elena\", \"Silverleaf\", \"Kingdom Revenue Service\"),\n",
    "            (\"Marcus\", \"Blackwater\", \"Royal Financial Guard\")\n",
    "        ]\n",
    "\n",
    "    def _setup_text_generators(self):\n",
    "        \"\"\"Initialize Markovify models with fantasy financial investigation text\"\"\"\n",
    "        \n",
    "        investigation_corpus = \"\"\"\n",
    "        The investigation revealed a complex coin laundering scheme involving multiple shell trading companies and offshore accounts. \n",
    "        Suspicious activity reports indicated large gold deposits inconsistent with merchant operations. \n",
    "        Bank records showed structured transactions designed to avoid royal reporting requirements.\n",
    "        Coin transfers to high-risk territories raised concerns for potential laundering activity.\n",
    "        The trading house showed minimal legitimate customer activity despite reporting substantial revenues.\n",
    "        Financial analysis revealed layering techniques using multiple accounts to obscure the source of funds.\n",
    "        Currency transaction reports showed patterns consistent with structuring violations under kingdom law.\n",
    "        The investigation uncovered a network of shell companies used to facilitate illicit financial transactions.\n",
    "        Surveillance indicated unusual business activity with limited customer traffic and suspicious gold handling.\n",
    "        Bank clerks reported customer reluctance to provide identification and business documentation.\n",
    "        The financial institution filed suspicious activity reports based on unusual transaction patterns.\n",
    "        Analysis of business records revealed discrepancies between reported income and actual trading activity.\n",
    "        The subject utilized multiple banking relationships to distribute transactions across different institutions.\n",
    "        Investigators identified connections between the subject and known coin laundering organizations.\n",
    "        \"\"\"\n",
    "        \n",
    "        whistleblower_corpus = \"\"\"\n",
    "        I am writing to report suspicious financial activities that I observed at the trading company.\n",
    "        During my employment, I witnessed large gold transactions that seemed unusual for our operations.\n",
    "        The management instructed workers to structure deposits to avoid royal reporting requirements.\n",
    "        I observed customers making multiple transactions just under the reporting threshold.\n",
    "        The business received coin transfers from territories known for banking secrecy.\n",
    "        Management was reluctant to maintain proper documentation for large gold transactions.\n",
    "        I noticed discrepancies between the gold reported and the actual business activity levels.\n",
    "        Workers were told to avoid asking questions about the source of large deposits.\n",
    "        The company maintained relationships with multiple banks to spread transaction activity.\n",
    "        I witnessed meetings with individuals who did not appear to be legitimate merchants.\n",
    "        Gold was frequently transported in bags and stored in areas not typical for normal trading.\n",
    "        The business reported income that seemed inconsistent with the observable customer activity.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            self.investigation_model = markovify.Text(investigation_corpus)\n",
    "            self.whistleblower_model = markovify.Text(whistleblower_corpus)\n",
    "        except:\n",
    "            self.investigation_model = None\n",
    "            self.whistleblower_model = None\n",
    "\n",
    "    def _generate_text(self, model, fallback: str, sentences: int = 3) -> str:\n",
    "        \"\"\"Generate text using Markovify or fallback\"\"\"\n",
    "        if model:\n",
    "            try:\n",
    "                result = []\n",
    "                for _ in range(sentences):\n",
    "                    sentence = model.make_sentence(tries=100)\n",
    "                    if sentence:\n",
    "                        result.append(sentence)\n",
    "                return \" \".join(result) if result else fallback\n",
    "            except:\n",
    "                pass\n",
    "        return fallback\n",
    "\n",
    "    def load_mission1_data(self, filepath: str = None) -> pd.DataFrame:\n",
    "        \"\"\"Load Mission 1 data or generate synthetic transaction data\"\"\"\n",
    "        \n",
    "        if filepath and os.path.exists(filepath):\n",
    "            try:\n",
    "                return pd.read_csv(filepath)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        print(\"📊 Generating synthetic transaction data aligned with Mission 1\")\n",
    "        \n",
    "        # Generate transactions for syndicate operations in K1C5\n",
    "        dates = pd.date_range(start='2024-01-01', end='2024-12-31', freq='D')\n",
    "        transactions = []\n",
    "        \n",
    "        syndicate_company = \"Golden Griffin Trading Co.\"  # Main shell company\n",
    "        account_numbers = [f\"****{random.randint(1000, 9999)}\" for _ in range(6)]\n",
    "        \n",
    "        for date in dates:\n",
    "            if random.random() < 0.35:  # Transaction probability\n",
    "                # Structured deposits (just under 10,000 gold pieces)\n",
    "                if random.random() < 0.4:\n",
    "                    amount = random.randint(9000, 9900)\n",
    "                    trans_type = \"Gold Deposit\"\n",
    "                # Large suspicious transfers\n",
    "                elif random.random() < 0.3:\n",
    "                    amount = random.randint(15000, 75000)\n",
    "                    trans_type = random.choice([\"Coin Transfer\", \"Trade Draft\", \"Merchant Exchange\"])\n",
    "                # Normal business\n",
    "                else:\n",
    "                    amount = random.randint(500, 5000)\n",
    "                    trans_type = random.choice([\"Trade Payment\", \"Merchant Draft\", \"Guild Transfer\"])\n",
    "                \n",
    "                transactions.append({\n",
    "                    'date': date,\n",
    "                    'location': 'K1C5',  # Syndicate base\n",
    "                    'account': random.choice(account_numbers),\n",
    "                    'amount': amount,\n",
    "                    'type': trans_type,\n",
    "                    'description': f\"{syndicate_company} - Trade Operations\",\n",
    "                    'counterparty': syndicate_company if random.random() < 0.6 else f\"Unknown Merchant {random.randint(1, 15)}\"\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(transactions)\n",
    "\n",
    "    def generate_whistleblower_report(self, case_data: Dict[str, Any]) -> str:\n",
    "        \"\"\"Generate Royal Financial Guard whistleblower report\"\"\"\n",
    "        \n",
    "        filename = f\"whistleblower_report_RG{random.randint(100000, 999999)}.pdf\"\n",
    "        filepath = os.path.join(self.output_dir, filename)\n",
    "        \n",
    "        doc = SimpleDocTemplate(filepath, pagesize=letter, topMargin=1*inch, bottomMargin=1*inch)\n",
    "        styles = getSampleStyleSheet()\n",
    "        story = []\n",
    "        \n",
    "        # Royal document styling\n",
    "        title_style = ParagraphStyle(\n",
    "            'RoyalTitle', parent=styles['Title'], fontSize=16, textColor=colors.darkblue,\n",
    "            spaceAfter=30, alignment=TA_CENTER, fontName='Helvetica-Bold'\n",
    "        )\n",
    "        \n",
    "        header_style = ParagraphStyle(\n",
    "            'RoyalHeader', parent=styles['Heading2'], fontSize=12, textColor=colors.black,\n",
    "            spaceBefore=15, spaceAfter=10, fontName='Helvetica-Bold'\n",
    "        )\n",
    "        \n",
    "        # Document header\n",
    "        story.append(Paragraph(\"CONFIDENTIAL - ROYAL SEAL\", \n",
    "                              ParagraphStyle('Seal', fontSize=8, alignment=TA_RIGHT, textColor=colors.red)))\n",
    "        story.append(Spacer(1, 20))\n",
    "        \n",
    "        story.append(Paragraph(\"ROYAL FINANCIAL GUARD\", title_style))\n",
    "        story.append(Paragraph(\"WHISTLEBLOWER INCIDENT REPORT\", title_style))\n",
    "        story.append(Spacer(1, 20))\n",
    "        \n",
    "        # Report metadata\n",
    "        report_id = f\"RG-{random.randint(100000, 999999)}\"\n",
    "        report_date = datetime.date.today() - timedelta(days=random.randint(1, 30))\n",
    "        \n",
    "        metadata = [\n",
    "            [\"Report ID:\", report_id],\n",
    "            [\"Date Filed:\", report_date.strftime(\"%B %d, %Y\")],\n",
    "            [\"Classification:\", \"ROYAL CONFIDENTIAL\"],\n",
    "            [\"Priority:\", random.choice([\"HIGH\", \"CRITICAL\"])],\n",
    "            [\"Reporting Method:\", \"Anonymous Royal Hotline\"],\n",
    "            [\"Assigned Inspector:\", f\"Inspector {random.choice(self.investigators)[1]}\"]\n",
    "        ]\n",
    "        \n",
    "        metadata_table = Table(metadata, colWidths=[1.5*inch, 3*inch])\n",
    "        metadata_table.setStyle(TableStyle([\n",
    "            ('FONTNAME', (0, 0), (-1, -1), 'Helvetica'),\n",
    "            ('FONTSIZE', (0, 0), (-1, -1), 10),\n",
    "            ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),\n",
    "            ('GRID', (0, 0), (-1, -1), 1, colors.black),\n",
    "            ('BACKGROUND', (0, 0), (0, -1), colors.lightgrey)\n",
    "        ]))\n",
    "        \n",
    "        story.append(metadata_table)\n",
    "        story.append(Spacer(1, 30))\n",
    "        \n",
    "        # Subject information\n",
    "        story.append(Paragraph(\"I. SUBJECT INVESTIGATION\", header_style))\n",
    "        \n",
    "        target_company = case_data.get('target_company', random.choice(self.shell_companies))\n",
    "        subject_info = [\n",
    "            [\"Trading House:\", target_company],\n",
    "            [\"Business Type:\", \"Import/Export Trading\"],\n",
    "            [\"Location:\", f\"{self.cities['K1C5']}, {self.kingdoms['K1']} Kingdom\"],\n",
    "            [\"Primary Contact:\", f\"{random.choice(['Lord', 'Master', 'Merchant'])} {random.choice(['Aldwin', 'Gareth', 'Thorne'])} {random.choice(['Goldhand', 'Coinsworth', 'Tradewing'])}\"],\n",
    "            [\"Guild Registration:\", f\"TG-{random.randint(10000, 99999)}\"]\n",
    "        ]\n",
    "        \n",
    "        subject_table = Table(subject_info, colWidths=[1.5*inch, 4*inch])\n",
    "        subject_table.setStyle(TableStyle([\n",
    "            ('FONTNAME', (0, 0), (-1, -1), 'Helvetica'),\n",
    "            ('FONTSIZE', (0, 0), (-1, -1), 9),\n",
    "            ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),\n",
    "            ('GRID', (0, 0), (-1, -1), 0.5, colors.grey)\n",
    "        ]))\n",
    "        \n",
    "        story.append(subject_table)\n",
    "        story.append(Spacer(1, 20))\n",
    "        \n",
    "        # Allegations\n",
    "        story.append(Paragraph(\"II. REPORTED ALLEGATIONS\", header_style))\n",
    "        \n",
    "        allegation_text = self._generate_text(\n",
    "            self.whistleblower_model,\n",
    "            f\"I am reporting suspicious coin laundering activities at {target_company} in {self.cities['K1C5']}. During my employment, I observed large gold transactions and structured deposits designed to avoid Royal Treasury reporting requirements. The trading house showed minimal legitimate merchant activity despite reporting substantial revenues.\",\n",
    "            sentences=4\n",
    "        )\n",
    "        \n",
    "        story.append(Paragraph(allegation_text, styles['Normal']))\n",
    "        story.append(Spacer(1, 15))\n",
    "        \n",
    "        # Specific observations\n",
    "        observations = [\n",
    "            f\"Gold deposits totaling approximately {random.randint(500, 2000):,} thousand pieces over six months\",\n",
    "            f\"Structured transactions just under 10,000 gold threshold on {random.randint(15, 25)} occasions\",\n",
    "            f\"Coin transfers to shell companies in {random.choice(self.offshore_locations)}\",\n",
    "            f\"Trading operations inconsistent with reported merchant activities\",\n",
    "            \"Customer reluctance to provide identification for large transactions\",\n",
    "            f\"Use of multiple accounts at {random.randint(3, 6)} different banking houses\"\n",
    "        ]\n",
    "        \n",
    "        for i, obs in enumerate(random.sample(observations, random.randint(4, 6)), 1):\n",
    "            story.append(Paragraph(f\"{i}. {obs}\", styles['Normal']))\n",
    "            story.append(Spacer(1, 8))\n",
    "        \n",
    "        # Evidence section\n",
    "        story.append(Spacer(1, 15))\n",
    "        story.append(Paragraph(\"III. AVAILABLE EVIDENCE\", header_style))\n",
    "        \n",
    "        evidence = [\n",
    "            \"Banking house transaction ledgers\",\n",
    "            \"Internal trading house correspondence\",\n",
    "            \"Surveillance records of unusual activities\",\n",
    "            \"Coin transfer documentation\",\n",
    "            \"Guild registration and licensing records\"\n",
    "        ]\n",
    "        \n",
    "        for item in random.sample(evidence, random.randint(3, 5)):\n",
    "            story.append(Paragraph(f\"• {item}\", styles['Normal']))\n",
    "            story.append(Spacer(1, 6))\n",
    "        \n",
    "        # Footer\n",
    "        story.append(Spacer(1, 30))\n",
    "        story.append(Paragraph(\"Submitted under Royal Decree 5328 - Financial Crimes Reporting Act\", \n",
    "                              ParagraphStyle('Footer', fontSize=8, alignment=TA_CENTER, fontStyle='italic')))\n",
    "        \n",
    "        doc.build(story)\n",
    "        print(f\"✨ Conjured whistleblower report: {filepath}\")\n",
    "        return filepath\n",
    "\n",
    "    def generate_bank_statement(self, transaction_data: pd.DataFrame) -> str:\n",
    "        \"\"\"Generate banking house statement showing syndicate transactions\"\"\"\n",
    "        \n",
    "        filename = f\"bank_statement_{random.randint(100000, 999999)}.pdf\"\n",
    "        filepath = os.path.join(self.output_dir, filename)\n",
    "        \n",
    "        doc = SimpleDocTemplate(filepath, pagesize=letter, topMargin=0.5*inch)\n",
    "        styles = getSampleStyleSheet()\n",
    "        story = []\n",
    "        \n",
    "        # Banking house details\n",
    "        bank_name = random.choice(self.banks)\n",
    "        account_holder = \"Golden Griffin Trading Co.\"  # Main syndicate front\n",
    "        account_number = f\"****-****-{random.randint(1000, 9999)}\"\n",
    "        statement_date = datetime.date.today().replace(day=1) - timedelta(days=1)\n",
    "        \n",
    "        # Bank header\n",
    "        header_style = ParagraphStyle(\n",
    "            'BankHeader', fontSize=18, textColor=colors.darkblue,\n",
    "            fontName='Helvetica-Bold', alignment=TA_CENTER, spaceAfter=20\n",
    "        )\n",
    "        \n",
    "        story.append(Paragraph(bank_name.upper(), header_style))\n",
    "        story.append(Paragraph(f\"{self.cities['K1C5']}, {self.kingdoms['K1']} Kingdom\", \n",
    "                              ParagraphStyle('Location', fontSize=10, alignment=TA_CENTER, spaceAfter=30)))\n",
    "        \n",
    "        # Account information\n",
    "        account_info = [\n",
    "            [\"Account Holder:\", account_holder],\n",
    "            [\"Account Number:\", account_number],\n",
    "            [\"Statement Period:\", f\"{statement_date.replace(day=1).strftime('%m/%d/%Y')} - {statement_date.strftime('%m/%d/%Y')}\"],\n",
    "            [\"Account Type:\", \"Merchant Trading Account\"],\n",
    "            [\"Location:\", f\"{self.cities['K1C5']} Branch\"]\n",
    "        ]\n",
    "        \n",
    "        info_table = Table(account_info, colWidths=[1.5*inch, 4*inch])\n",
    "        info_table.setStyle(TableStyle([\n",
    "            ('FONTNAME', (0, 0), (-1, -1), 'Helvetica'),\n",
    "            ('FONTSIZE', (0, 0), (-1, -1), 9),\n",
    "            ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),\n",
    "            ('BACKGROUND', (0, 0), (-1, -1), colors.lightgrey),\n",
    "            ('GRID', (0, 0), (-1, -1), 1, colors.black)\n",
    "        ]))\n",
    "        \n",
    "        story.append(info_table)\n",
    "        story.append(Spacer(1, 20))\n",
    "        \n",
    "        # Account summary\n",
    "        story.append(Paragraph(\"ACCOUNT SUMMARY\", \n",
    "                              ParagraphStyle('SectionTitle', fontSize=12, fontName='Helvetica-Bold', spaceAfter=10)))\n",
    "        \n",
    "        # Calculate from transaction data or generate\n",
    "        monthly_data = transaction_data[\n",
    "            pd.to_datetime(transaction_data['date']).dt.month == statement_date.month\n",
    "        ] if len(transaction_data) > 0 else pd.DataFrame()\n",
    "        \n",
    "        deposits = monthly_data[monthly_data['amount'] > 0]['amount'].sum() if len(monthly_data) > 0 else random.randint(500000, 1500000)\n",
    "        withdrawals = abs(monthly_data[monthly_data['amount'] < 0]['amount'].sum()) if len(monthly_data) > 0 else random.randint(300000, 800000)\n",
    "        beginning_balance = random.randint(50000, 200000)\n",
    "        ending_balance = beginning_balance + deposits - withdrawals\n",
    "        \n",
    "        summary_data = [\n",
    "            [\"Beginning Balance:\", f\"{beginning_balance:,} GP\"],\n",
    "            [\"Total Deposits:\", f\"{deposits:,} GP\"],\n",
    "            [\"Total Withdrawals:\", f\"{withdrawals:,} GP\"],\n",
    "            [\"Banking Fees:\", f\"{random.randint(25, 150)} GP\"],\n",
    "            [\"Ending Balance:\", f\"{ending_balance:,} GP\"]\n",
    "        ]\n",
    "        \n",
    "        summary_table = Table(summary_data, colWidths=[2*inch, 1.5*inch])\n",
    "        summary_table.setStyle(TableStyle([\n",
    "            ('FONTNAME', (0, 0), (-1, -1), 'Helvetica'),\n",
    "            ('FONTSIZE', (0, 0), (-1, -1), 10),\n",
    "            ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),\n",
    "            ('ALIGN', (1, 0), (1, -1), 'RIGHT'),\n",
    "            ('LINEBELOW', (0, -1), (-1, -1), 2, colors.black),\n",
    "            ('FONTNAME', (0, -1), (-1, -1), 'Helvetica-Bold')\n",
    "        ]))\n",
    "        \n",
    "        story.append(summary_table)\n",
    "        story.append(Spacer(1, 30))\n",
    "        \n",
    "        # Transaction details\n",
    "        story.append(Paragraph(\"TRANSACTION LEDGER\", \n",
    "                              ParagraphStyle('SectionTitle', fontSize=12, fontName='Helvetica-Bold', spaceAfter=15)))\n",
    "        \n",
    "        # Generate suspicious transaction patterns\n",
    "        transaction_details = [[\"Date\", \"Description\", \"Withdrawal\", \"Deposit\", \"Balance\"]]\n",
    "        \n",
    "        current_balance = beginning_balance\n",
    "        suspicious_transactions = []\n",
    "        \n",
    "        # Pattern 1: Structured gold deposits under 10,000 GP\n",
    "        for _ in range(random.randint(8, 15)):\n",
    "            amount = random.randint(9000, 9950)\n",
    "            date_obj = statement_date.replace(day=random.randint(1, 28))\n",
    "            suspicious_transactions.append({\n",
    "                'date': date_obj,\n",
    "                'description': f\"GOLD DEPOSIT - {account_holder[:20]}\",\n",
    "                'amount': amount,\n",
    "                'type': 'deposit'\n",
    "            })\n",
    "        \n",
    "        # Pattern 2: Large coin transfers from offshore\n",
    "        for _ in range(random.randint(3, 8)):\n",
    "            amount = random.randint(25000, 150000)\n",
    "            date_obj = statement_date.replace(day=random.randint(1, 28))\n",
    "            origin = random.choice(self.offshore_locations)\n",
    "            suspicious_transactions.append({\n",
    "                'date': date_obj,\n",
    "                'description': f\"COIN TRANSFER FROM {origin[:15]}\",\n",
    "                'amount': amount,\n",
    "                'type': 'deposit'\n",
    "            })\n",
    "        \n",
    "        # Pattern 3: Round number withdrawals\n",
    "        for _ in range(random.randint(5, 12)):\n",
    "            amount = random.choice([25000, 50000, 75000, 100000])\n",
    "            date_obj = statement_date.replace(day=random.randint(1, 28))\n",
    "            suspicious_transactions.append({\n",
    "                'date': date_obj,\n",
    "                'description': f\"TRADE DRAFT - BUSINESS EXPENSE\",\n",
    "                'amount': amount,\n",
    "                'type': 'withdrawal'\n",
    "            })\n",
    "        \n",
    "        # Sort by date\n",
    "        suspicious_transactions.sort(key=lambda x: x['date'])\n",
    "        \n",
    "        # Add to table\n",
    "        for trans in suspicious_transactions:\n",
    "            if trans['type'] == 'deposit':\n",
    "                current_balance += trans['amount']\n",
    "                transaction_details.append([\n",
    "                    trans['date'].strftime(\"%m/%d\"),\n",
    "                    trans['description'],\n",
    "                    \"\",\n",
    "                    f\"{trans['amount']:,} GP\",\n",
    "                    f\"{current_balance:,} GP\"\n",
    "                ])\n",
    "            else:\n",
    "                current_balance -= trans['amount']\n",
    "                transaction_details.append([\n",
    "                    trans['date'].strftime(\"%m/%d\"),\n",
    "                    trans['description'],\n",
    "                    f\"{trans['amount']:,} GP\",\n",
    "                    \"\",\n",
    "                    f\"{current_balance:,} GP\"\n",
    "                ])\n",
    "        \n",
    "        # Create table\n",
    "        trans_table = Table(transaction_details, colWidths=[0.8*inch, 3.2*inch, 1*inch, 1*inch, 1*inch])\n",
    "        trans_table.setStyle(TableStyle([\n",
    "            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
    "            ('FONTNAME', (0, 1), (-1, -1), 'Helvetica'),\n",
    "            ('FONTSIZE', (0, 0), (-1, -1), 8),\n",
    "            ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n",
    "            ('ALIGN', (2, 0), (-1, -1), 'RIGHT'),\n",
    "            ('GRID', (0, 0), (-1, -1), 0.5, colors.black),\n",
    "            ('ROWBACKGROUNDS', (0, 1), (-1, -1), [colors.white, colors.beige])\n",
    "        ]))\n",
    "        \n",
    "        story.append(trans_table)\n",
    "        \n",
    "        # Royal notice\n",
    "        story.append(Spacer(1, 30))\n",
    "        notice_text = f\"\"\"\n",
    "        ROYAL NOTICE: This account has been flagged for enhanced monitoring under Royal Treasury Decree. \n",
    "        Large transactions may be reported to the Royal Financial Guard. Contact our compliance officer \n",
    "        at the {self.cities['K1C5']} branch for inquiries.\n",
    "        \"\"\"\n",
    "        \n",
    "        story.append(Paragraph(notice_text, \n",
    "                              ParagraphStyle('Notice', fontSize=8, fontStyle='italic', \n",
    "                                           textColor=colors.red, leftIndent=20, rightIndent=20)))\n",
    "        \n",
    "        doc.build(story)\n",
    "        print(f\"✨ Conjured banking house statement: {filepath}\")\n",
    "        return filepath\n",
    "\n",
    "    def generate_suspicious_activity_report(self, case_data: Dict[str, Any]) -> str:\n",
    "        \"\"\"Generate Royal Treasury Suspicious Activity Report\"\"\"\n",
    "        \n",
    "        filename = f\"SAR_{random.randint(100000, 999999)}.pdf\"\n",
    "        filepath = os.path.join(self.output_dir, filename)\n",
    "        \n",
    "        doc = SimpleDocTemplate(filepath, pagesize=letter, topMargin=0.75*inch)\n",
    "        styles = getSampleStyleSheet()\n",
    "        story = []\n",
    "        \n",
    "        # Royal SAR header\n",
    "        header_style = ParagraphStyle(\n",
    "            'SARHeader', fontSize=14, fontName='Helvetica-Bold', alignment=TA_CENTER,\n",
    "            textColor=colors.darkred, spaceAfter=20\n",
    "        )\n",
    "        \n",
    "        story.append(Paragraph(\"SUSPICIOUS ACTIVITY REPORT\", header_style))\n",
    "        story.append(Paragraph(\"Royal Treasury Form RT-111\", \n",
    "                              ParagraphStyle('FormNumber', fontSize=10, alignment=TA_CENTER, spaceAfter=30)))\n",
    "        \n",
    "        # SAR identification\n",
    "        sar_number = f\"RT-{random.randint(1000000, 9999999)}\"\n",
    "        filing_date = datetime.date.today() - timedelta(days=random.randint(1, 15))\n",
    "        \n",
    "        identification_data = [\n",
    "            [\"SAR Number:\", sar_number],\n",
    "            [\"Filing Date:\", filing_date.strftime(\"%m/%d/%Y\")],\n",
    "            [\"Filing Institution:\", random.choice(self.banks)],\n",
    "            [\"Branch Location:\", f\"{self.cities['K1C5']}, {self.kingdoms['K1']} Kingdom\"],\n",
    "            [\"Guild Registration:\", f\"BG-{random.randint(1000000, 9999999)}\"]\n",
    "        ]\n",
    "        \n",
    "        id_table = Table(identification_data, colWidths=[1.5*inch, 3*inch])\n",
    "        id_table.setStyle(TableStyle([\n",
    "            ('FONTNAME', (0, 0), (-1, -1), 'Helvetica'),\n",
    "            ('FONTSIZE', (0, 0), (-1, -1), 9),\n",
    "            ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),\n",
    "            ('GRID', (0, 0), (-1, -1), 1, colors.black),\n",
    "            ('BACKGROUND', (0, 0), (0, -1), colors.lightgrey)\n",
    "        ]))\n",
    "        \n",
    "        story.append(id_table)\n",
    "        story.append(Spacer(1, 20))\n",
    "        \n",
    "        # Subject Information\n",
    "        story.append(Paragraph(\"PART I - SUBJECT INVESTIGATION\", \n",
    "                              ParagraphStyle('PartHeader', fontSize=11, fontName='Helvetica-Bold', spaceAfter=10)))\n",
    "        \n",
    "        subject_company = case_data.get('target_company', \"Golden Griffin Trading Co.\")\n",
    "        subject_data = [\n",
    "            [\"Subject Name:\", subject_company],\n",
    "            [\"Business Address:\", f\"Harbor District, {self.cities['K1C5']}\"],\n",
    "            [\"Kingdom Location:\", f\"{self.kingdoms['K1']} Kingdom\"],\n",
    "            [\"Guild Registration:\", f\"TG-{random.randint(1000000, 9999999)}\"],\n",
    "            [\"Account Number:\", f\"****-{random.randint(1000, 9999)}\"],\n",
    "            [\"Date Established:\", f\"{random.randint(1, 12)}/{random.randint(1, 28)}/{random.randint(2015, 2020)}\"]\n",
    "        ]\n",
    "        \n",
    "        subject_table = Table(subject_data, colWidths=[1.8*inch, 3.5*inch])\n",
    "        subject_table.setStyle(TableStyle([\n",
    "            ('FONTNAME', (0, 0), (-1, -1), 'Helvetica'),\n",
    "            ('FONTSIZE', (0, 0), (-1, -1), 9),\n",
    "            ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),\n",
    "            ('GRID', (0, 0), (-1, -1), 0.5, colors.grey)\n",
    "        ]))\n",
    "        \n",
    "        story.append(subject_table)\n",
    "        story.append(Spacer(1, 20))\n",
    "        \n",
    "        # Suspicious Activity Information\n",
    "        story.append(Paragraph(\"PART II - SUSPICIOUS ACTIVITY DETAILS\", \n",
    "                              ParagraphStyle('PartHeader', fontSize=11, fontName='Helvetica-Bold', spaceAfter=10)))\n",
    "        \n",
    "        activity_start = filing_date - timedelta(days=random.randint(90, 180))\n",
    "        activity_end = filing_date - timedelta(days=random.randint(1, 30))\n",
    "        \n",
    "        activity_data = [\n",
    "            [\"Activity Period:\", f\"{activity_start.strftime('%m/%d/%Y')} - {activity_end.strftime('%m/%d/%Y')}\"],\n",
    "            [\"Total Amount:\", f\"{random.randint(750, 2500):,},000 Gold Pieces\"],\n",
    "            [\"Transaction Count:\", f\"{random.randint(25, 75)}\"],\n",
    "            [\"Activity Type:\", \"Structured Deposits/Coin Laundering\"],\n",
    "            [\"Primary Location:\", f\"{self.cities['K1C5']} Branch\"],\n",
    "            [\"Prior SAR Filed:\", random.choice([\"Yes\", \"No\"])]\n",
    "        ]\n",
    "        \n",
    "        activity_table = Table(activity_data, colWidths=[2*inch, 3*inch])\n",
    "        activity_table.setStyle(TableStyle([\n",
    "            ('FONTNAME', (0, 0), (-1, -1), 'Helvetica'),\n",
    "            ('FONTSIZE', (0, 0), (-1, -1), 9),\n",
    "            ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),\n",
    "            ('GRID', (0, 0), (-1, -1), 0.5, colors.grey)\n",
    "        ]))\n",
    "        \n",
    "        story.append(activity_table)\n",
    "        story.append(Spacer(1, 20))\n",
    "        \n",
    "        # Narrative\n",
    "        story.append(Paragraph(\"PART III - INVESTIGATION NARRATIVE\", \n",
    "                              ParagraphStyle('PartHeader', fontSize=11, fontName='Helvetica-Bold', spaceAfter=10)))\n",
    "        \n",
    "        narrative_text = self._generate_text(\n",
    "            self.investigation_model,\n",
    "            f\"\"\"The subject, {subject_company}, operating from {self.cities['K1C5']}, has engaged in suspicious financial activity consistent with coin laundering operations. \n",
    "            Analysis of account activity reveals structured gold deposits designed to avoid Royal Treasury reporting requirements. \n",
    "            The subject made {random.randint(15, 25)} gold deposits ranging from 9,000 to 9,900 pieces, totaling over {random.randint(500, 1200):,},000 gold pieces during the reporting period. \n",
    "            The trading operations appear inconsistent with the level of gold activity, and the subject has been uncooperative when questioned about fund sources.\"\"\",\n",
    "            sentences=6\n",
    "        )\n",
    "        \n",
    "        story.append(Paragraph(narrative_text, styles['Normal']))\n",
    "        story.append(Spacer(1, 15))\n",
    "        \n",
    "        # Red flag indicators\n",
    "        story.append(Paragraph(\"Suspicious Activity Indicators:\", \n",
    "                              ParagraphStyle('SubHeader', fontSize=10, fontName='Helvetica-Bold', spaceAfter=8)))\n",
    "        \n",
    "        red_flags = [\n",
    "            f\"Multiple gold deposits just under 10,000 piece reporting threshold\",\n",
    "            f\"Customer reluctance to provide source documentation\",\n",
    "            f\"Trading activity inconsistent with gold volume\",\n",
    "            f\"Use of multiple accounts to fragment transactions\",\n",
    "            f\"Connections to high-risk territories through coin transfers\",\n",
    "            f\"Unusual transportation and handling of gold currency\"\n",
    "        ]\n",
    "        \n",
    "        for flag in red_flags:\n",
    "            story.append(Paragraph(f\"• {flag}\", styles['Normal']))\n",
    "            story.append(Spacer(1, 4))\n",
    "        \n",
    "        # Law enforcement contact\n",
    "        story.append(Spacer(1, 20))\n",
    "        story.append(Paragraph(\"PART IV - ROYAL ENFORCEMENT CONTACT\", \n",
    "                              ParagraphStyle('PartHeader', fontSize=11, fontName='Helvetica-Bold', spaceAfter=10)))\n",
    "        \n",
    "        investigator = random.choice(self.investigators)\n",
    "        contact_data = [\n",
    "            [\"Agency Notified:\", investigator[2]],\n",
    "            [\"Contact Name:\", f\"Inspector {investigator[0]} {investigator[1]}\"],\n",
    "            [\"Royal Station:\", f\"{self.cities['K1C1']} Headquarters\"],\n",
    "            [\"Date Contacted:\", (filing_date + timedelta(days=1)).strftime(\"%m/%d/%Y\")],\n",
    "            [\"Badge Number:\", f\"RG-{random.randint(1000, 9999)}\"]\n",
    "        ]\n",
    "        \n",
    "        contact_table = Table(contact_data, colWidths=[1.5*inch, 2.5*inch])\n",
    "        contact_table.setStyle(TableStyle([\n",
    "            ('FONTNAME', (0, 0), (-1, -1), 'Helvetica'),\n",
    "            ('FONTSIZE', (0, 0), (-1, -1), 9),\n",
    "            ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),\n",
    "            ('GRID', (0, 0), (-1, -1), 0.5, colors.grey)\n",
    "        ]))\n",
    "        \n",
    "        story.append(contact_table)\n",
    "        story.append(Spacer(1, 30))\n",
    "        \n",
    "        # Certification\n",
    "        story.append(Paragraph(\"ROYAL CERTIFICATION\", \n",
    "                              ParagraphStyle('CertHeader', fontSize=11, fontName='Helvetica-Bold', \n",
    "                                           alignment=TA_CENTER, spaceAfter=15)))\n",
    "        \n",
    "        cert_text = f\"\"\"\n",
    "        I certify that the information contained in this Suspicious Activity Report is true and accurate \n",
    "        to the best of my knowledge. I understand that providing false information may result in penalties \n",
    "        under Royal Treasury Law.\n",
    "        \n",
    "        \n",
    "        ________________________________                    Date: {filing_date.strftime('%m/%d/%Y')}\n",
    "        Banking House Compliance Officer\n",
    "        \n",
    "        {random.choice(['Master Aldwin Goldkeeper', 'Lord Gareth Coinwatch', 'Dame Sarah Vaultguard'])}\n",
    "        Royal Treasury Compliance Officer\n",
    "        \"\"\"\n",
    "        \n",
    "        story.append(Paragraph(cert_text, styles['Normal']))\n",
    "        \n",
    "        # Footer\n",
    "        story.append(Spacer(1, 20))\n",
    "        story.append(Paragraph(\"Royal Treasury Form RT-111 - Suspicious Activity Report\", \n",
    "                              ParagraphStyle('Footer', fontSize=8, alignment=TA_CENTER, \n",
    "                                           textColor=colors.grey)))\n",
    "        \n",
    "        doc.build(story)\n",
    "        print(f\"✨ Conjured Suspicious Activity Report: {filepath}\")\n",
    "        return filepath\n",
    "\n",
    "    def generate_all_documents(self, mission1_data_path: str = None) -> List[str]:\n",
    "        \"\"\"Generate all three investigation documents for the syndicate in K1C5\"\"\"\n",
    "        \n",
    "        print(\"🔮 Beginning Mission 2: Summoning Investigation Documents...\")\n",
    "        print(f\"🏰 Target: The syndicate operating from {self.cities['K1C5']}, {self.kingdoms['K1']} Kingdom\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Load transaction data\n",
    "        transaction_data = self.load_mission1_data(mission1_data_path)\n",
    "        \n",
    "        # Create consistent case data\n",
    "        case_data = {\n",
    "            'target_company': 'Golden Griffin Trading Co.',  # Main syndicate front\n",
    "            'base_location': 'K1C5',\n",
    "            'investigation_period': {\n",
    "                'start': datetime.date.today() - timedelta(days=180),\n",
    "                'end': datetime.date.today() - timedelta(days=30)\n",
    "            },\n",
    "            'total_suspicious_amount': random.randint(1000000, 3000000),\n",
    "            'primary_investigator': random.choice(self.investigators)\n",
    "        }\n",
    "        \n",
    "        generated_files = []\n",
    "        \n",
    "        # Generate documents\n",
    "        try:\n",
    "            generated_files.append(self.generate_whistleblower_report(case_data))\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error generating whistleblower report: {e}\")\n",
    "        \n",
    "        try:\n",
    "            generated_files.append(self.generate_bank_statement(transaction_data))\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error generating bank statement: {e}\")\n",
    "        \n",
    "        try:\n",
    "            generated_files.append(self.generate_suspicious_activity_report(case_data))\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error generating SAR: {e}\")\n",
    "        \n",
    "        print(\"=\" * 70)\n",
    "        print(f\"✅ Mission 2 Complete! Generated {len(generated_files)} investigation documents:\")\n",
    "        \n",
    "        document_types = [\"WHISTLEBLOWER REPORT\", \"BANK STATEMENT\", \"SUSPICIOUS ACTIVITY REPORT\"]\n",
    "        for i, file in enumerate(generated_files):\n",
    "            if i < len(document_types):\n",
    "                print(f\"   {i+1}. {document_types[i]}: {os.path.basename(file)}\")\n",
    "        \n",
    "        print(f\"\\n📁 All documents saved to: {self.output_dir}/\")\n",
    "        print(f\"🎯 Investigation Focus: {case_data['target_company']} in {self.cities[case_data['base_location']]}\")\n",
    "        \n",
    "        # Generate mission summary\n",
    "        self._generate_mission_summary(generated_files, case_data)\n",
    "        \n",
    "        return generated_files\n",
    "    \n",
    "    def _generate_mission_summary(self, generated_files: List[str], case_data: Dict[str, Any]):\n",
    "        \"\"\"Generate mission summary for instructors\"\"\"\n",
    "        \n",
    "        summary_file = os.path.join(self.output_dir, \"Mission2_Investigation_Summary.txt\")\n",
    "        \n",
    "        with open(summary_file, 'w') as f:\n",
    "            f.write(\"MISSION 2: FANTASY INVESTIGATION DOCUMENTS\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "            \n",
    "            f.write(\"INVESTIGATION TARGET:\\n\")\n",
    "            f.write(f\"• Syndicate Base: {self.cities[case_data['base_location']]}, {self.kingdoms['K1']} Kingdom\\n\")\n",
    "            f.write(f\"• Primary Shell Company: {case_data['target_company']}\\n\")\n",
    "            f.write(f\"• Investigation Period: {case_data['investigation_period']['start']} to {case_data['investigation_period']['end']}\\n\")\n",
    "            f.write(f\"• Estimated Illicit Activity: {case_data['total_suspicious_amount']:,} Gold Pieces\\n\\n\")\n",
    "            \n",
    "            f.write(\"SYNTHETIC DATA TECHNIQUES:\\n\")\n",
    "            f.write(\"• Markovify: Domain-specific text generation from financial investigation corpora\\n\")\n",
    "            f.write(\"• ReportLab: Professional document formatting matching real investigation standards\\n\")\n",
    "            f.write(\"• Fantasy theming: All real-world references converted to magical kingdom setting\\n\")\n",
    "            f.write(\"• Data consistency: Documents reference the same syndicate operations\\n\")\n",
    "            f.write(\"• Mission 1 integration: Bank statement aligns with transaction data patterns\\n\\n\")\n",
    "            \n",
    "            f.write(\"GENERATED DOCUMENTS:\\n\")\n",
    "            for i, file in enumerate(generated_files, 1):\n",
    "                f.write(f\"{i}. {os.path.basename(file)}\\n\")\n",
    "            \n",
    "            f.write(f\"\\nMONEY LAUNDERING PATTERNS DEMONSTRATED:\\n\")\n",
    "            f.write(\"• Structured deposits just under 10,000 gold piece reporting threshold\\n\")\n",
    "            f.write(\"• Large transfers from high-risk offshore territories\\n\")\n",
    "            f.write(\"• Shell company operations inconsistent with legitimate trading\\n\")\n",
    "            f.write(\"• Multiple banking relationships to distribute suspicious activity\\n\")\n",
    "            f.write(\"• Round-number withdrawals suggesting cash conversion\\n\\n\")\n",
    "            \n",
    "            f.write(\"EDUCATIONAL OUTCOMES:\\n\")\n",
    "            f.write(\"• Recognition of financial crime red flags in fantasy context\\n\")\n",
    "            f.write(\"• Understanding of regulatory reporting requirements\\n\")\n",
    "            f.write(\"• Experience analyzing professional investigation documents\\n\")\n",
    "            f.write(\"• Appreciation for cross-document consistency in investigations\\n\")\n",
    "            f.write(\"• Practical application of synthetic data in investigative scenarios\\n\")\n",
    "        \n",
    "        print(f\"📊 Investigation summary generated: {summary_file}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Execute Mission 2: Generate fantasy investigation documents\"\"\"\n",
    "    \n",
    "    # Initialize the document forge\n",
    "    doc_forge = FantasyDocumentForge()\n",
    "    \n",
    "    # Generate all investigation documents\n",
    "    # To integrate with Mission 1 data: doc_forge.generate_all_documents(\"path/to/mission1_data.csv\")\n",
    "    generated_files = doc_forge.generate_all_documents()\n",
    "    \n",
    "    return generated_files\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "_______________________________________________________\n",
    "\n",
    "# Mission 2 Review\n",
    "\n",
    "Arcane Investigators, in **Mission 2: The Document Summoning**, you’ve performed a mystical ritual to conjure synthetic investigation documents (`mission2_documents/`) that expose the K1C5 syndicate’s money laundering in Goldweave Port. Using the enchanted ledger from Mission 1 (crafted with Gaussian Copula, CTGAN, Graph Network, and TVAE), your ritual summons whistleblower reports, bank statements, and suspicious activity reports (SARs). Your notebook outputs—PDFs revealing specific syndicate schemes!\n",
    "\n",
    "## The Summoning Ritual\n",
    "\n",
    "The `FantasyDocumentForge` conjures three document types to unmask K1C5:\n",
    "\n",
    "### 1. Arcane Text Conjuring\n",
    "- **Action**: Weaves narrative spells (`markovify>=0.9.4`) from financial crime corpora.\n",
    "- **Features**: Generates realistic text for whistleblower and SAR narratives, e.g., “structured deposits to avoid reporting.”\n",
    "- **Output**: Coherent allegations tied to K1C5’s operations.\n",
    "\n",
    "**Role**: Provides credible, syndicate-focused text for documents.\n",
    "\n",
    "### 2. Document Forging\n",
    "- **Action**: Crafts PDFs (`reportlab>=4.2.2`) with tables, headers, and fantasy styling.\n",
    "- **Features**: Includes syndicate details (e.g., Golden Griffin Trading Co.), K1C5 locations, and suspicious patterns (structured deposits <10,000 gold pieces, offshore transfers).\n",
    "- **Output**: Whistleblower reports, bank statements (aligned with Mission 1 data), and SARs.\n",
    "\n",
    "**Role**: Creates documents mirroring real investigations.\n",
    "\n",
    "### 3. Syndicate Evidence Binding\n",
    "- **Action**: Integrates Mission 1’s synthetic transactions, ensuring consistency.\n",
    "- **Features**: Embeds patterns like structured deposits (9,000–9,950 gold pieces), large offshore transfers (25,000–150,000), and round-number withdrawals.\n",
    "- **Output**: Documents reflecting K1C5’s tactics, saved in `mission2_documents/`.\n",
    "\n",
    "**Role**: Links documents to transaction data, exposing illicit networks.\n",
    "\n",
    "## Why It Matters\n",
    "Your summoned documents reveal sydicate schemes in K1C5, like structured deposits, offshore transfers, and shell company operations—rooted in Mission 1’s synthetic data. These PDFs, rich with red flags, enable Mission 4’s chatbot queries and empower the Valdris Council to combat crime. \n",
    "_______________________________________________________\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mission 3: The Prophecy Familiar\n",
    "**Objective**: Train an anomaly detection mythical beast on synthetic data patterns to identify suspicious financial activities, strengthening the investigation against the money laundering syndicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Total transactions generated: 10914 (approx 5.0% ML syndicate cases)\n",
      "  💰 Corrupt syndicate cases: 1414\n",
      "  🏰 Transactions involving K1C5: 1405\n",
      "\n",
      "🕸️ Weaving Customer-Merchant Networks...\n",
      "✓ Network created: 500 customers, 200 merchants\n",
      "  High-risk merchants: 20\n",
      "  Suspicious customers: 50\n",
      "\n",
      "⏰ Conjuring Time-Series Financial Flows...\n",
      "✓ Time-series flows generated over 30 days\n",
      "  Features: timestamp, running_total, velocity_1h, velocity_24h\n",
      "🏰 Fantasy Kingdom AML Detection System\n",
      "==================================================\n",
      "Investigating corrupt syndicate in K1C5...\n",
      "\n",
      "Dataset Overview:\n",
      "  Total transactions: 10914\n",
      "  Money laundering cases: 1414\n",
      "  K1C5 involvement: 1405\n",
      "🔨 Building transaction graphs...\n",
      "✓ Created 422 temporal graph snapshots\n",
      "Sample graph - Nodes: torch.Size([382, 13]), Edges: torch.Size([2, 299]), Edge attr: torch.Size([299, 11])\n",
      "\n",
      "🚀 Training GAE model...\n",
      "==================================================\n",
      "Epoch 1/50\n",
      "  Train Loss: 0.7460, AUC: 0.8390\n",
      "  Val Loss: 0.6645, AUC: 0.8443, PR-AUC: 0.3946\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.4808, AUC: 0.7993\n",
      "  Val Loss: 0.4682, AUC: 0.8084, PR-AUC: 0.3354\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.4426, AUC: 0.8012\n",
      "  Val Loss: 0.4300, AUC: 0.8082, PR-AUC: 0.3346\n",
      "Epoch 15/50\n",
      "  Train Loss: 0.4147, AUC: 0.7934\n",
      "  Val Loss: 0.4020, AUC: 0.7971, PR-AUC: 0.3309\n",
      "Epoch 20/50\n",
      "  Train Loss: 0.3978, AUC: 0.7863\n",
      "  Val Loss: 0.3593, AUC: 0.7957, PR-AUC: 0.3364\n",
      "Epoch 25/50\n",
      "  Train Loss: 0.2935, AUC: 0.7928\n",
      "  Val Loss: 0.3044, AUC: 0.7983, PR-AUC: 0.3437\n",
      "Epoch 30/50\n",
      "  Train Loss: 0.2602, AUC: 0.7858\n",
      "  Val Loss: 0.2754, AUC: 0.7973, PR-AUC: 0.3455\n",
      "Epoch 35/50\n",
      "  Train Loss: 0.2400, AUC: 0.7839\n",
      "  Val Loss: 0.2608, AUC: 0.7851, PR-AUC: 0.3280\n",
      "Epoch 40/50\n",
      "  Train Loss: 0.2228, AUC: 0.7823\n",
      "  Val Loss: 0.2433, AUC: 0.7883, PR-AUC: 0.3367\n",
      "Epoch 45/50\n",
      "  Train Loss: 0.2130, AUC: 0.7828\n",
      "  Val Loss: 0.2338, AUC: 0.8020, PR-AUC: 0.3402\n",
      "Epoch 50/50\n",
      "  Train Loss: 0.2011, AUC: 0.7856\n",
      "  Val Loss: 0.2275, AUC: 0.7957, PR-AUC: 0.3306\n",
      "\n",
      "✓ Training complete!\n",
      "\n",
      "🔍 Analyzing test transactions for suspicious patterns...\n",
      "\n",
      "📊 Detection Results:\n",
      "  Accounts analyzed: 686\n",
      "  High-risk accounts detected: 102\n",
      "  K1C5 syndicate connections: 15\n",
      "  Precision: 0.5294\n",
      "\n",
      "🚨 Top Suspicious Accounts:\n",
      "  1. MERCH_0017 - Anomaly Score: 100.00 - Pattern: suspicious_activity\n",
      "  2. MERCH_0188 - Anomaly Score: 89.75 - Pattern: layering\n",
      "  3. MERCH_0018 - Anomaly Score: 89.48 - Pattern: suspicious_activity\n",
      "  4. MERCH_0048 - Anomaly Score: 87.71 - Pattern: layering\n",
      "  5. MERCH_0020 - Anomaly Score: 83.86 - Pattern: suspicious_activity\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_score\n",
    "import networkx as nx\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class TransactionGraphBuilder:\n",
    "    \"\"\"\n",
    "    Builds graph representations for AML detection, enhanced for K1C5 syndicate patterns.\n",
    "    Incorporates clustering coefficients and time-series features from Mission 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, time_window_hours=24, min_edge_weight=0.01):\n",
    "        self.time_window_hours = time_window_hours\n",
    "        self.min_edge_weight = min_edge_weight\n",
    "        self.flagged_accounts = {'K1C5', 'K2C5', 'K3C1', 'K4C3'}\n",
    "        \n",
    "    def create_temporal_graph(self, df, window_start=0, window_size=300):\n",
    "        window_df = df.iloc[window_start:window_start + window_size].copy()\n",
    "        \n",
    "        sender_col, receiver_col = ('customer_id', 'merchant_id') if 'customer_id' in df.columns else ('sender_location', 'receiver_location')\n",
    "        accounts = set(window_df[sender_col].unique()) | set(window_df[receiver_col].unique())\n",
    "        \n",
    "        if len(accounts) < 5:\n",
    "            return None, None\n",
    "        account_to_idx = {acc: idx for idx, acc in enumerate(sorted(accounts))}\n",
    "        \n",
    "        G = nx.DiGraph()\n",
    "        for _, row in window_df.iterrows():\n",
    "            G.add_edge(row[sender_col], row[receiver_col], weight=row['amount'])\n",
    "        \n",
    "        clustering_coeffs = nx.clustering(G, weight=None)\n",
    "        \n",
    "        edge_index = []\n",
    "        edge_features = []\n",
    "        \n",
    "        grouped = window_df.groupby([sender_col, receiver_col])\n",
    "        \n",
    "        for (sender, receiver), group in grouped:\n",
    "            if sender in account_to_idx and receiver in account_to_idx:\n",
    "                edge_index.append([account_to_idx[sender], account_to_idx[receiver]])\n",
    "                edge_feat = [\n",
    "                    group['amount'].sum(),\n",
    "                    group['amount'].mean(),\n",
    "                    len(group),\n",
    "                    group['amount'].std() if len(group) > 1 else 0,\n",
    "                    group['transaction_hour'].std() if len(group) > 1 else 0,\n",
    "                    group['cross_border'].mean(),\n",
    "                    group['cash_equivalent'].mean(),\n",
    "                    group['merchant_risk_score'].mean(),\n",
    "                    group['transactions_last_24h'].mean(),\n",
    "                    3 if sender == 'K1C5' or receiver == 'K1C5' else 1 if sender in self.flagged_accounts or receiver in self.flagged_accounts else 0,  # Boost K1C5\n",
    "                    group.get('velocity_24h', pd.Series([0] * len(group))).mean(),\n",
    "                ]\n",
    "                edge_features.append(edge_feat)\n",
    "        \n",
    "        node_features = []\n",
    "        node_labels = []\n",
    "        \n",
    "        for account in sorted(accounts):\n",
    "            sent = window_df[window_df[sender_col] == account]\n",
    "            received = window_df[window_df[receiver_col] == account]\n",
    "            \n",
    "            total_txns = len(sent) + len(received)\n",
    "            ml_sent = len(sent[sent['is_money_laundering'] == 1]) if len(sent) > 0 else 0\n",
    "            ml_received = len(received[received['is_money_laundering'] == 1]) if len(received) > 0 else 0\n",
    "            ml_count = ml_sent + ml_received\n",
    "            flagged_interactions = len(set(sent[receiver_col]) & self.flagged_accounts) + \\\n",
    "                                  len(set(received[sender_col]) & self.flagged_accounts)\n",
    "            k1c5_interactions = len(set(sent[receiver_col]) & {'K1C5'}) + len(set(received[sender_col]) & {'K1C5'})\n",
    "            \n",
    "            node_feat = [\n",
    "                len(sent),\n",
    "                len(received),\n",
    "                sent['amount'].sum() if len(sent) > 0 else 0,\n",
    "                received['amount'].sum() if len(received) > 0 else 0,\n",
    "                (received['amount'].sum() - sent['amount'].sum()) if total_txns > 0 else 0,\n",
    "                sent['account_age_days'].mean() if len(sent) > 0 else 0,\n",
    "                3 if account == 'K1C5' else 1 if account in self.flagged_accounts else 0,  # Boost K1C5\n",
    "                ml_count * 2.0,\n",
    "                flagged_interactions * 3.0,  # Increased weight\n",
    "                k1c5_interactions * 7.0,  # New K1C5-specific feature\n",
    "                sent.get('velocity_24h', pd.Series([0] * len(sent))).mean() if len(sent) > 0 else 0,\n",
    "                received.get('velocity_24h', pd.Series([0] * len(received))).mean() if len(received) > 0 else 0,\n",
    "                clustering_coeffs.get(account, 0.0),\n",
    "            ]\n",
    "            node_features.append(node_feat)\n",
    "            node_labels.append(1 if ml_count > 0 or k1c5_interactions > 0 else 0)  # Include K1C5 interactions in labels\n",
    "        \n",
    "        if len(edge_index) < 5:\n",
    "            return None, None\n",
    "        \n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t()\n",
    "        edge_attr = torch.tensor(edge_features, dtype=torch.float32)\n",
    "        x = torch.tensor(node_features, dtype=torch.float32)\n",
    "        y = torch.tensor(node_labels, dtype=torch.float32)\n",
    "        \n",
    "        return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y), account_to_idx\n",
    "    \n",
    "    def create_dataset(self, df, window_size=300, stride=20):\n",
    "        graphs = []\n",
    "        for start in range(0, len(df) - window_size + 1, stride):\n",
    "            graph, _ = self.create_temporal_graph(df, start, window_size)\n",
    "            if graph is not None and graph.edge_index.shape[1] > 0:\n",
    "                graphs.append(graph)\n",
    "        return graphs\n",
    "\n",
    "class AMLGAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Autoencoder for anomalous subgraph detection, fine-tuned for K1C5 syndicates.\n",
    "    \"\"\"\n",
    "    def __init__(self, node_features, edge_features, hidden_dim=32, latent_dim=16):\n",
    "        super(AMLGAE, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.ModuleList([\n",
    "            GCNConv(node_features, hidden_dim),\n",
    "            GCNConv(hidden_dim, latent_dim),\n",
    "        ])\n",
    "        \n",
    "        self.decoder = nn.ModuleList([\n",
    "            GCNConv(latent_dim, hidden_dim),\n",
    "            GCNConv(hidden_dim, node_features),\n",
    "        ])\n",
    "        \n",
    "        self.edge_reconstructor = nn.Sequential(\n",
    "            nn.Linear(latent_dim * 2 + edge_features, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "        \n",
    "        self.supervised_head = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "    \n",
    "    def encode(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.encoder):\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x) if i < len(self.encoder) - 1 else x\n",
    "        return x\n",
    "    \n",
    "    def decode_nodes(self, z, edge_index):\n",
    "        for i, conv in enumerate(self.decoder):\n",
    "            z = conv(z, edge_index)\n",
    "            z = F.relu(z) if i < len(self.decoder) - 1 else z\n",
    "        return z\n",
    "    \n",
    "    def decode_edges(self, z, edge_index, edge_attr):\n",
    "        row, col = edge_index\n",
    "        edge_inputs = torch.cat([z[row], z[col], edge_attr], dim=1)\n",
    "        return self.edge_reconstructor(edge_inputs)\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        z = self.encode(x, edge_index)\n",
    "        node_recon = self.decode_nodes(z, edge_index)\n",
    "        edge_recon = self.decode_edges(z, edge_index, edge_attr) if edge_attr is not None else None\n",
    "        supervised_score = self.supervised_head(z)\n",
    "        return z, node_recon, edge_recon, supervised_score\n",
    "\n",
    "class AMLDetectionSystem:\n",
    "    \"\"\"\n",
    "    AML system using GAE with semi-supervised fine-tuning for K1C5 syndicate detection.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=32, latent_dim=16, learning_rate=0.001):\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = None\n",
    "        self.graph_builder = TransactionGraphBuilder()\n",
    "        self.scaler_node = None\n",
    "        self.scaler_edge = None\n",
    "        self.threshold = None\n",
    "        self.val_recon_errors = None\n",
    "        self.val_error_mean = None\n",
    "        self.val_error_std = None\n",
    "    \n",
    "    def train(self, df, epochs=50, batch_size=16, val_split=0.2):\n",
    "        print(\"🔨 Building transaction graphs...\")\n",
    "        graphs = self.graph_builder.create_dataset(df, window_size=300, stride=20)\n",
    "        graphs = [g for g in graphs if g is not None and g.edge_index.shape[1] > 0]\n",
    "        print(f\"✓ Created {len(graphs)} temporal graph snapshots\")\n",
    "        if graphs:\n",
    "            print(f\"Sample graph - Nodes: {graphs[0].x.shape}, Edges: {graphs[0].edge_index.shape}, Edge attr: {graphs[0].edge_attr.shape}\")\n",
    "        \n",
    "        train_size = int(len(graphs) * (1 - val_split))\n",
    "        train_graphs = graphs[:train_size]\n",
    "        val_graphs = graphs[train_size:]\n",
    "        \n",
    "        all_x_train = np.vstack([g.x.numpy() for g in train_graphs]) if train_graphs else np.empty((0, graphs[0].x.shape[1]))\n",
    "        self.scaler_node = StandardScaler().fit(all_x_train) if len(all_x_train) > 0 else None\n",
    "        all_edge_train = np.vstack([g.edge_attr.numpy() for g in train_graphs if g.edge_attr.shape[0] > 0]) if any(g.edge_attr.shape[0] > 0 for g in train_graphs) else np.empty((0, 11))\n",
    "        self.scaler_edge = StandardScaler().fit(all_edge_train) if len(all_edge_train) > 0 else None\n",
    "        \n",
    "        for graphs_list in [train_graphs, val_graphs]:\n",
    "            for g in graphs_list:\n",
    "                if self.scaler_node is not None:\n",
    "                    g.x = torch.tensor(self.scaler_node.transform(g.x.numpy()), dtype=torch.float32)\n",
    "                if g.edge_attr.shape[0] > 0 and self.scaler_edge is not None:\n",
    "                    g.edge_attr = torch.tensor(self.scaler_edge.transform(g.edge_attr.numpy()), dtype=torch.float32)\n",
    "        \n",
    "        train_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_graphs, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        node_features = graphs[0].x.shape[1] if graphs else 0\n",
    "        edge_features = graphs[0].edge_attr.shape[1] if graphs and graphs[0].edge_attr.shape[0] > 0 else 11\n",
    "        \n",
    "        self.model = AMLGAE(node_features, edge_features, self.hidden_dim, self.latent_dim).to(self.device)\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=5e-4)\n",
    "        \n",
    "        # Compute validation threshold and statistics for z-score scaling\n",
    "        self.model.eval()\n",
    "        self.val_recon_errors = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(self.device)\n",
    "                _, node_recon, _, _ = self.model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "                errors = torch.mean((node_recon - batch.x) ** 2, dim=1).cpu().numpy()\n",
    "                self.val_recon_errors.extend(errors)\n",
    "        self.threshold = np.percentile(self.val_recon_errors, 98) if self.val_recon_errors else 0.5\n",
    "        self.val_error_mean = np.mean(self.val_recon_errors) if self.val_recon_errors else 0\n",
    "        self.val_error_std = np.std(self.val_recon_errors) if self.val_recon_errors else 1\n",
    "        \n",
    "        print(\"\\n🚀 Training GAE model...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience = 10\n",
    "        counter = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            all_train_scores = []\n",
    "            all_train_labels = []\n",
    "            \n",
    "            for batch in train_loader:\n",
    "                batch = batch.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                z, node_recon, edge_recon, supervised_score = self.model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "                \n",
    "                node_loss = F.mse_loss(node_recon, batch.x)\n",
    "                edge_loss = F.binary_cross_entropy_with_logits(edge_recon.squeeze(), torch.ones_like(edge_recon.squeeze())) if edge_recon is not None else 0\n",
    "                supervised_loss = F.mse_loss(supervised_score.squeeze(), batch.y) if batch.y is not None else 0\n",
    "                loss = node_loss + 0.1 * edge_loss + 0.1 * supervised_loss\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                \n",
    "                recon_errors = torch.mean((node_recon - batch.x) ** 2, dim=1).cpu().detach().numpy()\n",
    "                all_train_scores.append(recon_errors)\n",
    "                all_train_labels.append(batch.y.cpu().numpy())\n",
    "            \n",
    "            self.model.eval()\n",
    "            val_loss = 0\n",
    "            all_val_scores = []\n",
    "            all_val_labels = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    batch = batch.to(self.device)\n",
    "                    z, node_recon, edge_recon, supervised_score = self.model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "                    \n",
    "                    node_loss = F.mse_loss(node_recon, batch.x)\n",
    "                    edge_loss = F.binary_cross_entropy_with_logits(edge_recon.squeeze(), torch.ones_like(edge_recon.squeeze())) if edge_recon is not None else 0\n",
    "                    supervised_loss = F.mse_loss(supervised_score.squeeze(), batch.y) if batch.y is not None else 0\n",
    "                    loss = node_loss + 0.1 * edge_loss + 0.1 * supervised_loss\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                    \n",
    "                    recon_errors = torch.mean((node_recon - batch.x) ** 2, dim=1).cpu().numpy()\n",
    "                    all_val_scores.append(recon_errors)\n",
    "                    all_val_labels.append(batch.y.cpu().numpy())\n",
    "            \n",
    "            avg_train_loss = train_loss / len(train_loader) if len(train_loader) > 0 else 0\n",
    "            avg_val_loss = val_loss / len(val_loader) if len(val_loader) > 0 else 0\n",
    "            \n",
    "            train_auc = roc_auc_score(np.concatenate(all_train_labels), np.concatenate(all_train_scores)) if all_train_labels and np.any(np.concatenate(all_train_labels)) else 0\n",
    "            val_auc = roc_auc_score(np.concatenate(all_val_labels), np.concatenate(all_val_scores)) if all_val_labels and np.any(np.concatenate(all_val_labels)) else 0\n",
    "            val_pr_auc = average_precision_score(np.concatenate(all_val_labels), np.concatenate(all_val_scores)) if all_val_labels and np.any(np.concatenate(all_val_labels)) else 0\n",
    "            \n",
    "            if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "                print(f\"  Train Loss: {avg_train_loss:.4f}, AUC: {train_auc:.4f}\")\n",
    "                print(f\"  Val Loss: {avg_val_loss:.4f}, AUC: {val_auc:.4f}, PR-AUC: {val_pr_auc:.4f}\")\n",
    "            \n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(\"Early stopping!\")\n",
    "                    break\n",
    "        \n",
    "        print(\"\\n✓ Training complete!\")\n",
    "        return self.model\n",
    "    \n",
    "    def detect_suspicious_patterns(self, df, threshold=None):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained yet!\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        graph, account_to_idx = self.graph_builder.create_temporal_graph(df, 0, len(df))\n",
    "        if graph is None or account_to_idx is None:\n",
    "            return {'suspicious_accounts': [], 'total_accounts_analyzed': 0, 'high_risk_count': 0, 'k1c5_involvement': 0, 'suspicious_clusters': [], 'precision': 0.0}\n",
    "        \n",
    "        if self.scaler_node is not None:\n",
    "            graph.x = torch.tensor(self.scaler_node.transform(graph.x.numpy()), dtype=torch.float32)\n",
    "        if graph.edge_attr.shape[0] > 0 and self.scaler_edge is not None:\n",
    "            graph.edge_attr = torch.tensor(self.scaler_edge.transform(graph.edge_attr.numpy()), dtype=torch.float32)\n",
    "        \n",
    "        graph = graph.to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            z, node_recon, edge_recon, _ = self.model(graph.x, graph.edge_index, graph.edge_attr)\n",
    "            recon_errors = torch.mean((node_recon - graph.x) ** 2, dim=1).cpu().numpy()\n",
    "        \n",
    "        # Z-score based anomaly score scaling\n",
    "        scaled_errors = np.zeros_like(recon_errors)\n",
    "        if self.val_recon_errors:\n",
    "            scaled_errors = (recon_errors - self.val_error_mean) / (self.val_error_std + 1e-10)\n",
    "            scaled_errors = np.log1p(np.clip(scaled_errors, 0, None))  # Logarithmic transformation\n",
    "            scaled_errors = 100 * (scaled_errors - np.min(scaled_errors)) / (np.max(scaled_errors) - np.min(scaled_errors) + 1e-10)\n",
    "            scaled_errors = np.clip(scaled_errors, 0, 100)\n",
    "        \n",
    "        threshold = threshold if threshold is not None else self.threshold\n",
    "        accounts = sorted(set(df['customer_id'].unique() if 'customer_id' in df.columns else df['sender_location'].unique()) | \n",
    "                         set(df['merchant_id'].unique() if 'merchant_id' in df.columns else df['receiver_location'].unique()))\n",
    "        suspicious_accounts = []\n",
    "        pred_labels = []\n",
    "        true_labels = []\n",
    "        \n",
    "        for i, (account, error, scaled_error) in enumerate(zip(accounts, recon_errors, scaled_errors)):\n",
    "            is_suspicious = error > threshold\n",
    "            pred_labels.append(1 if is_suspicious else 0)\n",
    "            true_labels.append(graph.y[i].item())\n",
    "            if is_suspicious:\n",
    "                suspicious_accounts.append({\n",
    "                    'account': account,\n",
    "                    'anomaly_score': float(scaled_error),\n",
    "                    'is_high_risk_location': account == 'K1C5' or ('CUST_' in account and df[df['customer_id'] == account]['sender_location'].iloc[0] == 'K1C5' if 'customer_id' in df.columns else False),\n",
    "                    'pattern': self._identify_pattern(df, account)\n",
    "                })\n",
    "        \n",
    "        precision = precision_score(true_labels, pred_labels) if any(true_labels) else 0.0\n",
    "        \n",
    "        G = nx.DiGraph()\n",
    "        for _, row in df.iterrows():\n",
    "            sender = row['customer_id' if 'customer_id' in df.columns else 'sender_location']\n",
    "            receiver = row['merchant_id' if 'merchant_id' in df.columns else 'receiver_location']\n",
    "            G.add_edge(sender, receiver, weight=row['amount'])\n",
    "        \n",
    "        suspicious_clusters = []\n",
    "        components = list(nx.weakly_connected_components(G))\n",
    "        for component in components:\n",
    "            valid_accounts = [acc for acc in component if acc in account_to_idx]\n",
    "            if not valid_accounts or len(valid_accounts) <= 3:  # Relaxed to >2 nodes\n",
    "                continue\n",
    "            avg_error = np.mean([recon_errors[account_to_idx[acc]] for acc in valid_accounts])\n",
    "            avg_scaled_error = np.mean([scaled_errors[account_to_idx[acc]] for acc in valid_accounts])\n",
    "            k1c5_nodes = sum(1 for acc in component if acc == 'K1C5' or ('CUST_' in account and df[df['customer_id'] == acc]['sender_location'].iloc[0] == 'K1C5' if 'customer_id' in df.columns else False))\n",
    "            if avg_error > threshold and k1c5_nodes > 0:\n",
    "                suspicious_clusters.append({\n",
    "                    'nodes': list(component),\n",
    "                    'avg_anomaly_score': float(avg_scaled_error),\n",
    "                    'k1c5_nodes': k1c5_nodes\n",
    "                })\n",
    "        \n",
    "        suspicious_accounts.sort(key=lambda x: x['anomaly_score'], reverse=True)\n",
    "        suspicious_clusters.sort(key=lambda x: x['avg_anomaly_score'], reverse=True)\n",
    "        \n",
    "        return {\n",
    "            'suspicious_accounts': suspicious_accounts,\n",
    "            'total_accounts_analyzed': len(accounts),\n",
    "            'high_risk_count': len(suspicious_accounts),\n",
    "            'k1c5_involvement': sum(1 for acc in suspicious_accounts if acc['is_high_risk_location']),\n",
    "            'suspicious_clusters': suspicious_clusters[:5],\n",
    "            'precision': precision\n",
    "        }\n",
    "    \n",
    "    def _identify_pattern(self, df, account):\n",
    "        sent = df[df['customer_id' if 'customer_id' in df.columns else 'sender_location'] == account]\n",
    "        received = df[df['merchant_id' if 'merchant_id' in df.columns else 'receiver_location'] == account]\n",
    "        all_txns = pd.concat([sent, received])\n",
    "        \n",
    "        if len(all_txns) == 0:\n",
    "            return \"unknown\"\n",
    "        \n",
    "        patterns = []\n",
    "        if len(all_txns) > 5:\n",
    "            amounts = all_txns['amount'].values\n",
    "            if np.std(amounts) < np.mean(amounts) * 0.3:\n",
    "                time_spread = all_txns['transaction_hour'].max() - all_txns['transaction_hour'].min()\n",
    "                if time_spread < 6:\n",
    "                    patterns.append(\"structuring\")\n",
    "        \n",
    "        if all_txns['cross_border'].mean() > 0.7:\n",
    "            if len(set(all_txns['sender_location'].unique()) | set(all_txns['receiver_location'].unique())) > 4:\n",
    "                patterns.append(\"layering\")\n",
    "        \n",
    "        if all_txns['cash_equivalent'].mean() > 0.5:\n",
    "            if 'business' in all_txns['customer_segment'].values:\n",
    "                patterns.append(\"integration\")\n",
    "        \n",
    "        return \", \".join(patterns) if patterns else \"suspicious_activity\"\n",
    "\n",
    "def demonstrate_aml_system(df):\n",
    "    print(\"🏰 Fantasy Kingdom AML Detection System\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Investigating corrupt syndicate in K1C5...\")\n",
    "    print(f\"\\nDataset Overview:\")\n",
    "    print(f\"  Total transactions: {len(df)}\")\n",
    "    print(f\"  Money laundering cases: {df['is_money_laundering'].sum()}\")\n",
    "    print(f\"  K1C5 involvement: {((df['sender_location'] == 'K1C5') | (df['receiver_location'] == 'K1C5')).sum()}\")\n",
    "    \n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    aml_system = AMLDetectionSystem(hidden_dim=32, latent_dim=16)\n",
    "    train_size = int(len(df) * 0.8)\n",
    "    train_df = df.iloc[:train_size]\n",
    "    test_df = df.iloc[train_size:]\n",
    "    \n",
    "    aml_system.train(train_df, epochs=50, batch_size=16)\n",
    "    \n",
    "    print(\"\\n🔍 Analyzing test transactions for suspicious patterns...\")\n",
    "    results = aml_system.detect_suspicious_patterns(test_df)\n",
    "    \n",
    "    print(f\"\\n📊 Detection Results:\")\n",
    "    print(f\"  Accounts analyzed: {results['total_accounts_analyzed']}\")\n",
    "    print(f\"  High-risk accounts detected: {results['high_risk_count']}\")\n",
    "    print(f\"  K1C5 syndicate connections: {results['k1c5_involvement']}\")\n",
    "    print(f\"  Precision: {results['precision']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n🚨 Top Suspicious Accounts:\")\n",
    "    for i, acc in enumerate(results['suspicious_accounts'][:5], 1):\n",
    "        print(f\"  {i}. {acc['account']} - Anomaly Score: {acc['anomaly_score']:.2f} - Pattern: {acc['pattern']}\")\n",
    "        if acc['is_high_risk_location']:\n",
    "            print(f\"     ⚠️  ALERT: Direct K1C5 syndicate member!\")\n",
    "    \n",
    "    \n",
    "    return aml_system, results\n",
    "\n",
    "# Example usage with Mission 1 data\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate base data\n",
    "    df = create_base_transaction_data(n_samples=10000, ml_ratio=0.05)\n",
    "    \n",
    "    # Enhance with customer-merchant networks\n",
    "    df = create_customer_merchant_networks(df, n_customers=500, n_merchants=200)\n",
    "    \n",
    "    # Add time-series features\n",
    "    df = generate_time_series_flows(df, days=30)\n",
    "    \n",
    "    # Run AML detection\n",
    "    aml_system, results = demonstrate_aml_system(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________\n",
    "\n",
    "# Mission 3 Review\n",
    "\n",
    "You’ve trained a mystical familiar to hunt money laundering in Goldweave Port’s synthetic transaction ledger. This enchanted pet sniffs out K1C5 syndicate patterns, revealing suspicious accounts and networks. Your notebook outputs—high-risk accounts, K1C5 ties, and precision metrics—expose the syndicate’s schemes!\n",
    "\n",
    "## The Familiar’s Magic\n",
    "\n",
    "The Prophecy Familiar uses three powers to track K1C5:\n",
    "\n",
    "### 1. Web Weaving\n",
    "- **Action**: Spins transactions into webs (`networkx>=3.4.2`), with accounts as nodes and transactions as weighted threads (`amount`).\n",
    "- **Features**: Adds transaction counts, K1C5 flags (weighted x7), and time-series data (`velocity_24h`).\n",
    "- **Output**: Temporal graph snapshots (300 transactions, stride 20).\n",
    "\n",
    "**Role**: Maps K1C5 networks for detection.\n",
    "\n",
    "### 2. Arcane Vision\n",
    "- **Action**: Uses Graph Autoencoder (`torch>=2.8.0`, `torch-geometric>=2.6.1`) to learn normal patterns and spot anomalies.\n",
    "- **Process**: Encodes nodes with GCNs, reconstructs webs, and flags high-error nodes (anomaly scores 0–100).\n",
    "- **Training**: Fine-tunes on K1C5 labels.\n",
    "\n",
    "**Role**: Detects suspicious accounts (e.g., structuring, layering).\n",
    "\n",
    "### 3. Syndicate Hunt\n",
    "- **Action**: Trains on 80% of synthetic data (50 epochs), tests on 20%, and identifies suspicious clusters.\n",
    "- **Outputs**: Lists top accounts, scores, patterns (e.g., structuring for frequent small deposits), and K1C5-linked webs.\n",
    "- **Metrics**: Reports precision, account counts, K1C5 involvement.\n",
    "\n",
    "**Role**: Pinpoints high-risk accounts and networks.\n",
    "\n",
    "## Why It Matters\n",
    "Your familiar’s visions expose K1C5’s tactics, like structured deposits (<10,000 gold pieces) or cross-border layering, using synthetic data’s realistic patterns. With high precision, it bridges Missions 1 and 2, turning data into actionable omens. \n",
    "\n",
    "_______________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mission 4: The Oracle's Golem\n",
    "**Objective**: Develop a Retrieval-Augmented Generation (RAG) golem to analyze and answer questions about the synthetic data and insights for Missions 1, and 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "🔮 **Initiating Valdris Financial Investigation System...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "================================================================================"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "✓ **Initialized Arcane RAG System for Transaction and Document Analysis**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "1️⃣ **Loading Enchanted Ledger from mission1_synthetic_data.csv...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "📋 **Column types**: {'timestamp': dtype('O'), 'amount': dtype('float64'), 'account_age_days': dtype('int64'), 'transaction_hour': dtype('int64'), 'day_of_week': dtype('int64'), 'transactions_last_24h': dtype('int64'), 'account_balance_ratio': dtype('float64'), 'merchant_risk_score': dtype('float64'), 'cross_border': dtype('int64'), 'cash_equivalent': dtype('int64'), 'transaction_type': dtype('O'), 'customer_segment': dtype('O'), 'sender_location': dtype('O'), 'receiver_location': dtype('O'), 'is_money_laundering': dtype('int64'), 'customer_id': dtype('O'), 'merchant_id': dtype('O'), 'running_total': dtype('float64'), 'hour_of_day': dtype('int64'), 'velocity_1h': dtype('float64'), 'velocity_24h': dtype('float64')}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "📋 **Sample timestamp**: ['2024-01-02 07:36:34', '2024-01-03 12:51:26', '2024-01-07 18:19:47']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "✓ **Loaded 2000 ledger entries**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "2️⃣ **Deciphering Ancient Scrolls from mission2_documents...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "📜 **Decoding bank_statement_265055.pdf...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "📜 **Decoding SAR_883301.pdf...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "📜 **Decoding whistleblower_report_RG924718.pdf...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "✓ **Processed 19 scroll fragments**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "✓ **Indexed 2019 artifacts with FAISS and BM25**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "🏰 **Arcane Investigation Chamber Active**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Type your query or 'exit' to close the chamber."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Example Queries:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- Largest transaction in the database\n",
       "- Account with the highest merchant risk score\n",
       "- Suspicious transactions in K1C5"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "🔍 **Arcane Query: Account with the highest merchant risk score**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "📜 **Arcane Findings**:\n",
       " - Suspicious Patterns: Frequent transactions with Account CUST_0015 at the Goldweave Port (K3C4), specifically in K1C5.\n",
       "   - Key Entities: Account CUST_00009, Location K3C4, Counterparty MERCH_0015.\n",
       "   - Money Laundering Indicators: High total amount of 1,073.09 Gold Pieces in a single account. Irregular transactions at the Goldweave Port.\n",
       "   - Risk Assessment: Increase surveillance and monitoring of Account CUST_00009's activities with Counterparty MERCH_0015 in K1C5. Investigate potential money laundering activities due to high risk score, irregular patterns, and large transaction amounts."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "👋 **Chamber Closed. 1 queries investigated.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pdfplumber\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from IPython.display import display, Markdown\n",
    "import ollama\n",
    "import signal\n",
    "import sys\n",
    "\n",
    "# Signal handler for graceful exit\n",
    "def signal_handler(sig, frame):\n",
    "    display(Markdown(\"🪄 **Arcane Investigation Terminated.**\"))\n",
    "    sys.exit(0)\n",
    "\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "display(Markdown(\"🔮 **Initiating Valdris Financial Investigation System...**\"))\n",
    "display(Markdown(\"=\" * 80))\n",
    "\n",
    "class FantasyRAGChatbot:\n",
    "    def __init__(self, transaction_csv=\"mission1_synthetic_data.csv\", docs_folder=\"mission2_documents\"):\n",
    "        self.transaction_csv = transaction_csv\n",
    "        self.docs_folder = docs_folder\n",
    "        self.documents = []\n",
    "        self.embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        self.faiss_index = None\n",
    "        self.bm25 = None\n",
    "        self.model_name = \"mistral:7b-instruct-v0.3-q4_0\"\n",
    "        self.df = None\n",
    "        display(Markdown(\"✓ **Initialized Arcane RAG System for Transaction and Document Analysis**\"))\n",
    "\n",
    "    def load_transaction_data(self):\n",
    "        \"\"\"Load and process transaction data from CSV, aligned with provided structure\"\"\"\n",
    "        display(Markdown(f\"\\n1️⃣ **Loading Enchanted Ledger from {self.transaction_csv}...**\"))\n",
    "        try:\n",
    "            self.df = pd.read_csv(self.transaction_csv)\n",
    "            expected_columns = [\n",
    "                'timestamp', 'amount', 'account_age_days', 'transaction_hour', 'day_of_week',\n",
    "                'transactions_last_24h', 'account_balance_ratio', 'merchant_risk_score',\n",
    "                'cross_border', 'cash_equivalent', 'transaction_type', 'customer_segment',\n",
    "                'sender_location', 'receiver_location', 'is_money_laundering', 'customer_id',\n",
    "                'merchant_id', 'running_total', 'hour_of_day', 'velocity_1h', 'velocity_24h'\n",
    "            ]\n",
    "            if not all(col in self.df.columns for col in expected_columns):\n",
    "                missing = [col for col in expected_columns if col not in self.df.columns]\n",
    "                display(Markdown(f\"⚠️ **Error: CSV missing columns: {missing}.**\"))\n",
    "                return []\n",
    "            # Debug: Show column info\n",
    "            display(Markdown(f\"📋 **Column types**: {self.df.dtypes.to_dict()}\"))\n",
    "            display(Markdown(f\"📋 **Sample timestamp**: {self.df['timestamp'].head(3).to_list()}\"))\n",
    "            # Convert timestamp to date (YYYY-MM-DD) and rename to 'date'\n",
    "            self.df['date'] = pd.to_datetime(self.df['timestamp'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "            if self.df['date'].isna().all():\n",
    "                display(Markdown(\"⚠️ **Error: All timestamps are invalid.**\"))\n",
    "                self.df['date'] = \"Unknown\"\n",
    "        except FileNotFoundError:\n",
    "            display(Markdown(f\"⚠️ **Error: {self.transaction_csv} not found. Ensure Mission 1 data is generated.**\"))\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            display(Markdown(f\"⚠️ **Error loading CSV: {str(e)}.**\"))\n",
    "            return []\n",
    "\n",
    "        transaction_docs = []\n",
    "        for idx, row in self.df.iterrows():\n",
    "            doc_text = f\"\"\"\n",
    "TRANSACTION #{idx}\n",
    "Date: {row['date']}\n",
    "Location: {row['sender_location']}\n",
    "Account: {row['customer_id']}\n",
    "Amount: {row['amount']:,} Gold Pieces\n",
    "Type: {row['transaction_type']}\n",
    "Description: {row.get('description', 'N/A')}\n",
    "Counterparty: {row['merchant_id']}\n",
    "            \"\"\".strip()\n",
    "            metadata = {\n",
    "                \"type\": \"transaction\",\n",
    "                \"transaction_id\": str(idx),\n",
    "                \"location\": row['sender_location'],\n",
    "                \"amount\": float(row['amount']),\n",
    "                \"counterparty\": row['merchant_id'],\n",
    "                \"description\": row.get('description', 'N/A'),\n",
    "                \"is_money_laundering\": bool(row['is_money_laundering']),\n",
    "                \"merchant_risk_score\": float(row['merchant_risk_score'])\n",
    "            }\n",
    "            transaction_docs.append({\"text\": doc_text, \"metadata\": metadata})\n",
    "        display(Markdown(f\"✓ **Loaded {len(transaction_docs)} ledger entries**\"))\n",
    "        return transaction_docs\n",
    "\n",
    "    def load_investigation_documents(self):\n",
    "        \"\"\"Extract text and tables from PDFs with improved error handling\"\"\"\n",
    "        display(Markdown(f\"\\n2️⃣ **Deciphering Ancient Scrolls from {self.docs_folder}...**\"))\n",
    "        docs_path = Path(self.docs_folder)\n",
    "        if not docs_path.exists():\n",
    "            display(Markdown(f\"⚠️ **Error: {self.docs_folder} not found. Run Mission 2 to generate documents.**\"))\n",
    "            return []\n",
    "        \n",
    "        investigation_docs = []\n",
    "        for pdf_file in docs_path.glob(\"*.pdf\"):\n",
    "            display(Markdown(f\"📜 **Decoding {pdf_file.name}...**\"))\n",
    "            try:\n",
    "                with pdfplumber.open(pdf_file) as pdf:\n",
    "                    full_text = \"\"\n",
    "                    for page in pdf.pages:\n",
    "                        try:\n",
    "                            text = page.extract_text() or \"\"\n",
    "                            tables = page.extract_tables() or []\n",
    "                            for table in tables:\n",
    "                                table_str = \"\\n\".join([\",\".join(str(cell or '') for cell in row) for row in table])\n",
    "                                text += f\"\\nTable:\\n{table_str}\\n\"\n",
    "                            full_text += text + \"\\n\"\n",
    "                        except Exception as e:\n",
    "                            display(Markdown(f\"⚠️ **Warning: Failed to extract page from {pdf_file.name}: {str(e)}**\"))\n",
    "                    if not full_text.strip():\n",
    "                        display(Markdown(f\"⚠️ **Warning: No text extracted from {pdf_file.name}**\"))\n",
    "                        continue\n",
    "                    chunks = [full_text[i:i+500] for i in range(0, len(full_text), 500)]\n",
    "                    doc_type = \"unknown\"\n",
    "                    if \"whistleblower\" in pdf_file.name.lower():\n",
    "                        doc_type = \"whistleblower_report\"\n",
    "                    elif \"bank_statement\" in pdf_file.name.lower():\n",
    "                        doc_type = \"bank_statement\"\n",
    "                    elif \"sar\" in pdf_file.name.lower():\n",
    "                        doc_type = \"suspicious_activity_report\"\n",
    "                    for chunk_idx, chunk in enumerate(chunks):\n",
    "                        metadata = {\n",
    "                            \"type\": \"document\",\n",
    "                            \"document_type\": doc_type,\n",
    "                            \"filename\": pdf_file.name,\n",
    "                            \"chunk_id\": chunk_idx\n",
    "                        }\n",
    "                        investigation_docs.append({\"text\": chunk, \"metadata\": metadata})\n",
    "            except Exception as e:\n",
    "                display(Markdown(f\"⚠️ **Error processing {pdf_file.name}: {str(e)}**\"))\n",
    "                continue\n",
    "        display(Markdown(f\"✓ **Processed {len(investigation_docs)} scroll fragments**\"))\n",
    "        return investigation_docs\n",
    "\n",
    "    def setup_index(self):\n",
    "        \"\"\"Build FAISS and BM25 indices\"\"\"\n",
    "        self.documents = self.load_transaction_data() + self.load_investigation_documents()\n",
    "        if not self.documents:\n",
    "            display(Markdown(\"⚠️ **Error: No documents loaded. Check data and document paths.**\"))\n",
    "            return\n",
    "        for doc in self.documents:\n",
    "            try:\n",
    "                doc[\"embedding\"] = self.embedder.encode(doc[\"text\"], convert_to_numpy=True)\n",
    "            except Exception as e:\n",
    "                display(Markdown(f\"⚠️ **Error embedding document: {str(e)}**\"))\n",
    "                doc[\"embedding\"] = np.zeros(384)\n",
    "        embeddings = np.array([doc[\"embedding\"] for doc in self.documents]).astype('float32')\n",
    "        dimension = embeddings.shape[1]\n",
    "        self.faiss_index = faiss.IndexFlatL2(dimension)\n",
    "        self.faiss_index.add(embeddings)\n",
    "        tokenized_docs = [doc[\"text\"].lower().split() for doc in self.documents]\n",
    "        self.bm25 = BM25Okapi(tokenized_docs)\n",
    "        display(Markdown(f\"✓ **Indexed {len(self.documents)} artifacts with FAISS and BM25**\"))\n",
    "\n",
    "    def search_documents(self, query, top_k=5, prioritize_transactions=False):\n",
    "        \"\"\"Hybrid search with option to prioritize transaction data\"\"\"\n",
    "        try:\n",
    "            query_embedding = self.embedder.encode(query, convert_to_numpy=True).astype('float32')\n",
    "            tokenized_query = query.lower().split()\n",
    "            bm25_scores = self.bm25.get_scores(tokenized_query)\n",
    "            distances, indices = self.faiss_index.search(query_embedding.reshape(1, -1), top_k * 2)\n",
    "            semantic_scores = [1 / (1 + d) for d in distances[0]]\n",
    "            combined_scores = []\n",
    "            for j, i in enumerate(indices[0]):\n",
    "                score = 0.4 * bm25_scores[i] + 0.6 * semantic_scores[j]\n",
    "                if prioritize_transactions and self.documents[i][\"metadata\"][\"type\"] == \"transaction\":\n",
    "                    score *= 1.5\n",
    "                combined_scores.append((i, score))\n",
    "            top_indices = sorted(combined_scores, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "            results = {\n",
    "                \"documents\": [self.documents[i][\"text\"] for i, _ in top_indices],\n",
    "                \"metadatas\": [self.documents[i][\"metadata\"] for i, _ in top_indices]\n",
    "            }\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            display(Markdown(f\"⚠️ **Error during search: {str(e)}**\"))\n",
    "            return {\"documents\": [], \"metadatas\": []}\n",
    "\n",
    "    def generate_transaction_stats(self, query):\n",
    "        \"\"\"Compute specific statistics from CSV based on query\"\"\"\n",
    "        if not hasattr(self, 'df') or self.df is None or self.df.empty:\n",
    "            return \"No transaction data available.\"\n",
    "        \n",
    "        try:\n",
    "            if 'date' not in self.df.columns:\n",
    "                display(Markdown(\"⚠️ **Error: 'date' column missing in CSV.**\"))\n",
    "                return \"Unable to compute statistics due to missing 'date' column.\"\n",
    "            \n",
    "            if \"largest transaction\" in query.lower():\n",
    "                max_trans = self.df.loc[self.df['amount'].idxmax()]\n",
    "                date_str = str(max_trans['date']) if not pd.isna(max_trans['date']) else \"Unknown\"\n",
    "                return f\"\"\"\n",
    "Largest Transaction:\n",
    "- Amount: {max_trans['amount']:,} Gold Pieces\n",
    "- Date: {date_str}\n",
    "- Location: {max_trans['sender_location']}\n",
    "- Account: {max_trans['customer_id']}\n",
    "- Counterparty: {max_trans['merchant_id']}\n",
    "- Type: {max_trans['transaction_type']}\n",
    "\"\"\"\n",
    "            elif \"highest merchant risk score\" in query.lower():\n",
    "                max_risk = self.df.loc[self.df['merchant_risk_score'].idxmax()]\n",
    "                return f\"\"\"\n",
    "Highest Risk Account:\n",
    "- Account: {max_risk['customer_id']}\n",
    "- Merchant Risk Score: {max_risk['merchant_risk_score']:.2f}\n",
    "- Total Amount: {max_risk['amount']:,} Gold Pieces\n",
    "- Location: {max_risk['sender_location']}\n",
    "- Counterparty: {max_risk['merchant_id']}\n",
    "\"\"\"\n",
    "            elif \"suspicious\" in query.lower() or \"K1C5\" in query.lower() or \"money laundering\" in query.lower():\n",
    "                suspicious_df = self.df[\n",
    "                    (self.df['is_money_laundering'] == 1) |\n",
    "                    (self.df['amount'].between(9000, 9950)) |\n",
    "                    (self.df['amount'] > 15000) |\n",
    "                    (self.df['sender_location'] == 'K1C5')\n",
    "                ]\n",
    "                return f\"\"\"\n",
    "Transaction Summary:\n",
    "- Suspicious Transactions: {len(suspicious_df)}\n",
    "- Total Amount: {suspicious_df['amount'].sum():,.2f} Gold Pieces\n",
    "- Top Locations: {suspicious_df['sender_location'].value_counts().head(3).to_dict()}\n",
    "- Top Transaction Types: {suspicious_df['transaction_type'].value_counts().head(2).to_dict()}\n",
    "\"\"\"\n",
    "        except Exception as e:\n",
    "            display(Markdown(f\"⚠️ **Error in stats computation: {str(e)}**\"))\n",
    "            return f\"Unable to compute statistics: {str(e)}\"\n",
    "        return \"\"\n",
    "\n",
    "    def generate_response(self, query, search_results):\n",
    "        \"\"\"Generate concise response with Ollama, prioritizing CSV for database queries\"\"\"\n",
    "        is_csv_query = any(term in query.lower() for term in [\"transaction\", \"database\", \"largest\", \"highest\", \"account\", \"amount\"])\n",
    "        context = \"\"\n",
    "        \n",
    "        if is_csv_query:\n",
    "            stats = self.generate_transaction_stats(query)\n",
    "            context = stats if stats else \"\\n\\n\".join(search_results[\"documents\"][:3])\n",
    "        else:\n",
    "            context = \"\\n\\n\".join(search_results[\"documents\"][:3])\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "By the decree of the Valdris Council, you are the Arcane Investigator, tasked with exposing financial crimes.\n",
    "CONTEXT (prioritize transactions for database queries, then documents; K1C5 is Goldweave Port):\n",
    "{context}\n",
    "QUESTION: {query}\n",
    "Respond in 100 words or less with concise bullet points:\n",
    "- Suspicious Patterns: [e.g., frequent K1C5 transactions]\n",
    "- Key Entities: [Accounts, locations, counterparties]\n",
    "- Money Laundering Indicators: [From transactions or documents]\n",
    "- Risk Assessment: [Recommendation]\n",
    "Use specific data from context. Avoid elaboration.\n",
    "\"\"\"\n",
    "        try:\n",
    "            response = ollama.chat(model=self.model_name, messages=[{'role': 'user', 'content': prompt}])\n",
    "            response_text = response['message']['content']\n",
    "            words = response_text.split()\n",
    "            if len(words) > 100:\n",
    "                response_text = ' '.join(words[:100]) + \"...\"\n",
    "            return response_text\n",
    "        except Exception as e:\n",
    "            display(Markdown(f\"⚠️ **Error: Ollama failed - {str(e)}. Ensure Ollama server is running with model {self.model_name}.**\"))\n",
    "            return \"Unable to generate response due to Ollama error.\"\n",
    "\n",
    "    def investigate(self, query):\n",
    "        \"\"\"Run investigation query\"\"\"\n",
    "        display(Markdown(f\"\\n🔍 **Arcane Query: {query}**\"))\n",
    "        if not self.documents or self.faiss_index is None:\n",
    "            display(Markdown(\"⚠️ **Error: Index not set up. Run setup_index first.**\"))\n",
    "            return \"\"\n",
    "        prioritize_transactions = any(term in query.lower() for term in [\"transaction\", \"database\", \"largest\", \"highest\", \"account\", \"amount\"])\n",
    "        search_results = self.search_documents(query, top_k=10, prioritize_transactions=prioritize_transactions)\n",
    "        if not search_results[\"documents\"]:\n",
    "            display(Markdown(\"⚠️ **No relevant documents found for query.**\"))\n",
    "            return \"No relevant information found.\"\n",
    "        response = self.generate_response(query, search_results)\n",
    "        display(Markdown(f\"📜 **Arcane Findings**:\\n{response}\"))\n",
    "        return response\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Main loop for interactive investigation\"\"\"\n",
    "        self.setup_index()\n",
    "        if not self.documents:\n",
    "            display(Markdown(\"⚠️ **Error: No data loaded. Investigation halted.**\"))\n",
    "            return\n",
    "        display(Markdown(\"\\n🏰 **Arcane Investigation Chamber Active**\"))\n",
    "        display(Markdown(\"Type your query or 'exit' to close the chamber.\"))\n",
    "        display(Markdown(\"Example Queries:\"))\n",
    "        display(Markdown(\"- Largest transaction in the database\\n- Account with the highest merchant risk score\\n- Suspicious transactions in K1C5\"))\n",
    "        question_count = 0\n",
    "        while True:\n",
    "            try:\n",
    "                question = input(f\"🪄 Query #{question_count + 1}: \").strip()\n",
    "                if question.lower() in ['exit', 'quit', 'stop']:\n",
    "                    display(Markdown(f\"\\n👋 **Chamber Closed. {question_count} queries investigated.**\"))\n",
    "                    break\n",
    "                if question:\n",
    "                    self.investigate(question)\n",
    "                    question_count += 1\n",
    "                else:\n",
    "                    display(Markdown(\"💡 **Enter a query or 'exit'.**\"))\n",
    "            except Exception as e:\n",
    "                display(Markdown(f\"⚠️ **Error during query: {str(e)}**\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot = FantasyRAGChatbot()\n",
    "    chatbot.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "_______________________________________________________\n",
    "\n",
    "# Mission 4 Review \n",
    "\n",
    "Your mission was to deploy the `FantasyRAGChatbot`, a mystical Retrieval-Augmented Generation (RAG) golem, to query the enchanted transaction ledger (`mission1_synthetic_data.csv`) and ancient scrolls (`mission2_documents/`) from Goldweave Port. By running the chatbot in your Jupyter notebook, you’ve extracted insights about suspicious transactions and documents, uncovering potential money laundering tied to the K1C5 syndicate. Let’s dive into how this arcane tool works and what it reveals!\n",
    "\n",
    "## How It Works\n",
    "\n",
    "The `FantasyRAGChatbot` harnesses synthetic data and documents:\n",
    "\n",
    "- **Transaction Loader** (`pandas>=2.2.3`): Reads synthetic CSV with columns like `amount`, `customer_id`, `is_money_laundering`. Converts timestamps to dates. Creates text summaries (e.g., “10,000 Gold Pieces, K1C5”).\n",
    "- **Document Extractor** (`pdfplumber>=0.11.7`): Extracts text/tables from PDFs (e.g., whistleblower reports). Splits into 500-char chunks, labeling as `bank_statement` or `suspicious_activity_report`.\n",
    "- **RAG System**: Uses BM25 (`rank-bm25>=0.2.2`) and SentenceTransformers (`sentence-transformers==3.2.0`) for retrieval, FAISS (`faiss-cpu==1.9.0`) for embedding search, and Mistral (`ollama>=0.5.3`) for <100-word responses. Prioritizes synthetic transaction data for queries like “largest transaction.”\n",
    "\n",
    "## Why It Matters\n",
    "Mission 1’s synthetic data mimics real AML patterns (e.g., K1C5’s high-amount transfers) using Gaussian Copula for statistical fidelity and CTGAN for complex distributions. The golem queries these patterns, revealing suspicious transactions or document clues. Empower the high court giving them the tool to further assess the evnidence, expose synthetic patterns, and take down the money laundering syndicate!\n",
    "\n",
    "____________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Conclusion: Triumph of the Arcane Investigator\n",
    "\n",
    "Noble Investigator, through the 4 missions, you hhave successfully unraveled Goldweave Port’s financial mysteries and money laundering patterns! \n",
    "\n",
    "In Mission 1, you forged a synthetic ledger (mission1_synthetic_data.csv) using Gaussian Copula, CTGAN, Graph Network, and TVAE, crafting realistic transaction patterns to expose K1C5’s schemes. \n",
    "\n",
    "Mission 2, conjured ancient scrolls (mission2_documents/) with whistleblower reports and bank statements. \n",
    "\n",
    "Mission 3, your synthetic data laid the groud work for training your familiar so it has the power to detect suspicious patterns utilizing... \n",
    "\n",
    "In Mission 4, the FantasyRAGChatbot golem empowered you to query transactions and documents, revealing high-value transfers and syndicate clues with BM25, SentenceTransformers, and Mistral. Your synthetic data-driven insights have armed the Valdris Council to combat money laundering. Your arcane mastery has safeguarded the realm!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
